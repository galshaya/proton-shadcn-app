This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
deploy/
  newsletter_agent/
    templates/
      final_index.html
    agent_newsletter_generator.py
    final_app.py
    requirements.txt
    sample_newsletter.json
    wsgi.py
  scraper/
    add_feed.py
    list_articles.py
    render.yaml
    requirements.txt
    run_scraper.py
    test_deployed.py
    vector_embeddings.py
  scraping_package_gui/
    proton_db_setup.py
    requirements.txt
    run_scraping_system.py
    scraping_package_gui.py
    scraping_package_model.py
    scraping_package_scheduler.py
  web/
    article_viewer_simple.py
    render.yaml
    wsgi.py
templates/
  article.html
  articles.html
  base.html
  basic_index.html
  final_index.html
  fixed_index.html
  index.html
  package.html
  packages.html
  simple_index.html
  test_streaming.html
xx/
  rag_tester/
    templates/
      index.html
    .gitignore
    app.py
    README.md
    render.yaml
    requirements.txt
  agent_newsletter_app.py
  AGENT_NEWSLETTER_DOCS.md
  agent_newsletter_generator.py
  agent_newsletter_README.md
  agent_requirements.txt
  article_viewer.py
  basic_app.py
  final_app.py
  fixed_app.py
  scraper_example.py
  simple_streaming_app.py
  test_agent.py
  test_embeddings.py
  test_scraping.py
  test_streaming.py
  update_embeddings.py
  view_articles.py
  wsgi.py
-la DÔÄ∫githubproton_dbdeploy
.gitignore
Dockerfile
PROTON - Content curation and delivery system.md
Proton Backend Specs.md
Proton Product Req.md
proton_db_setup.py
README.md
render.yaml
requirements.txt
run_scraper.py
run_scraping_system.py
sample_newsletter.json
scraping_package_scheduler.py
vector_embeddings.py
vercel.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="-la DÔÄ∫githubproton_dbdeploy">
[1mdiff --git a/.gitignore b/.gitignore[m
[1mindex cce3c8a..fc1dcc0 100644[m
[1m--- a/.gitignore[m
[1m+++ b/.gitignore[m
[36m@@ -53,7 +53,7 @@[m [mnltk_data/[m
 logs/[m
 [m
 # Local templates generated by article_viewer.py[m
[31m-# templates/[m
[32m+[m[32mtemplates/[m
 [m
 # System files[m
 .DS_Store[m
[1mdiff --git a/agent_newsletter_README.md b/agent_newsletter_README.md[m
[1mdeleted file mode 100644[m
[1mindex 10da967..0000000[m
[1m--- a/agent_newsletter_README.md[m
[1m+++ /dev/null[m
[36m@@ -1,91 +0,0 @@[m
[31m-# Agent-Based Newsletter Generator[m
[31m-[m
[31m-This application provides an agent-based approach to newsletter generation, replacing the RAG-based approach while maintaining the same interface. The agent can query the MongoDB vector database, process context, and generate newsletters based on user instructions.[m
[31m-[m
[31m-## Features[m
[31m-[m
[31m-- Uses an AI agent to generate newsletters instead of a simple RAG approach[m
[31m-- Connects to the same MongoDB database as the RAG system[m
[31m-- Filters articles by date range and scraping packages[m
[31m-- Supports search queries to find relevant articles[m
[31m-- Allows adding client and project context[m
[31m-- Supports uploading additional context files (TXT, PDF, MD, DOCX)[m
[31m-- Uses OpenAI models for generation[m
[31m-[m
[31m-## Setup[m
[31m-[m
[31m-1. Make sure you have all the required environment variables in your `.env` file:[m
[31m-   ```[m
[31m-   MONGODB_URI=your_mongodb_connection_string[m
[31m-   MONGODB_DB_NAME=proton[m
[31m-   OPENAI_API_KEY=your_openai_api_key[m
[31m-   FLASK_SECRET_KEY=your_secret_key[m
[31m-   ```[m
[31m-[m
[31m-2. Install the required dependencies:[m
[31m-   ```[m
[31m-   pip install -r requirements.txt[m
[31m-   ```[m
[31m-[m
[31m-3. Run the application:[m
[31m-   ```[m
[31m-   python agent_newsletter_app.py[m
[31m-   ```[m
[31m-[m
[31m-4. Open your browser and navigate to `http://localhost:5000`[m
[31m-[m
[31m-## How It Works[m
[31m-[m
[31m-1. **Agent Initialization**: The application initializes an AI agent that connects to your MongoDB database and OpenAI API.[m
[31m-[m
[31m-2. **Article Search**: When you submit a search query, the agent searches for relevant articles in the database using text search, regex search, or filters.[m
[31m-[m
[31m-3. **Context Building**: The agent builds context from the retrieved articles, client information, project details, and any uploaded files.[m
[31m-[m
[31m-4. **Newsletter Generation**: The agent uses the OpenAI API to generate a newsletter based on the context and your instructions.[m
[31m-[m
[31m-## Usage[m
[31m-[m
[31m-1. **Model Selection**: Choose an OpenAI model from the dropdown menu.[m
[31m-[m
[31m-2. **Data Filtering**:[m
[31m-   - Select a date range for articles (All Time, Last 7 Days, Last 30 Days, Last 90 Days)[m
[31m-   - Choose which scraping packages to include[m
[31m-   - Enter a search query to find relevant articles[m
[31m-[m
[31m-3. **Context Information**:[m
[31m-   - Enter information about the client[m
[31m-   - Enter information about the project[m
[31m-   - Upload additional context files (TXT, PDF, MD, DOCX)[m
[31m-[m
[31m-4. **Newsletter Instructions**: Enter detailed instructions for the newsletter generation.[m
[31m-[m
[31m-5. Click "Generate Newsletter" to create the content.[m
[31m-[m
[31m-## Differences from RAG Approach[m
[31m-[m
[31m-The agent-based approach offers several advantages over the simple RAG approach:[m
[31m-[m
[31m-1. **More Intelligent Processing**: The agent can better understand the relationships between articles and context.[m
[31m-[m
[31m-2. **Better Context Integration**: The agent can more effectively integrate client and project context with the article information.[m
[31m-[m
[31m-3. **Improved Structure**: The agent can create more coherent and well-structured newsletters.[m
[31m-[m
[31m-4. **Adaptability**: The agent can adapt to different types of newsletter requirements and styles.[m
[31m-[m
[31m-## Customization[m
[31m-[m
[31m-You can customize the agent's behavior by modifying the `NewsletterAgent` class in `agent_newsletter_generator.py`. For example, you can:[m
[31m-[m
[31m-- Change the system message to give the agent different instructions[m
[31m-- Adjust the search parameters for finding relevant articles[m
[31m-- Modify how context is built and formatted[m
[31m-- Change the temperature or other generation parameters[m
[31m-[m
[31m-## Troubleshooting[m
[31m-[m
[31m-- **MongoDB Connection Issues**: Make sure your MongoDB URI is correct and that your database is accessible.[m
[31m-- **OpenAI API Issues**: Verify that your OpenAI API key is valid and has sufficient quota.[m
[31m-- **Missing Articles**: Check that your database contains articles and that the scraping packages are correctly configured.[m
[31m-- **File Upload Issues**: Ensure that the uploaded files are in the supported formats and are not too large.[m
[1mdiff --git a/agent_newsletter_app.py b/agent_newsletter_app.py[m
[1mdeleted file mode 100644[m
[1mindex a3db558..0000000[m
[1m--- a/agent_newsletter_app.py[m
[1m+++ /dev/null[m
[36m@@ -1,579 +0,0 @@[m
[31m-"""[m
[31m-Agent-Based Newsletter Generator Web Interface[m
[31m-[m
[31m-This Flask application provides a UI for testing agent-based newsletter generation[m
[31m-with different models, filters, and context options.[m
[31m-"""[m
[31m-[m
[31m-import os[m
[31m-import datetime[m
[31m-import logging[m
[31m-from flask import Flask, render_template, request, jsonify, flash, Response, session[m
[31m-from werkzeug.utils import secure_filename[m
[31m-from dotenv import load_dotenv[m
[31m-import PyPDF2[m
[31m-import io[m
[31m-[m
[31m-# Import our agent[m
[31m-from agent_newsletter_generator import NewsletterAgent[m
[31m-[m
[31m-# Configure logging[m
[31m-logging.basicConfig([m
[31m-    level=logging.INFO,[m
[31m-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'[m
[31m-)[m
[31m-logger = logging.getLogger(__name__)[m
[31m-[m
[31m-# Load environment variables[m
[31m-load_dotenv()[m
[31m-[m
[31m-# Initialize Flask app[m
[31m-app = Flask(__name__)[m
[31m-app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')[m
[31m-[m
[31m-# Configure upload folder[m
[31m-UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')[m
[31m-os.makedirs(UPLOAD_FOLDER, exist_ok=True)[m
[31m-app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER[m
[31m-app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size[m
[31m-ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}[m
[31m-[m
[31m-# Initialize global agent[m
[31m-agent = None[m
[31m-[m
[31m-def init_agent():[m
[31m-    """Initialize the newsletter agent"""[m
[31m-    global agent[m
[31m-    try:[m
[31m-        agent = NewsletterAgent()[m
[31m-        logger.info("Newsletter agent initialized")[m
[31m-    except Exception as e:[m
[31m-        logger.error(f"Failed to initialize newsletter agent: {e}")[m
[31m-        agent = None[m
[31m-[m
[31m-def allowed_file(filename):[m
[31m-    """Check if file has an allowed extension"""[m
[31m-    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS[m
[31m-[m
[31m-def extract_text_from_file(file):[m
[31m-    """Extract text from uploaded file"""[m
[31m-    filename = secure_filename(file.filename)[m
[31m-    file_ext = filename.rsplit('.', 1)[1].lower()[m
[31m-[m
[31m-    if file_ext == 'txt' or file_ext == 'md':[m
[31m-        # Text files[m
[31m-        return file.read().decode('utf-8')[m
[31m-    elif file_ext == 'pdf':[m
[31m-        # PDF files[m
[31m-        try:[m
[31m-            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))[m
[31m-            text = ""[m
[31m-            for page in pdf_reader.pages:[m
[31m-                text += page.extract_text() + "\n"[m
[31m-            return text[m
[31m-        except Exception as e:[m
[31m-            logger.error(f"Error extracting text from PDF: {e}")[m
[31m-            return ""[m
[31m-    elif file_ext == 'docx':[m
[31m-        # DOCX files (requires python-docx package)[m
[31m-        try:[m
[31m-            import docx[m
[31m-            doc = docx.Document(io.BytesIO(file.read()))[m
[31m-            text = ""[m
[31m-            for para in doc.paragraphs:[m
[31m-                text += para.text + "\n"[m
[31m-            return text[m
[31m-        except Exception as e:[m
[31m-            logger.error(f"Error extracting text from DOCX: {e}")[m
[31m-            return ""[m
[31m-[m
[31m-    return ""[m
[31m-[m
[31m-# Add context processor for current datetime[m
[31m-@app.context_processor[m
[31m-def inject_now():[m
[31m-    return {'now': datetime.datetime.now()}[m
[31m-[m
[31m-@app.route('/')[m
[31m-def index():[m
[31m-    """Render the main page"""[m
[31m-    global agent[m
[31m-[m
[31m-    if agent is None:[m
[31m-        init_agent()[m
[31m-[m
[31m-    if agent is None:[m
[31m-        flash("Failed to initialize newsletter agent", "error")[m
[31m-        return render_template('index.html', packages=[])[m
[31m-[m
[31m-    try:[m
[31m-        # Get all scraping packages[m
[31m-        packages = agent.get_scraping_packages()[m
[31m-[m
[31m-        # Define available models[m
[31m-        openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"] if agent.openai_client else [][m
[31m-[m
[31m-        return render_template([m
[31m-            'index.html',[m
[31m-            packages=packages,[m
[31m-            openai_models=openai_models,[m
[31m-            anthropic_models=[]  # We're only using OpenAI for now[m
[31m-        )[m
[31m-    except Exception as e:[m
[31m-        logger.error(f"Error loading index page: {e}")[m
[31m-        flash(f"Error: {str(e)}", "error")[m
[31m-        return render_template('index.html', packages=[])[m
[31m-[m
[31m-# Create a streaming route for the agent[m
[31m-@app.route('/generate-stream', methods=['GET', 'POST'])[m
[31m-def generate_stream():[m
[31m-    """Generate newsletter content using the agent with streaming"""[m
[31m-    global agent[m
[31m-[m
[31m-    if agent is None:[m
[31m-        init_agent()[m
[31m-[m
[31m-    if agent is None:[m
[31m-        return jsonify({"error": "Failed to initialize newsletter agent"}), 500[m
[31m-[m
[31m-    # Check if this is a GET request (for SSE connection) or POST (for form submission)[m
[31m-    if request.method == 'GET':[m
[31m-        # This is the SSE connection[m
[31m-        # Create a generator function for streaming[m
[31m-        def generate_stream_content():[m
[31m-            # Queue for collecting chunks from the callback[m
[31m-            from queue import Queue[m
[31m-            chunk_queue = Queue()[m
[31m-[m
[31m-            # Define the callback function for streaming[m
[31m-            def stream_callback(content):[m
[31m-                chunk_queue.put(content)[m
[31m-[m
[31m-            try:[m
[31m-                # Initial message[m
[31m-                yield "data: Starting newsletter generation...\n\n"[m
[31m-[m
[31m-                # Start the agent in a separate thread[m
[31m-                import threading[m
[31m-[m
[31m-                # Get form data from the most recent POST request[m
[31m-                # This is a simplification - in a real app, you'd use session or a database[m
[31m-                # to store the form data between requests[m
[31m-                model_name = app.config.get('LAST_MODEL', 'gpt-4o')[m
[31m-                date_range = app.config.get('LAST_DATE_RANGE', 'all')[m
[31m-                package_ids = app.config.get('LAST_PACKAGE_IDS', [])[m
[31m-                search_query = app.config.get('LAST_SEARCH_QUERY', '')[m
[31m-                client_context = app.config.get('LAST_CLIENT_CONTEXT', '')[m
[31m-                project_context = app.config.get('LAST_PROJECT_CONTEXT', '')[m
[31m-                prompt = app.config.get('LAST_PROMPT', '')[m
[31m-                uploaded_context = app.config.get('LAST_UPLOADED_CONTEXT', '')[m
[31m-[m
[31m-                def run_agent():[m
[31m-                    try:[m
[31m-                        agent.run_agent_with_query([m
[31m-                            search_query=search_query,[m
[31m-                            date_range=date_range,[m
[31m-                            package_ids=package_ids,[m
[31m-                            client_context=client_context,[m
[31m-                            project_context=project_context,[m
[31m-                            prompt=prompt,[m
[31m-                            uploaded_context=uploaded_context,[m
[31m-                            model=model_name,[m
[31m-                            stream_callback=stream_callback[m
[31m-                        )[m
[31m-                        # Signal completion[m
[31m-                        chunk_queue.put(None)[m
[31m-                    except Exception as e:[m
[31m-                        logger.error(f"Thread error: {e}")[m
[31m-                        chunk_queue.put(f"Error: {str(e)}")[m
[31m-                        chunk_queue.put(None)[m
[31m-[m
[31m-                # Start the agent thread[m
[31m-                agent_thread = threading.Thread(target=run_agent)[m
[31m-                agent_thread.daemon = True[m
[31m-                agent_thread.start()[m
[31m-[m
[31m-                # Stream chunks as they become available[m
[31m-                while True:[m
[31m-                    try:[m
[31m-                        # Use a timeout to avoid blocking forever[m
[31m-                        chunk = chunk_queue.get(timeout=1.0)[m
[31m-                        if chunk is None:  # End signal[m
[31m-                            break[m
[31m-                        yield f"data: {chunk}\n\n"[m
[31m-                    except Exception as e:[m
[31m-                        # Check if the thread is still alive[m
[31m-                        if not agent_thread.is_alive():[m
[31m-                            logger.error(f"Agent thread died: {e}")[m
[31m-                            break[m
[31m-                        # Otherwise continue waiting[m
[31m-                        continue[m
[31m-[m
[31m-                # Signal the end of the stream[m
[31m-                yield "data: [DONE]\n\n"[m
[31m-[m
[31m-            except Exception as e:[m
[31m-                logger.error(f"Error in streaming: {e}")[m
[31m-                yield f"data: Error in streaming: {str(e)}\n\n"[m
[31m-                yield "data: [DONE]\n\n"[m
[31m-[m
[31m-        return Response(generate_stream_content(), mimetype='text/event-stream')[m
[31m-    else:[m
[31m-        # This is the form submission[m
[31m-        # Get form data[m
[31m-        model_name = request.form.get('model_name', 'gpt-4o')[m
[31m-        date_range = request.form.get('date_range', 'all')[m
[31m-        package_ids = request.form.getlist('package_ids')[m
[31m-        search_query = request.form.get('search_query', '')[m
[31m-        client_context = request.form.get('client_context', '')[m
[31m-        project_context = request.form.get('project_context', '')[m
[31m-        prompt = request.form.get('prompt', '')[m
[31m-[m
[31m-        # Process uploaded files[m
[31m-        uploaded_context = ""[m
[31m-        if 'context_files' in request.files:[m
[31m-            files = request.files.getlist('context_files')[m
[31m-            for file in files:[m
[31m-                if file and file.filename and allowed_file(file.filename):[m
[31m-                    file_text = extract_text_from_file(file)[m
[31m-                    if file_text:[m
[31m-                        uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"[m
[31m-[m
[31m-        # Store the form data in the app config for the SSE connection to use[m
[31m-        app.config['LAST_MODEL'] = model_name[m
[31m-        app.config['LAST_DATE_RANGE'] = date_range[m
[31m-        app.config['LAST_PACKAGE_IDS'] = package_ids[m
[31m-        app.config['LAST_SEARCH_QUERY'] = search_query[m
[31m-        app.config['LAST_CLIENT_CONTEXT'] = client_context[m
[31m-        app.config['LAST_PROJECT_CONTEXT'] = project_context[m
[31m-        app.config['LAST_PROMPT'] = prompt[m
[31m-        app.config['LAST_UPLOADED_CONTEXT'] = uploaded_context[m
[31m-[m
[31m-        # Return a success response[m
[31m-        return jsonify({"status": "success"})[m
[31m-[m
[31m-@app.route('/generate', methods=['POST'])[m
[31m-def generate():[m
[31m-    """Generate newsletter content using the agent"""[m
[31m-    global agent[m
[31m-[m
[31m-    if agent is None:[m
[31m-        init_agent()[m
[31m-[m
[31m-    if agent is None:[m
[31m-        return jsonify({"error": "Failed to initialize newsletter agent"}), 500[m
[31m-[m
[31m-    try:[m
[31m-        # Get form data[m
[31m-        model_name = request.form.get('model_name', 'gpt-4o')[m
[31m-        date_range = request.form.get('date_range', 'all')[m
[31m-        package_ids = request.form.getlist('package_ids')[m
[31m-        search_query = request.form.get('search_query', '')[m
[31m-        client_context = request.form.get('client_context', '')[m
[31m-        project_context = request.form.get('project_context', '')[m
[31m-        prompt = request.form.get('prompt', '')[m
[31m-[m
[31m-        # Process uploaded files[m
[31m-        uploaded_context = ""[m
[31m-        if 'context_files' in request.files:[m
[31m-            files = request.files.getlist('context_files')[m
[31m-            for file in files:[m
[31m-                if file and file.filename and allowed_file(file.filename):[m
[31m-                    file_text = extract_text_from_file(file)[m
[31m-                    if file_text:[m
[31m-                        uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"[m
[31m-[m
[31m-        # Run the agent[m
[31m-        result = agent.run_agent_with_query([m
[31m-            search_query=search_query,[m
[31m-            date_range=date_range,[m
[31m-            package_ids=package_ids,[m
[31m-            client_context=client_context,[m
[31m-            project_context=project_context,[m
[31m-            prompt=prompt,[m
[31m-            uploaded_context=uploaded_context,[m
[31m-            model=model_name[m
[31m-        )[m
[31m-[m
[31m-        return jsonify(result)[m
[31m-[m
[31m-    except Exception as e:[m
[31m-        logger.error(f"Error generating content: {e}")[m
[31m-        return jsonify({"error": str(e)}), 500[m
[31m-[m
[31m-# Create templates directory and files[m
[31m-def create_templates():[m
[31m-    """Create Flask templates for the application"""[m
[31m-    # Create templates directory if it doesn't exist[m
[31m-    if not os.path.exists('templates'):[m
[31m-        os.makedirs('templates')[m
[31m-[m
[31m-    # Create base template[m
[31m-    with open('templates/base.html', 'w', encoding='utf-8') as f:[m
[31m-        f.write('''<!DOCTYPE html>[m
[31m-<html lang="en">[m
[31m-<head>[m
[31m-    <meta charset="UTF-8">[m
[31m-    <meta name="viewport" content="width=device-width, initial-scale=1.0">[m
[31m-    <title>{% block title %}Agent Newsletter Generator{% endblock %}</title>[m
[31m-    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">[m
[31m-    <style>[m
[31m-        body { padding-top: 20px; }[m
[31m-        .form-group { margin-bottom: 1rem; }[m
[31m-        #loading { display: none; }[m
[31m-        #result { display: none; }[m
[31m-        .card { margin-bottom: 20px; }[m
[31m-    </style>[m
[31m-</head>[m
[31m-<body>[m
[31m-    <div class="container">[m
[31m-        <header class="mb-4">[m
[31m-            <h1>{% block header %
</file>

<file path="deploy/newsletter_agent/wsgi.py">
"""
WSGI entry point for the Newsletter Agent application.
This file is used by Gunicorn to serve the application.
"""

from final_app import app

if __name__ == "__main__":
    app.run()
</file>

<file path="deploy/scraper/list_articles.py">
#!/usr/bin/env python3
"""
List most recent articles from the MongoDB database
"""
import os
import sys
import logging
import datetime
from bson.objectid import ObjectId

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Add path to import ProtonDB
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(os.path.dirname(current_dir))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Import ProtonDB for database operations
try:
    from proton_db_setup import ProtonDB
except ImportError:
    logger.error("Failed to import ProtonDB. Make sure the module is in your PYTHONPATH.")
    sys.exit(1)

def list_recent_articles(limit=20, package_filter=None):
    """List the most recent articles from the database"""
    db = ProtonDB()
    try:
        logger.info(f"Connected to database: {db.db.name}")
        
        # List available collections
        collections = db.db.list_collection_names()
        logger.info(f"Available collections: {', '.join(collections)}")
        
        # Check if articles collection exists
        if "articles" not in collections:
            logger.error("Articles collection not found in the database")
            return
        
        # Build query
        query = {}
        if package_filter:
            query["package_name"] = package_filter
        
        # Get recent articles, sorted by date scraped (newest first)
        articles = list(db.db.articles.find(query).sort("date_scraped", -1).limit(limit))
        
        if not articles:
            logger.info("No articles found matching the criteria")
            return
        
        logger.info(f"Found {len(articles)} recent articles")
        
        # Print article information
        print("\n" + "="*100)
        print(f"MOST RECENT {len(articles)} ARTICLES")
        print("="*100)
        
        for i, article in enumerate(articles, 1):
            title = article.get("title", "No title")
            source = article.get("source_name", "Unknown source")
            date_scraped = article.get("date_scraped", "Unknown date")
            date_str = date_scraped.strftime("%Y-%m-%d %H:%M:%S") if isinstance(date_scraped, datetime.datetime) else str(date_scraped)
            
            # Handle different relevance score formats
            relevance = 0
            if "relevance_score" in article:
                relevance = article["relevance_score"]
            elif "relevance_scores" in article and isinstance(article["relevance_scores"], dict):
                relevance = article["relevance_scores"].get("overall", 0)
                
            package_name = article.get("package_name", "")
            if not package_name and "scraping_package_id" in article:
                # Try to look up package name from the ID
                try:
                    package_id = article["scraping_package_id"]
                    package = db.db.scraping_packages.find_one({"_id": package_id})
                    if package:
                        package_name = package.get("name", "Unknown package")
                except:
                    pass
            
            print(f"{i}. {title}")
            print(f"   Source: {source}")
            print(f"   Package: {package_name}")
            print(f"   Date: {date_str}")
            print(f"   Relevance: {relevance:.2f}")
            
            # Print URL if available
            if article.get("url"):
                print(f"   URL: {article.get('url')}")
            
            # Print keywords if available
            if article.get("keywords"):
                keywords = ", ".join(article.get("keywords")[:8])  # Limit to first 8 keywords
                print(f"   Keywords: {keywords}")
            
            # Print content snippet
            content = article.get("content", "")
            if content:
                snippet = content[:150] + "..." if len(content) > 150 else content
                print(f"   Snippet: {snippet}")
            
            print("-"*100)
    except Exception as e:
        logger.error(f"Error listing articles: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Close MongoDB connection
        db.close()
        logger.info("MongoDB connection closed")

def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="List recent articles from MongoDB")
    parser.add_argument("--limit", type=int, default=20, help="Maximum number of articles to display")
    parser.add_argument("--package", type=str, help="Filter by package name")
    
    args = parser.parse_args()
    
    list_recent_articles(limit=args.limit, package_filter=args.package)

if __name__ == "__main__":
    main()
</file>

<file path="deploy/scraper/test_deployed.py">
#!/usr/bin/env python3
"""
Test script for the deployed scraper code

This script tests the NLP processing and vector embeddings functionality
to verify that everything is working correctly in the deployed code.
"""

import os
import sys
import logging
import time
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Make sure we're using the local modules
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import local modules
from vector_embeddings import EmbeddingGenerator
from run_scraper import extract_keywords, extract_entities, generate_summary

def test_nlp_processing():
    """Test NLP processing functionality"""
    logger.info("=" * 50)
    logger.info("TESTING NLP PROCESSING")
    logger.info("=" * 50)

    # Test text
    test_text = """
    Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction
    between computers and humans through natural language. The ultimate objective of NLP is to read, decipher,
    understand, and make sense of human language in a valuable way.

    NLP combines computational linguistics‚Äîrule-based modeling of human language‚Äîwith statistical, machine learning,
    and deep learning models. These technologies enable computers to process human language in the form of text or
    voice data and to 'understand' its full meaning, complete with the speaker or writer's intent and sentiment.

    NLP drives computer programs that translate text from one language to another, respond to spoken commands,
    and summarize large volumes of text rapidly‚Äîeven in real time. There's a good chance you've interacted with
    NLP in the form of voice-operated GPS systems, digital assistants, speech-to-text dictation software,
    customer service chatbots, and other consumer conveniences.
    """

    # Test package info
    package_name = "NLP Test Package"
    package_description = "Testing NLP processing capabilities"

    # Test keyword extraction
    logger.info("\nTesting keyword extraction:")
    try:
        start_time = time.time()
        keywords = extract_keywords(test_text, package_name, package_description)
        duration = time.time() - start_time

        logger.info(f"Keywords extracted in {duration:.2f} seconds")
        logger.info(f"Found {len(keywords)} keywords: {', '.join(keywords)}")
        logger.info("‚úÖ Keyword extraction test passed")
    except Exception as e:
        logger.error(f"‚ùå Keyword extraction test failed: {e}")

    # Test entity extraction
    logger.info("\nTesting entity extraction:")
    try:
        start_time = time.time()
        entities = extract_entities(test_text)
        duration = time.time() - start_time

        logger.info(f"Entities extracted in {duration:.2f} seconds")
        logger.info(f"Found {len(entities)} entities")
        for entity in entities[:5]:  # Show first 5 entities
            logger.info(f"  {entity['type']}: {entity['text']}")
        logger.info("‚úÖ Entity extraction test passed")
    except Exception as e:
        logger.error(f"‚ùå Entity extraction test failed: {e}")

    # Test summary generation
    logger.info("\nTesting summary generation:")
    try:
        start_time = time.time()
        fallback_summary = "This is a fallback summary."
        summary = generate_summary(test_text, fallback_summary)
        duration = time.time() - start_time

        logger.info(f"Summary generated in {duration:.2f} seconds")
        logger.info(f"Summary length: {len(summary)} characters")
        logger.info(f"Summary: {summary}")
        logger.info("‚úÖ Summary generation test passed")
    except Exception as e:
        logger.error(f"‚ùå Summary generation test failed: {e}")

def test_vector_embeddings():
    """Test vector embeddings functionality"""
    logger.info("\n" + "=" * 50)
    logger.info("TESTING VECTOR EMBEDDINGS")
    logger.info("=" * 50)

    test_texts = [
        "This is a test of the embedding system for Proton CRM.",
        "Vector embeddings are used for semantic search and content recommendation.",
        "Natural language processing helps extract meaning from unstructured text data."
    ]

    # Test OpenAI embeddings
    logger.info("\nTesting OpenAI embeddings:")
    try:
        openai_embedder = EmbeddingGenerator(model_type="openai")
        logger.info(f"Initialized OpenAI embedder (actual model type: {openai_embedder.model_type})")

        start_time = time.time()
        openai_embedding = openai_embedder.get_embedding(test_texts[0])
        duration = time.time() - start_time

        logger.info(f"OpenAI embedding generated in {duration:.2f} seconds")
        logger.info(f"Embedding dimension: {len(openai_embedding)}")
        logger.info(f"First 5 values: {openai_embedding[:5]}")

        # Test batch embeddings
        start_time = time.time()
        batch_embeddings = openai_embedder.batch_get_embeddings(test_texts)
        duration = time.time() - start_time

        logger.info(f"Batch embeddings ({len(batch_embeddings)}) generated in {duration:.2f} seconds")
        logger.info(f"All have same dimensions: {all(len(emb) == len(openai_embedding) for emb in batch_embeddings)}")

        logger.info("‚úÖ OpenAI embeddings test passed")
    except Exception as e:
        logger.error(f"‚ùå OpenAI embeddings test failed: {e}")

    # Test local embeddings
    logger.info("\nTesting local embeddings:")
    try:
        local_embedder = EmbeddingGenerator(model_type="local")
        logger.info(f"Initialized local embedder (model: {local_embedder.model_type})")

        start_time = time.time()
        local_embedding = local_embedder.get_embedding(test_texts[0])
        duration = time.time() - start_time

        logger.info(f"Local embedding generated in {duration:.2f} seconds")
        logger.info(f"Embedding dimension: {len(local_embedding)}")
        logger.info(f"First 5 values: {local_embedding[:5]}")

        # Test batch embeddings
        start_time = time.time()
        batch_embeddings = local_embedder.batch_get_embeddings(test_texts)
        duration = time.time() - start_time

        logger.info(f"Batch embeddings ({len(batch_embeddings)}) generated in {duration:.2f} seconds")
        logger.info(f"All have same dimensions: {all(len(emb) == len(local_embedding) for emb in batch_embeddings)}")

        logger.info("‚úÖ Local embeddings test passed")
    except Exception as e:
        logger.error(f"‚ùå Local embeddings test failed: {e}")

if __name__ == "__main__":
    # Test NLP processing
    test_nlp_processing()

    # Test vector embeddings
    test_vector_embeddings()

    logger.info("\n" + "=" * 50)
    logger.info("ALL TESTS COMPLETED")
    logger.info("=" * 50)
</file>

<file path="deploy/scraping_package_gui/proton_db_setup.py">
from pymongo import MongoClient
from pymongo.collection import Collection
from pymongo.database import Database
import pymongo
import datetime
from typing import Dict, List, Optional, Any, Union
import logging
import os
from dotenv import load_dotenv
import dns.resolver
import hashlib
from bson.objectid import ObjectId

# Configure DNS resolver explicitly
dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
dns.resolver.default_resolver.nameservers = ['8.8.8.8', '8.8.4.4']  # Use Google's public DNS

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProtonDB:
    """
    Class to handle MongoDB connection and operations for Proton CRM
    """
    
    def __init__(self, connection_string: Optional[str] = None):
        """
        Initialize the MongoDB connection
        
        Args:
            connection_string: MongoDB connection string. If None, will use MONGODB_URI env variable.
        """
        # Get connection string from environment variable if not provided
        if connection_string is None:
            connection_string = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
        
        # Connect to MongoDB
        try:
            # Add connection options to handle DNS issues
            self.client = MongoClient(
                connection_string,
                connectTimeoutMS=30000,
                socketTimeoutMS=360000,
                serverSelectionTimeoutMS=30000
            )
            
            # Test connection
            self.client.admin.command('ping')
            
            self.db = self.client["proton"]
            logger.info("Connected to MongoDB successfully")
            
            # Create and configure collections with schema validation
            self._setup_articles_collection()
            
        except Exception as e:
            logger.error(f"Failed to connect to MongoDB: {e}")
            raise
    
    def _setup_articles_collection(self) -> None:
        """
        Setup the articles collection with schema validation
        """
        # Define the article schema for validation
        article_schema = {
            "$jsonSchema": {
                "bsonType": "object",
                "required": ["title", "content", "source_url", "scraping_package_id", "date_scraped", "project_ids"],
                "properties": {
                    "title": {
                        "bsonType": "string",
                        "description": "Title of the article"
                    },
                    "content": {
                        "bsonType": "string",
                        "description": "Main content of the article"
                    },
                    "summary": {
                        "bsonType": "string",
                        "description": "AI-generated summary of the article content"
                    },
                    "source_url": {
                        "bsonType": "string",
                        "description": "URL where the article was scraped from"
                    },
                    "source_name": {
                        "bsonType": "string",
                        "description": "Name of the source website/feed"
                    },
                    "author": {
                        "bsonType": "string",
                        "description": "Author of the article"
                    },
                    "published_date": {
                        "bsonType": ["date", "null"],
                        "description": "Original publication date of the article"
                    },
                    "date_scraped": {
                        "bsonType": "date",
                        "description": "Date and time when the article was scraped"
                    },
                    "date_updated": {
                        "bsonType": "date",
                        "description": "Date and time when the article was last updated in the database"
                    },
                    "scraping_package_id": {
                        "bsonType": "objectId",
                        "description": "Reference to the scraping package that retrieved this article"
                    },
                    "project_ids": {
                        "bsonType": "array",
                        "description": "List of project IDs this article is associated with",
                        "items": {
                            "bsonType": "objectId"
                        }
                    },
                    "keywords": {
                        "bsonType": "array",
                        "description": "List of keywords extracted from the article",
                        "items": {
                            "bsonType": "string"
                        }
                    },
                    "entities": {
                        "bsonType": "array",
                        "description": "Named entities extracted from the content",
                        "items": {
                            "bsonType": "object",
                            "required": ["text", "type"],
                            "properties": {
                                "text": {
                                    "bsonType": "string",
                                    "description": "Entity text"
                                },
                                "type": {
                                    "bsonType": "string",
                                    "description": "Entity type (e.g., PERSON, ORGANIZATION, etc.)"
                                }
                            }
                        }
                    },
                    "relevance_scores": {
                        "bsonType": "object",
                        "description": "Scores indicating relevance to different criteria",
                        "properties": {
                            "overall": {
                                "bsonType": "double",
                                "description": "Overall relevance score (0-1)"
                            },
                            "recency": {
                                "bsonType": "double",
                                "description": "Recency score (0-1)"
                            },
                            "persona_specific": {
                                "bsonType": "object",
                                "description": "Relevance scores specific to different personas",
                                "additionalProperties": True
                            }
                        }
                    },
                    "vector_embedding": {
                        "bsonType": "array",
                        "description": "Vector embedding of the article content for semantic search",
                        "items": {
                            "bsonType": "double"
                        }
                    },
                    "metadata": {
                        "bsonType": "object",
                        "description": "Additional metadata about the article",
                        "additionalProperties": True
                    },
                    "content_type": {
                        "bsonType": "string",
                        "description": "Type of content (e.g., news, blog, press release)",
                        "enum": ["news", "blog", "press_release", "research", "social_media", "other"]
                    },
                    "language": {
                        "bsonType": "string",
                        "description": "Language of the article content"
                    },
                    "tags": {
                        "bsonType": "array",
                        "description": "Custom tags assigned to the article",
                        "items": {
                            "bsonType": "string"
                        }
                    },
                    "used_in_newsletters": {
                        "bsonType": "array",
                        "description": "List of newsletter IDs this article has been used in",
                        "items": {
                            "bsonType": "objectId"
                        }
                    },
                    "sentiment": {
                        "bsonType": "object",
                        "description": "Sentiment analysis of the article content",
                        "properties": {
                            "score": {
                                "bsonType": "double",
                                "description": "Sentiment score from -1 (negative) to 1 (positive)"
                            },
                            "magnitude": {
                                "bsonType": "double",
                                "description": "Magnitude of the sentiment (intensity)"
                            }
                        }
                    },
                    "status": {
                        "bsonType": "string",
                        "description": "Status of the article in the system",
                        "enum": ["active", "archived", "flagged"]
                    },
                    "image_url": {
                        "bsonType": "string",
                        "description": "URL of the main image associated with the article"
                    }
                }
            }
        }
        
        # Create or update the articles collection with schema validation
        try:
            # Check if we're using MongoDB Atlas (look for "+srv" in connection string)
            is_atlas = "+srv" in os.getenv("MONGODB_URI", "")
            
            # Check if collection exists
            collection_names = self.db.list_collection_names()
            
            if "articles" not in collection_names:
                # Create new collection - if on Atlas, create without validation to avoid permission issues
                if is_atlas:
                    self.db.create_collection("articles")
                    logger.info("Created articles collection without schema validation (Atlas detected)")
                else:
                    # For self-hosted MongoDB, use validation
                    self.db.create_collection("articles", validator=article_schema)
                    logger.info("Created articles collection with schema validation")
            else:
                # Collection exists, only try to update schema if not on Atlas
                if not is_atlas:
                    try:
                        self.db.command("collMod", "articles", validator=article_schema)
                        logger.info("Updated articles collection schema validation")
                    except Exception as e:
                        logger.warning(f"Could not update schema validation: {e}")
                        logger.warning("Continuing without schema validation")
            
            # Create indexes for efficient querying
            articles_collection = self.db["articles"]
            
            # Create indexes regardless of Atlas or self-hosted
            try:
                articles_collection.create_index("source_url", unique=True)
                articles_collection.create_index("date_scraped")
                articles_collection.create_index("project_ids")
                articles_collection.create_index("scraping_package_id")
                articles_collection.create_index("keywords")
                articles_collection.create_index("content_type")
                articles_collection.create_index("status")
                articles_collection.create_index([("title", pymongo.TEXT), ("content", pymongo.TEXT)], name="text_search_index")
                
                logger.info("Created indexes for articles collection")
            except Exception as e:
                logger.warning(f"Could not create all indexes: {e}")
                logger.warning("Some queries may be slower than expected")
            
            # Create vector search index if enabled and supported by the server
            vector_search_enabled = os.getenv("MONGODB_VECTOR_SEARCH_ENABLED", "False").lower() in ("true", "1", "yes")
            if vector_search_enabled and is_atlas:
                try:
                    articles_collection.create_index([("vector_embedding", "2dsphere")], name="vector_search_index")
                    logger.info("Created vector search index")
                except Exception as e:
                    logger.warning(f"Could not create vector search index: {e}")
                    logger.warning("Vector search may not be available without MongoDB Atlas Vector Search")
            
        except Exception as e:
            logger.error(f"Failed to setup articles collection: {e}")
            logger.warning("Continuing without schema validation or all indexes")
    
    def insert_article(self, article_data):
        """
        Insert a new article into the database

        Args:
            article_data (dict): Article data

        Returns:
            str: ID of the inserted article
        """
        try:
            # Check if an article with this URL already exists
            existing = self.db.articles.find_one({"source_url": article_data["source_url"]})
            if existing:
                logger.info(f"Article with URL {article_data['source_url']} already exists in the database (ID: {existing['_id']})")
                return str(existing["_id"])

            # Set default fields if not provided
            if "date_scraped" not in article_data:
                article_data["date_scraped"] = datetime.datetime.utcnow()
            
            if "content_type" not in article_data:
                article_data["content_type"] = "news"
                
            if "status" not in article_data:
                article_data["status"] = "active"
                
            if "relevance_scores" not in article_data:
                # Default relevance score
                article_data["relevance_scores"] = {"overall": 0.5}
            
            # Calculate a content hash for deduplication checks
            content_to_hash = f"{article_data.get('title', '')}{article_data.get('content', '')}"
            article_data["content_hash"] = hashlib.md5(content_to_hash.encode()).hexdigest()
            
            # Store relationship to scraping package if available
            if "scraping_package_id" in article_data and not isinstance(article_data["scraping_package_id"], ObjectId):
                try:
                    article_data["scraping_package_id"] = ObjectId(article_data["scraping_package_id"])
                except Exception as e:
                    logger.warning(f"Invalid scraping_package_id format: {e}")
                    # Keep it as is if conversion fails
            
            # Ensure we capture package name if available
            if "scraping_package_name" not in article_data and "scraping_package_id" in article_data:
                # Try to look up package name from database
                try:
                    package = self.db.scraping_packages.find_one({"_id": article_data["scraping_package_id"]})
                    if package and "name" in package:
                        article_data["scraping_package_name"] = package["name"]
                except Exception as e:
                    logger.warning(f"Failed to look up package name: {e}")
            
            # Insert the article
            result = self.db.articles.insert_one(article_data)
            logger.info(f"Inserted article '{article_data.get('title', 'Untitled')}' with ID {result.inserted_id}")
            
            # Also update package statistics if package ID is provided
            if "scraping_package_id" in article_data:
                try:
                    self.db.scraping_packages.update_one(
                        {"_id": article_data["scraping_package_id"]},
                        {
                            "$inc": {"article_count": 1, "articles_last_run": 1},
                            "$set": {"last_run": datetime.datetime.utcnow()}
                        }
                    )
                except Exception as e:
                    logger.warning(f"Failed to update package statistics: {e}")
            
            return str(result.inserted_id)
        except Exception as e:
            logger.error(f"Failed to insert article: {e}")
            return None

    def get_article_by_id(self, article_id: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve an article by its ID
        
        Args:
            article_id: ID of the article to retrieve
            
        Returns:
            Optional[Dict[str, Any]]: Article data or None if not found
        """
        try:
            article = self.db.articles.find_one({"_id": ObjectId(article_id)})
            return article
        except Exception as e:
            logger.error(f"Failed to retrieve article: {e}")
            return None

    def get_articles_for_project(self, project_id: str, limit: int = 100, skip: int = 0) -> List[Dict[str, Any]]:
        """
        Retrieve articles associated with a specific project
        
        Args:
            project_id: ID of the project
            limit: Maximum number of articles to retrieve
            skip: Number of articles to skip (for pagination)
            
        Returns:
            List[Dict[str, Any]]: List of article data
        """
        try:
            articles = list(self.db.articles.find(
                {"project_ids": ObjectId(project_id), "status": "active"}
            ).sort("date_scraped", -1).skip(skip).limit(limit))
            return articles
        except Exception as e:
            logger.error(f"Failed to retrieve articles for project: {e}")
            return []

    def get_articles_by_scraping_package(self, package_id: str, limit: int = 100, skip: int = 0) -> List[Dict[str, Any]]:
        """
        Retrieve articles collected by a specific scraping package
        
        Args:
            package_id: ID of the scraping package
            limit: Maximum number of articles to retrieve
            skip: Number of articles to skip (for pagination)
            
        Returns:
            List[Dict[str, Any]]: List of article data
        """
        try:
            articles = list(self.db.articles.find(
                {"scraping_package_id": ObjectId(package_id), "status": "active"}
            ).sort("date_scraped", -1).skip(skip).limit(limit))
            return articles
        except Exception as e:
            logger.error(f"Failed to retrieve articles for scraping package: {e}")
            return []

    def update_article(self, article_id: str, updates: Dict[str, Any]) -> bool:
        """
        Update an existing article
        
        Args:
            article_id: ID of the article to update
            updates: Dictionary containing fields to update
            
        Returns:
            bool: True if update was successful, False otherwise
        """
        try:
            # Add update timestamp
            updates["date_updated"] = datetime.datetime.utcnow()
            
            result = self.db.articles.update_one(
                {"_id": ObjectId(article_id)},
                {"$set": updates}
            )
            
            return result.modified_count > 0
        except Exception as e:
            logger.error(f"Failed to update article: {e}")
            return False

    def search_articles(self, query: str, project_id: Optional[str] = None, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Search articles using text search
        
        Args:
            query: Search query string
            project_id: Optional project ID to restrict search
            limit: Maximum number of results
            
        Returns:
            List[Dict[str, Any]]: List of matching articles
        """
        try:
            search_filter = {"$text": {"$search": query}, "status": "active"}
            
            # Add project filter if specified
            if project_id:
                search_filter["project_ids"] = ObjectId(project_id)
            
            articles = list(self.db.articles.find(
                search_filter,
                {"score": {"$meta": "textScore"}}
            ).sort([("score", {"$meta": "textScore"})]).limit(limit))
            
            return articles
        except Exception as e:
            logger.error(f"Failed to search articles: {e}")
            return []

    def vector_search(self, embedding: List[float], project_id: Optional[str] = None, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Search articles using vector similarity
        Note: Requires MongoDB Atlas Vector Search
        
        Args:
            embedding: Vector embedding to search for similar articles
            project_id: Optional project ID to restrict search
            limit: Maximum number of results
            
        Returns:
            List[Dict[str, Any]]: List of matching articles
        """
        try:
            # Define the search pipeline
            pipeline = [
                {
                    "$search": {
                        "index": "vector_search_index",
                        "knnBeta": {
                            "vector": embedding,
                            "path": "vector_embedding",
                            "k": limit
                        }
                    }
                },
                {"$match": {"status": "active"}}
            ]
            
            # Add project filter if specified
            if project_id:
                pipeline.append({"$match": {"project_ids": ObjectId(project_id)}})
            
            pipeline.append({"$limit": limit})
            
            # Execute the aggregation pipeline
            articles = list(self.db.articles.aggregate(pipeline))
            return articles
            
        except Exception as e:
            logger.error(f"Failed to perform vector search: {e}")
            logger.error("Note: Vector search requires MongoDB Atlas with Vector Search enabled")
            return []

    def mark_article_used_in_newsletter(self, article_id: str, newsletter_id: str) -> bool:
        """
        Mark an article as used in a newsletter
        
        Args:
            article_id: ID of the article
            newsletter_id: ID of the newsletter
            
        Returns:
            bool: True if update was successful, False otherwise
        """
        try:
            result = self.db.articles.update_one(
                {"_id": ObjectId(article_id)},
                {
                    "$addToSet": {"used_in_newsletters": ObjectId(newsletter_id)},
                    "$set": {"date_updated": datetime.datetime.utcnow()}
                }
            )
            return result.modified_count > 0
        except Exception as e:
            logger.error(f"Failed to mark article as used in newsletter: {e}")
            return False

    def close(self) -> None:
        """
        Close the MongoDB connection
        """
        if hasattr(self, 'client'):
            self.client.close()
            logger.info("MongoDB connection closed")


if __name__ == "__main__":
    # Example usage
    try:
        db = ProtonDB()
        logger.info("ProtonDB initialized successfully")
        
        # Example: Insert a test article
        test_article = {
            "title": "Test Article",
            "content": "This is a test article for the Proton CRM system.",
            "source_url": "https://example.com/test-article",
            "source_name": "Example News",
            "scraping_package_id": ObjectId(),  # Replace with actual scraping package ID
            "project_ids": [ObjectId()],  # Replace with actual project ID
            "date_scraped": datetime.datetime.utcnow(),
            "content_type": "news",
            "keywords": ["test", "proton", "example"]
        }
        
        article_id = db.insert_article(test_article)
        logger.info(f"Test article inserted with ID: {article_id}")
        
        # Close the connection
        db.close()
        
    except Exception as e:
        logger.error(f"Error in example usage: {e}")
</file>

<file path="deploy/scraping_package_gui/requirements.txt">
pymongo==4.5.0
dnspython==2.4.2
python-dotenv==1.0.0
tkinter
</file>

<file path="deploy/scraping_package_gui/run_scraping_system.py">
"""
Proton CRM Scraping System Runner

This script provides a command-line interface for running the scraping system:
- List all scraping packages
- Create a new test scraping package
- Run all active scraping packages once
- Run in scheduler mode
"""

import os
import sys
import time
import datetime
import logging
import argparse
from typing import Dict, Any, List, Optional
from bson.objectid import ObjectId

# Import ProtonDB for database operations
from proton_db_setup import ProtonDB

# Import scraping package modules
from scraping_package_model import ScrapingPackageModel
from scraping_package_scheduler import ScrapingPackage, ScrapingScheduler

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def list_packages():
    """List all scraping packages in the database"""
    db = ProtonDB()
    try:
        packages = list(db.db.scraping_packages.find())
        
        if not packages:
            print("No scraping packages found in the database")
            return
            
        print(f"Found {len(packages)} scraping package(s):")
        for idx, package in enumerate(packages, 1):
            print(f"\n{idx}. {package.get('name', 'Unnamed package')} (ID: {package['_id']})")
            print(f"   Description: {package.get('description', 'No description')}")
            print(f"   Feed count: {len(package.get('rss_feeds', []))}")
            print(f"   Schedule: {package.get('schedule_interval', 'Unknown')}")
            print(f"   Status: {package.get('status', 'Unknown')}")
            print(f"   Article count: {package.get('article_count', 0)}")
            
            # Show last run info if available
            if package.get('last_run'):
                # Format the datetime
                last_run = package['last_run']
                last_run_str = last_run.strftime("%Y-%m-%d %H:%M:%S") if hasattr(last_run, 'strftime') else str(last_run)
                print(f"   Last run: {last_run_str} ({package.get('articles_last_run', 0)} articles)")
    except Exception as e:
        logger.error(f"Error listing packages: {e}")
    finally:
        db.close()

def create_test_package():
    """Create a test scraping package"""
    package_model = ScrapingPackageModel()
    
    try:
        # Define a test package
        package_data = {
            "name": "Tech News Package",
            "description": "Combines tech news from The Verge, TechCrunch, and Wired",
            "project_ids": [ObjectId()],  # Generate a random project ID
            "rss_feeds": [
                "https://www.theverge.com/rss/index.xml",
                "https://feeds.feedburner.com/TechCrunch/",
                "https://www.wired.com/feed/rss"
            ],
            "schedule_interval": "1h",
            "max_articles_per_run": 5,
            "status": "active",
            "calculate_embeddings": True
        }
        
        # Create the package
        package_id = package_model.create_package(package_data)
        
        if package_id:
            print(f"Created test scraping package with ID: {package_id}")
            return package_id
        else:
            print("Failed to create test scraping package")
            return None
    except Exception as e:
        logger.error(f"Error creating test package: {e}")
        return None
    finally:
        package_model.close()

def load_packages() -> List[ScrapingPackage]:
    """
    Load all active scraping packages from the database
    
    Returns:
        List[ScrapingPackage]: List of active scraping package objects
    """
    db = ProtonDB()
    packages = []
    
    try:
        # Query all active packages
        package_docs = list(db.db.scraping_packages.find({"status": "active"}))
        
        if not package_docs:
            logger.info("No active scraping packages found in the database")
            return []
            
        logger.info(f"Loading {len(package_docs)} active scraping packages")
        
        # Create ScrapingPackage objects
        for package_doc in package_docs:
            try:
                package = ScrapingPackage(
                    package_id=str(package_doc["_id"]),
                    project_ids=[str(pid) for pid in package_doc.get("project_ids", [])],
                    name=package_doc.get("name", "Unnamed package"),
                    description=package_doc.get("description", ""),
                    rss_feeds=package_doc.get("rss_feeds", []),
                    schedule_interval=package_doc.get("schedule_interval", "1h"),
                    max_articles_per_run=package_doc.get("max_articles_per_run", 10),
                    calculate_embeddings=package_doc.get("calculate_embeddings", True)
                )
                packages.append(package)
            except Exception as e:
                logger.error(f"Error loading package {package_doc.get('_id')}: {e}")
        
        return packages
    except Exception as e:
        logger.error(f"Error loading packages: {e}")
        return []
    finally:
        db.close()

def update_package_stats(package_id: str, articles_processed: int) -> None:
    """
    Update package statistics after a run
    
    Args:
        package_id: ID of the package
        articles_processed: Number of articles processed in this run
    """
    db = ProtonDB()
    try:
        db.db.scraping_packages.update_one(
            {"_id": ObjectId(package_id)},
            {
                "$inc": {"article_count": articles_processed, "articles_last_run": articles_processed},
                "$set": {"last_run": datetime.datetime.utcnow()}
            }
        )
    except Exception as e:
        logger.error(f"Error updating package stats: {e}")
    finally:
        db.close()

def run_packages_once() -> None:
    """Run all active scraping packages once"""
    packages = load_packages()
    
    if not packages:
        print("No active packages to run")
        return
        
    print(f"Running {len(packages)} active scraping package(s)")
    
    for package in packages:
        try:
            print(f"Running package: {package.name}")
            article_ids = package.run()
            print(f"Processed {len(article_ids)} articles from {package.name}")
            
            # Update package statistics
            update_package_stats(package.package_id, len(article_ids))
        except Exception as e:
            logger.error(f"Error running package {package.name}: {e}")

def run_scheduler() -> None:
    """Run the scraping scheduler (continuous mode)"""
    packages = load_packages()
    
    if not packages:
        print("No active packages to run in scheduler mode")
        return
        
    print(f"Starting scheduler with {len(packages)} active package(s)")
    
    # Initialize scheduler
    scheduler = ScrapingScheduler()
    
    # Add packages to scheduler
    for package in packages:
        scheduler.add_package(package)
    
    # Start the scheduler
    scheduler.start()
    
    # Run all packages immediately
    for package in packages:
        scheduler.run_package_now(package.package_id)
    
    # Keep running until interrupted
    try:
        print("Scheduler running. Press Ctrl+C to stop...")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("Stopping scheduler...")
        scheduler.stop()
        print("Scheduler stopped.")

def refresh_relevance_scores(package_id=None):
    """
    Refresh relevance scores for all articles or for a specific package
    
    Args:
        package_id: Optional package ID to refresh scores for only articles from this package
    """
    db = ProtonDB()
    try:
        # Build the query
        query = {}
        if package_id:
            query["scraping_package_id"] = ObjectId(package_id)
            package_info = db.db.scraping_packages.find_one({"_id": ObjectId(package_id)})
            if not package_info:
                print(f"Package with ID {package_id} not found")
                return
            print(f"Refreshing relevance scores for articles from package: {package_info.get('name', 'Unknown')}")
        else:
            print("Refreshing relevance scores for all articles")
        
        # Get all articles matching the query
        articles = list(db.db.articles.find(query))
        print(f"Found {len(articles)} articles to update")
        
        # Process each article
        updated_count = 0
        for article in articles:
            try:
                # Skip articles without necessary fields
                if not article.get("content") or not article.get("scraping_package_id"):
                    continue
                
                # Get package information
                package_info = db.db.scraping_packages.find_one({"_id": article["scraping_package_id"]})
                if not package_info:
                    continue
                
                # Calculate new scores
                new_scores = calculate_article_relevance(
                    article, 
                    package_name=package_info.get("name", ""),
                    package_description=package_info.get("description", "")
                )
                
                # Update the article
                db.db.articles.update_one(
                    {"_id": article["_id"]},
                    {"$set": {"relevance_scores": new_scores}}
                )
                updated_count += 1
                
                if updated_count % 10 == 0:
                    print(f"Updated {updated_count}/{len(articles)} articles")
                
            except Exception as e:
                logger.error(f"Error updating article {article.get('_id')}: {e}")
        
        print(f"Successfully updated relevance scores for {updated_count} articles")
        
    except Exception as e:
        logger.error(f"Error refreshing relevance scores: {e}")
    finally:
        db.close()

def calculate_article_relevance(article, package_name, package_description):
    """
    Calculate relevance scores for an article based on package information
    
    Args:
        article: Article document
        package_name: Name of the scraping package
        package_description: Description of the scraping package
        
    Returns:
        Dict: Updated relevance scores
    """
    import datetime
    import numpy as np
    
    scores = {
        "overall": 0.75,  # Default score
        "recency": 0.0,   # Will be calculated based on publish date
        "package_fit": 0.8  # Default package fit score
    }
    
    # Calculate recency score (1.0 for today, decreasing with age)
    if "published_date" in article and article["published_date"]:
        now = datetime.datetime.now()
        age_days = (now - article["published_date"]).days
        
        # Exponential decay: score = exp(-age_days/14)
        # This gives ~0.95 for today, ~0.5 for 2 weeks old, ~0.25 for 1 month old
        recency = np.exp(-age_days/14) if age_days >= 0 else 0.95
        scores["recency"] = min(1.0, max(0.0, recency))
    else:
        # If no date, assume it's not very recent
        scores["recency"] = 0.5
    
    # Calculate package fit score (if we have content to analyze)
    content_to_check = article.get("original_content", article.get("content", ""))
    if content_to_check:
        # Look for package name and description keywords in the content
        package_keywords = set()
        
        # Add package name words
        for word in package_name.lower().split():
            if len(word) > 3:  # Only consider longer words
                package_keywords.add(word)
        
        # Add description words
        if package_description:
            for word in package_description.lower().split():
                if len(word) > 3:  # Only consider longer words
                    package_keywords.add(word)
        
        # Count matches in content
        content_lower = content_to_check.lower()
        match_count = sum(1 for word in package_keywords if word in content_lower)
        
        # Calculate package fit score
        if package_keywords:
            # Score is ratio of matched keywords to total keywords, with a minimum of 0.5
            package_fit = max(0.5, min(1.0, match_count / len(package_keywords)))
            scores["package_fit"] = package_fit
    
    # Adjust overall score based on recency and package fit
    scores["overall"] = 0.3 * scores["recency"] + 0.7 * scores["package_fit"]
    
    # Add persona-specific scores (placeholder for now)
    scores["persona_specific"] = {
        "general": scores["overall"]  # Default persona
    }
    
    return scores

def main():
    # Define command-line arguments
    parser = argparse.ArgumentParser(description="Proton CRM Scraping System Runner")
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    
    # List command
    list_parser = subparsers.add_parser("list", help="List all scraping packages")
    
    # Create command
    create_parser = subparsers.add_parser("create", help="Create a test scraping package")
    
    # Run command
    run_parser = subparsers.add_parser("run", help="Run scraping packages")
    run_parser.add_argument("--scheduler", action="store_true", help="Run in scheduler mode")
    
    # Refresh command
    refresh_parser = subparsers.add_parser("refresh", help="Refresh article relevance scores")
    refresh_parser.add_argument("--package", type=str, help="Optional package ID to refresh only articles from this package")
    
    # Parse arguments
    args = parser.parse_args()
    
    # Handle commands
    if args.command == "list":
        list_packages()
    elif args.command == "create":
        create_test_package()
    elif args.command == "run":
        if args.scheduler:
            run_scheduler()
        else:
            run_packages_once()
    elif args.command == "refresh":
        refresh_relevance_scores(args.package)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
</file>

<file path="deploy/scraping_package_gui/scraping_package_gui.py">
"""
Scraping Package GUI for Proton CRM

This module provides a GUI for creating and managing scraping packages.
Users can set up packages by specifying name, description, RSS feeds, and other configuration options.
"""

import os
import sys
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import threading
import datetime
from typing import List, Dict, Any
from bson.objectid import ObjectId

# Fix path issues by adding the current directory to the path
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# Import modules
from scraping_package_model import ScrapingPackageModel
from run_scraping_system import run_packages_once

# Pre-import modules needed for running packages to verify they're available
try:
    from scraping_package_scheduler import ScrapingPackage
    package_module_loaded = True
except ImportError:
    package_module_loaded = False
    print("WARNING: Failed to import ScrapingPackage module - running tests will not work")

class ScrapingPackageGUI:
    """GUI application for creating and managing scraping packages"""
    
    def __init__(self, root):
        """Initialize the GUI application"""
        self.root = root
        self.root.title("Proton CRM - Scraping Package Manager")
        self.root.geometry("800x700")
        self.root.minsize(800, 700)
        
        # Set up GUI components
        self.setup_ui()
        
        # Initialize database connection
        self.db = ScrapingPackageModel()
        
        # Load existing packages
        self.load_packages()
    
    def setup_ui(self):
        """Set up the user interface"""
        # Create main notebook (tab control)
        self.notebook = ttk.Notebook(self.root)
        self.notebook.pack(fill='both', expand=True, padx=10, pady=10)
        
        # Create tabs
        self.create_tab = ttk.Frame(self.notebook)
        self.manage_tab = ttk.Frame(self.notebook)
        self.edit_tab = ttk.Frame(self.notebook)
        self.test_tab = ttk.Frame(self.notebook)
        
        self.notebook.add(self.create_tab, text="Create Package")
        self.notebook.add(self.manage_tab, text="Manage Packages")
        self.notebook.add(self.edit_tab, text="Edit Package")
        self.notebook.add(self.test_tab, text="Test & Run")
        
        # Set up each tab
        self.setup_create_tab()
        self.setup_manage_tab()
        self.setup_edit_tab()
        self.setup_test_tab()
        
        # Status bar at the bottom
        self.status_var = tk.StringVar()
        self.status_var.set("Ready")
        status_bar = ttk.Label(self.root, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)
        status_bar.pack(side=tk.BOTTOM, fill=tk.X)
    
    def setup_create_tab(self):
        """Set up the Create Package tab"""
        # Create a frame with padding
        frame = ttk.Frame(self.create_tab, padding="10")
        frame.pack(fill='both', expand=True)
        
        # Package information
        ttk.Label(frame, text="Package Name:").grid(column=0, row=0, sticky=tk.W, pady=5)
        self.name_var = tk.StringVar()
        ttk.Entry(frame, textvariable=self.name_var, width=50).grid(column=1, row=0, sticky=tk.W, pady=5)
        
        ttk.Label(frame, text="Description:").grid(column=0, row=1, sticky=tk.W, pady=5)
        self.desc_var = tk.StringVar()
        ttk.Entry(frame, textvariable=self.desc_var, width=50).grid(column=1, row=1, sticky=tk.W, pady=5)
        
        # RSS Feeds section
        ttk.Label(frame, text="RSS Feeds (one per line):").grid(column=0, row=2, sticky=tk.W, pady=5)
        self.feeds_text = scrolledtext.ScrolledText(frame, width=60, height=8)
        self.feeds_text.grid(column=0, row=3, columnspan=2, sticky=tk.W, pady=5)
        
        # Schedule settings
        ttk.Label(frame, text="Schedule Interval:").grid(column=0, row=4, sticky=tk.W, pady=5)
        self.interval_var = tk.StringVar(value="1h")
        interval_frame = ttk.Frame(frame)
        interval_frame.grid(column=1, row=4, sticky=tk.W, pady=5)
        
        ttk.Radiobutton(interval_frame, text="30 minutes", variable=self.interval_var, value="30m").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="1 hour", variable=self.interval_var, value="1h").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="6 hours", variable=self.interval_var, value="6h").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="12 hours", variable=self.interval_var, value="12h").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="1 day", variable=self.interval_var, value="1d").pack(side=tk.LEFT, padx=5)
        
        # Max articles per run
        ttk.Label(frame, text="Max Articles Per Run:").grid(column=0, row=5, sticky=tk.W, pady=5)
        self.max_articles_var = tk.StringVar(value="10")
        ttk.Spinbox(frame, from_=1, to=50, textvariable=self.max_articles_var, width=5).grid(column=1, row=5, sticky=tk.W, pady=5)
        
        # Keywords for filtering
        ttk.Label(frame, text="Include Keywords (comma separated):").grid(column=0, row=6, sticky=tk.W, pady=5)
        self.keywords_var = tk.StringVar()
        ttk.Entry(frame, textvariable=self.keywords_var, width=50).grid(column=1, row=6, sticky=tk.W, pady=5)
        
        # Processing options
        ttk.Label(frame, text="Processing Options:").grid(column=0, row=7, sticky=tk.W, pady=5)
        options_frame = ttk.Frame(frame)
        options_frame.grid(column=1, row=7, sticky=tk.W, pady=5)
        
        self.calc_embeddings_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="Calculate Vector Embeddings", variable=self.calc_embeddings_var).pack(anchor=tk.W)
        
        self.extract_entities_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="Extract Named Entities", variable=self.extract_entities_var).pack(anchor=tk.W)
        
        self.summarize_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="Generate Summaries", variable=self.summarize_var).pack(anchor=tk.W)
        
        # Projects section
        ttk.Label(frame, text="Associate with Project ID (optional):").grid(column=0, row=8, sticky=tk.W, pady=5)
        self.project_id_var = tk.StringVar()
        ttk.Entry(frame, textvariable=self.project_id_var, width=30).grid(column=1, row=8, sticky=tk.W, pady=5)
        
        # Create button
        create_button = ttk.Button(frame, text="Create Package", command=self.create_package)
        create_button.grid(column=1, row=9, sticky=tk.E, pady=20)
        
        # Clear button
        clear_button = ttk.Button(frame, text="Clear Form", command=self.clear_form)
        clear_button.grid(column=0, row=9, sticky=tk.W, pady=20)
    
    def setup_manage_tab(self):
        """Set up the Manage Packages tab"""
        # Create a frame with padding
        frame = ttk.Frame(self.manage_tab, padding="10")
        frame.pack(fill='both', expand=True)
        
        # Packages treeview
        ttk.Label(frame, text="Existing Packages:").grid(column=0, row=0, sticky=tk.W, pady=5)
        
        # Create treeview with scrollbar
        treeview_frame = ttk.Frame(frame)
        treeview_frame.grid(column=0, row=1, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.packages_treeview = ttk.Treeview(treeview_frame, columns=("name", "feeds", "schedule", "status", "last_run"), show="headings")
        self.packages_treeview.heading("name", text="Name")
        self.packages_treeview.heading("feeds", text="Feeds")
        self.packages_treeview.heading("schedule", text="Schedule")
        self.packages_treeview.heading("status", text="Status")
        self.packages_treeview.heading("last_run", text="Last Run")
        
        self.packages_treeview.column("name", width=150)
        self.packages_treeview.column("feeds", width=100)
        self.packages_treeview.column("schedule", width=100)
        self.packages_treeview.column("status", width=80)
        self.packages_treeview.column("last_run", width=150)
        
        scrollbar = ttk.Scrollbar(treeview_frame, orient=tk.VERTICAL, command=self.packages_treeview.yview)
        self.packages_treeview.configure(yscroll=scrollbar.set)
        
        self.packages_treeview.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # Package details section
        ttk.Label(frame, text="Package Details:").grid(column=0, row=2, sticky=tk.W, pady=5)
        self.details_text = scrolledtext.ScrolledText(frame, width=80, height=10, wrap=tk.WORD)
        self.details_text.grid(column=0, row=3, sticky=(tk.W, tk.E, tk.N, tk.S))
        self.details_text.config(state=tk.DISABLED)
        
        # Buttons for actions
        button_frame = ttk.Frame(frame)
        button_frame.grid(column=0, row=4, sticky=tk.E, pady=10)
        
        refresh_button = ttk.Button(button_frame, text="Refresh List", command=self.load_packages)
        refresh_button.pack(side=tk.LEFT, padx=5)
        
        view_button = ttk.Button(button_frame, text="View Details", command=self.view_package_details)
        view_button.pack(side=tk.LEFT, padx=5)
        
        pause_button = ttk.Button(button_frame, text="Pause/Activate", command=self.toggle_package_status)
        pause_button.pack(side=tk.LEFT, padx=5)
        
        delete_button = ttk.Button(button_frame, text="Delete Package", command=self.delete_package)
        delete_button.pack(side=tk.LEFT, padx=5)
        
        # Make treeview selection trigger details view
        self.packages_treeview.bind("<<TreeviewSelect>>", lambda e: self.view_package_details())
    
    def setup_edit_tab(self):
        """Set up the Edit Package tab"""
        # Create a frame with padding
        frame = ttk.Frame(self.edit_tab, padding="10")
        frame.pack(fill='both', expand=True)
        
        # Package selection section
        ttk.Label(frame, text="Select Package to Edit:").grid(column=0, row=0, sticky=tk.W, pady=5)
        self.edit_package_var = tk.StringVar()
        self.edit_package_combo = ttk.Combobox(frame, textvariable=self.edit_package_var, width=50, state="readonly")
        self.edit_package_combo.grid(column=1, row=0, sticky=tk.W, pady=5)
        
        # Load button
        load_button = ttk.Button(frame, text="Load Package", command=self.load_package_for_edit)
        load_button.grid(column=2, row=0, sticky=tk.W, pady=5, padx=5)
        
        # Package editing section
        edit_frame = ttk.LabelFrame(frame, text="Package Details", padding="10")
        edit_frame.grid(column=0, row=1, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=10)
        
        # Package information
        ttk.Label(edit_frame, text="Package Name:").grid(column=0, row=0, sticky=tk.W, pady=5)
        self.edit_name_var = tk.StringVar()
        ttk.Entry(edit_frame, textvariable=self.edit_name_var, width=50).grid(column=1, row=0, sticky=tk.W, pady=5)
        
        ttk.Label(edit_frame, text="Description:").grid(column=0, row=1, sticky=tk.W, pady=5)
        self.edit_desc_var = tk.StringVar()
        ttk.Entry(edit_frame, textvariable=self.edit_desc_var, width=50).grid(column=1, row=1, sticky=tk.W, pady=5)
        
        # RSS Feeds section
        ttk.Label(edit_frame, text="RSS Feeds (one per line):").grid(column=0, row=2, sticky=tk.W, pady=5)
        self.edit_feeds_text = scrolledtext.ScrolledText(edit_frame, width=60, height=8)
        self.edit_feeds_text.grid(column=0, row=3, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        # Schedule settings
        ttk.Label(edit_frame, text="Schedule Interval:").grid(column=0, row=4, sticky=tk.W, pady=5)
        self.edit_interval_var = tk.StringVar(value="1h")
        interval_frame = ttk.Frame(edit_frame)
        interval_frame.grid(column=1, row=4, sticky=tk.W, pady=5)
        
        ttk.Radiobutton(interval_frame, text="30 minutes", variable=self.edit_interval_var, value="30m").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="1 hour", variable=self.edit_interval_var, value="1h").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="6 hours", variable=self.edit_interval_var, value="6h").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="12 hours", variable=self.edit_interval_var, value="12h").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(interval_frame, text="1 day", variable=self.edit_interval_var, value="1d").pack(side=tk.LEFT, padx=5)
        
        # Max articles per run
        ttk.Label(edit_frame, text="Max Articles Per Run:").grid(column=0, row=5, sticky=tk.W, pady=5)
        self.edit_max_articles_var = tk.StringVar(value="10")
        ttk.Spinbox(edit_frame, from_=1, to=50, textvariable=self.edit_max_articles_var, width=5).grid(column=1, row=5, sticky=tk.W, pady=5)
        
        # Status selection
        ttk.Label(edit_frame, text="Status:").grid(column=0, row=6, sticky=tk.W, pady=5)
        self.edit_status_var = tk.StringVar(value="active")
        status_frame = ttk.Frame(edit_frame)
        status_frame.grid(column=1, row=6, sticky=tk.W, pady=5)
        
        ttk.Radiobutton(status_frame, text="Active", variable=self.edit_status_var, value="active").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(status_frame, text="Paused", variable=self.edit_status_var, value="paused").pack(side=tk.LEFT, padx=5)
        
        # Update button
        update_button = ttk.Button(edit_frame, text="Update Package", command=self.update_package)
        update_button.grid(column=1, row=7, sticky=tk.E, pady=20)
        
        # Store selected package ID
        self.selected_edit_package_id = None
    
    def setup_test_tab(self):
        """Set up the Test & Run tab"""
        # Create a frame with padding
        frame = ttk.Frame(self.test_tab, padding="10")
        frame.pack(fill='both', expand=True)
        
        # Package selection for testing
        ttk.Label(frame, text="Select Package to Test:").grid(column=0, row=0, sticky=tk.W, pady=5)
        self.test_package_var = tk.StringVar()
        self.test_package_combo = ttk.Combobox(frame, textvariable=self.test_package_var, width=50, state="readonly")
        self.test_package_combo.grid(column=1, row=0, sticky=tk.W, pady=5)
        
        # Run button
        run_button = ttk.Button(frame, text="Run Selected Package", command=self.run_package)
        run_button.grid(column=1, row=1, sticky=tk.W, pady=5)
        
        # Console output
        ttk.Label(frame, text="Console Output:").grid(column=0, row=2, sticky=tk.W, pady=5)
        self.console_text = scrolledtext.ScrolledText(frame, width=80, height=20, wrap=tk.WORD, bg="black", fg="white")
        self.console_text.grid(column=0, row=3, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S))
        self.console_text.config(state=tk.DISABLED)
        
        # Auto-refresh checkbox
        self.auto_refresh_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(frame, text="Auto-refresh package list", variable=self.auto_refresh_var).grid(column=0, row=4, sticky=tk.W, pady=5)
    
    def create_package(self):
        """Create a new scraping package with the specified settings"""
        try:
            # Validate input
            name = self.name_var.get().strip()
            if not name:
                messagebox.showerror("Error", "Package name is required")
                return
            
            feeds_text = self.feeds_text.get("1.0", tk.END).strip()
            if not feeds_text:
                messagebox.showerror("Error", "At least one RSS feed is required")
                return
                
            # Parse RSS feeds
            rss_feeds = [feed.strip() for feed in feeds_text.split("\n") if feed.strip()]
            
            # Parse keywords
            keywords_text = self.keywords_var.get().strip()
            keywords = [k.strip() for k in keywords_text.split(",") if k.strip()] if keywords_text else []
            
            # Parse project IDs
            project_id_text = self.project_id_var.get().strip()
            project_ids = []
            if project_id_text:
                try:
                    project_ids = [ObjectId(project_id_text)]
                except:
                    messagebox.showerror("Error", "Invalid Project ID format")
                    return
            else:
                # Create a test project ID if none provided
                project_ids = [ObjectId()]
            
            # Prepare package data
            package_data = {
                "name": name,
                "description": self.desc_var.get().strip(),
                "rss_feeds": rss_feeds,
                "project_ids": project_ids,
                "schedule_interval": self.interval_var.get(),
                "max_articles_per_run": int(self.max_articles_var.get()),
                "status": "active",
                "calculate_embeddings": self.calc_embeddings_var.get(),
                "extract_entities": self.extract_entities_var.get(),
                "summarize": self.summarize_var.get(),
                "date_created": datetime.datetime.utcnow(),
                "date_updated": datetime.datetime.utcnow(),
                "article_count": 0,
                "articles_last_run": 0
            }
            
            # Add filters if keywords provided
            if keywords:
                package_data["filters"] = {
                    "keywords_include": keywords
                }
            
            # Insert directly into MongoDB
            client = self.db.db.client
            db = client['proton']
            result = db.scraping_packages.insert_one(package_data)
            package_id = str(result.inserted_id)
            
            print(f"Created package with ID: {package_id}")
            
            # Show success message
            messagebox.showinfo("Success", f"Scraping package '{name}' created successfully.\nID: {package_id}")
            
            # Clear form
            self.clear_form()
            
            # Refresh package list
            self.load_packages()
            
            # Switch to manage tab
            self.notebook.select(1)  # Index 1 is the Manage tab
            
        except Exception as e:
            import traceback
            print(f"ERROR creating package: {str(e)}")
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to create package: {str(e)}")
    
    def clear_form(self):
        """Clear the form fields"""
        self.name_var.set("")
        self.desc_var.set("")
        self.feeds_text.delete("1.0", tk.END)
        self.interval_var.set("1h")
        self.max_articles_var.set("10")
        self.keywords_var.set("")
        self.calc_embeddings_var.set(True)
        self.extract_entities_var.set(True)
        self.summarize_var.set(True)
        self.project_id_var.set("")
    
    def load_packages(self):
        """Load existing packages from the database"""
        try:
            # Clear existing items
            for item in self.packages_treeview.get_children():
                self.packages_treeview.delete(item)
            
            # Clear package lookup
            self.package_lookup = {}
            
            # Get all packages
            # Get MongoDB client directly from db object
            client = self.db.db.client
            db = client['proton']  # Use the same database name as in ProtonDB
            packages = list(db.scraping_packages.find())
            
            print(f"Found {len(packages)} packages in database")
            
            # Debug: Print package info
            for pkg in packages:
                print(f"  Package: {pkg.get('name')} (ID: {pkg.get('_id')})")
                print(f"    Feeds: {len(pkg.get('rss_feeds', []))}")
                print(f"    Status: {pkg.get('status', 'unknown')}")
            
            # Update treeview
            for pkg in packages:
                package_id = str(pkg["_id"])
                
                # Store in lookup for later use
                self.package_lookup[package_id] = pkg
                
                # Format data for display
                feed_count = len(pkg.get("rss_feeds", []))
                status = pkg.get("status", "active")
                schedule = pkg.get("schedule_interval", "1h")
                
                # Format last run time
                last_run = pkg.get("last_run", "Never")
                last_run_str = last_run.strftime("%Y-%m-%d %H:%M:%S") if hasattr(last_run, "strftime") else "Never"
                
                # Add to treeview
                self.packages_treeview.insert("", tk.END, iid=package_id, values=(
                    pkg.get("name", "Unnamed"),
                    f"{feed_count} feeds",
                    schedule,
                    status,
                    last_run_str
                ))
            
            # Update package combobox in test tab with properly formatted values
            combo_values = [f"{pkg.get('name', 'Unnamed')} (ID: {pkg_id})" 
                            for pkg_id, pkg in self.package_lookup.items()]
            print(f"Setting combo values: {combo_values}")
            
            # Update both comboboxes
            self.test_package_combo["values"] = combo_values
            self.edit_package_combo["values"] = combo_values
            
            # Update status
            self.status_var.set(f"Loaded {len(packages)} packages")
            
        except Exception as e:
            import traceback
            print(f"ERROR loading packages: {str(e)}")
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to load packages: {str(e)}")
    
    def view_package_details(self):
        """View details of the selected package"""
        selected = self.packages_treeview.selection()
        if not selected:
            return
            
        package_id = selected[0]
        if package_id not in self.package_lookup:
            return
            
        # Get package details
        pkg = self.package_lookup[package_id]
        
        # Format details for display
        details = []
        details.append(f"Name: {pkg.get('name', 'Unnamed')}")
        details.append(f"ID: {package_id}")
        details.append(f"Description: {pkg.get('description', 'No description')}")
        details.append(f"Status: {pkg.get('status', 'active')}")
        details.append(f"Schedule: Every {pkg.get('schedule_interval', '1h')}")
        details.append(f"Max Articles: {pkg.get('max_articles_per_run', 10)}")
        
        # Add feeds
        details.append("\nRSS Feeds:")
        for feed in pkg.get("rss_feeds", []):
            details.append(f"- {feed}")
            
        # Add project IDs
        details.append("\nProject IDs:")
        for project_id in pkg.get("project_ids", []):
            details.append(f"- {project_id}")
            
        # Add processing options
        details.append("\nProcessing Options:")
        details.append(f"- Calculate Embeddings: {pkg.get('calculate_embeddings', True)}")
        details.append(f"- Extract Entities: {pkg.get('extract_entities', True)}")
        details.append(f"- Generate Summaries: {pkg.get('summarize', True)}")
        
        # Add filters
        filters = pkg.get("filters", {})
        if filters:
            details.append("\nFilters:")
            
            keywords_include = filters.get("keywords_include", [])
            if keywords_include:
                details.append(f"- Include Keywords: {', '.join(keywords_include)}")
                
            keywords_exclude = filters.get("keywords_exclude", [])
            if keywords_exclude:
                details.append(f"- Exclude Keywords: {', '.join(keywords_exclude)}")
                
            min_length = filters.get("min_length")
            if min_length:
                details.append(f"- Minimum Article Length: {min_length} characters")
        
        # Add statistics
        article_count = pkg.get("article_count", 0)
        articles_last_run = pkg.get("articles_last_run", 0)
        last_run = pkg.get("last_run", "Never")
        last_run_str = last_run.strftime("%Y-%m-%d %H:%M:%S") if hasattr(last_run, "strftime") else "Never"
        
        details.append("\nStatistics:")
        details.append(f"- Total Articles: {article_count}")
        details.append(f"- Last Run: {last_run_str}")
        details.append(f"- Articles Last Run: {articles_last_run}")
        
        # Display details
        self.details_text.config(state=tk.NORMAL)
        self.details_text.delete("1.0", tk.END)
        self.details_text.insert(tk.END, "\n".join(details))
        self.details_text.config(state=tk.DISABLED)
        
        # Remove any existing edit button
        try:
            if hasattr(self, 'edit_button') and self.edit_button:
                self.edit_button.destroy()
        except:
            pass
        
        # Add an edit button
        self.edit_button = ttk.Button(self.manage_tab, text="Edit This Package", 
                                     command=lambda: self.edit_selected_package(package_id))
        self.edit_button.place(relx=0.8, rely=0.9)
    
    def edit_selected_package(self, package_id):
        """Switch to edit tab and load the selected package"""
        if package_id not in self.package_lookup:
            return
        
        # Set the package in the edit combobox
        pkg = self.package_lookup[package_id]
        self.edit_package_var.set(f"{pkg.get('name', 'Unnamed')} (ID: {package_id})")
        
        # Load the package for editing
        self.load_package_for_edit()
        
        # Switch to edit tab
        self.notebook.select(2)  # Index 2 is the Edit tab
    
    def toggle_package_status(self):
        """Toggle the status of the selected package (active/paused)"""
        selected = self.packages_treeview.selection()
        if not selected:
            messagebox.showinfo("Info", "Please select a package to toggle")
            return
            
        package_id = selected[0]
        if package_id not in self.package_lookup:
            return
            
        # Get current status
        pkg = self.package_lookup[package_id]
        current_status = pkg.get("status", "active")
        
        # Toggle status
        new_status = "paused" if current_status == "active" else "active"
        
        # Update in database
        try:
            # Update directly in MongoDB
            client = self.db.db.client
            db = client['proton']
            result = db.scraping_packages.update_one(
                {"_id": ObjectId(package_id)},
                {"$set": {"status": new_status, "date_updated": datetime.datetime.utcnow()}}
            )
            
            if result.modified_count > 0:
                messagebox.showinfo("Success", f"Package status changed to: {new_status}")
                
                # Refresh package list
                self.load_packages()
            else:
                messagebox.showwarning("Warning", "No changes were made to the package")
            
        except Exception as e:
            import traceback
            print(f"ERROR toggling package status: {str(e)}")
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to update package status: {str(e)}")
    
    def delete_package(self):
        """Delete the selected package"""
        selected = self.packages_treeview.selection()
        if not selected:
            messagebox.showinfo("Info", "Please select a package to delete")
            return
            
        package_id = selected[0]
        if package_id not in self.package_lookup:
            return
            
        # Get package name
        pkg = self.package_lookup[package_id]
        name = pkg.get("name", "Unnamed")
        
        # Confirm deletion
        if not messagebox.askyesno("Confirm Delete", f"Are you sure you want to delete the package '{name}'?"):
            return
            
        # Delete from database
        try:
            # Delete directly from MongoDB
            client = self.db.db.client
            db = client['proton']
            result = db.scraping_packages.delete_one({"_id": ObjectId(package_id)})
            
            if result.deleted_count > 0:
                messagebox.showinfo("Success", f"Package '{name}' deleted successfully")
                
                # Refresh package list
                self.load_packages()
            else:
                messagebox.showwarning("Warning", "No package was deleted")
            
        except Exception as e:
            import traceback
            print(f"ERROR deleting package: {str(e)}")
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to delete package: {str(e)}")
    
    def run_package(self):
        """Run the selected package"""
        selected_package = self.test_package_var.get()
        if not selected_package:
            messagebox.showerror("Error", "No package selected")
            return
            
        # Extract package ID from the selection (format: "Name (ID: xxx)")
        import re
        match = re.search(r"\(ID: ([a-f0-9]+)\)", selected_package)
        if not match:
            messagebox.showerror("Error", "Could not determine package ID")
            return
            
        package_id = match.group(1)
        
        # Clear console
        self.console_text.config(state=tk.NORMAL)
        self.console_text.delete("1.0", tk.END)
        self.console_text.config(state=tk.DISABLED)
        
        # Update status
        self.status_var.set(f"Running package {package_id}...")
        
        # Run in a separate thread to avoid freezing the UI
        thread = threading.Thread(target=self._run_package_thread, args=(package_id,))
        thread.daemon = True
        thread.start()
    
    def _run_package_thread(self, package_id):
        """Run a package in a separate thread"""
        try:
            self._update_console(f"Starting run for package {package_id} at {datetime.datetime.now()}\n")
            self._update_console("Loading package...\n")
            
            # Get the package directly from MongoDB
            client = self.db.db.client
            db = client['proton']
            package = db.scraping_packages.find_one({"_id": ObjectId(package_id)})
            
            if not package:
                self._update_console("ERROR: Package not found\n")
                self.status_var.set("Error: Package not found")
                return
                
            self._update_console(f"Running package: {package.get('name')}\n")
            self._update_console(f"RSS Feeds: {', '.join(package.get('rss_feeds', []))}\n")
            self._update_console("Starting scraping process...\n\n")
            
            # Import needed modules here to avoid circular imports
            try:
                from scraping_package_scheduler import ScrapingPackage
            except ImportError as e:
                self._update_console(f"ERROR importing ScrapingPackage: {str(e)}\n")
                self._update_console("This might be due to a Python path issue.\n")
                self.status_var.set("Error: Failed to import required modules")
                return
            
            try:
                # Create a ScrapingPackage object
                self._update_console("Creating ScrapingPackage object...\n")
                scraper = ScrapingPackage(
                    package_id=package_id,
                    project_ids=[str(pid) for pid in package.get("project_ids", [])],
                    name=package.get("name", "Unnamed package"),
                    description=package.get("description", ""),
                    rss_feeds=package.get("rss_feeds", []),
                    schedule_interval=package.get("schedule_interval", "1h"),
                    max_articles_per_run=package.get("max_articles_per_run", 10),
                    calculate_embeddings=package.get("calculate_embeddings", True)
                )
                
                # Run the package
                self._update_console("Running the package...\n")
                article_ids = scraper.run()
                
                # Update console with results
                self._update_console(f"\nRun completed. Processed {len(article_ids)} articles.\n")
                for i, article_id in enumerate(article_ids, 1):
                    self._update_console(f"{i}. Article ID: {article_id}\n")
                    
                # Update package stats directly in MongoDB
                update_result = db.scraping_packages.update_one(
                    {"_id": ObjectId(package_id)},
                    {
                        "$inc": {"article_count": len(article_ids), "articles_last_run": len(article_ids)},
                        "$set": {"last_run": datetime.datetime.utcnow()}
                    }
                )
                
                self._update_console(f"Updated package stats: {update_result.modified_count} document(s) modified\n")
                
                # Update status
                self.status_var.set(f"Package run completed. Processed {len(article_ids)} articles.")
                
                # Refresh package list if auto-refresh is enabled
                if self.auto_refresh_var.get():
                    self.load_packages()
                    
            except Exception as e:
                import traceback
                self._update_console(f"ERROR during execution: {str(e)}\n")
                self._update_console(f"Traceback:\n{traceback.format_exc()}\n")
                self.status_var.set(f"Error: {str(e)}")
                
        except Exception as e:
            import traceback
            self._update_console(f"ERROR: {str(e)}\n")
            self._update_console(f"Traceback:\n{traceback.format_exc()}\n")
            self.status_var.set(f"Error: {str(e)}")
            
            # Also print to terminal for debugging
            print(f"ERROR in _run_package_thread: {str(e)}")
            traceback.print_exc()
    
    def _update_console(self, text):
        """Update the console with text"""
        self.console_text.config(state=tk.NORMAL)
        self.console_text.insert(tk.END, text)
        self.console_text.config(state=tk.DISABLED)
        self.console_text.see(tk.END)

    def close(self):
        """Close the application and database connection"""
        try:
            self.db.close()
        except:
            pass
        self.root.destroy()

    def load_package_for_edit(self):
        """Load the selected package data into the edit form"""
        selected_package = self.edit_package_var.get()
        if not selected_package:
            messagebox.showinfo("Info", "Please select a package to edit")
            return
        
        # Extract package ID from the selection (format: "Name (ID: xxx)")
        import re
        match = re.search(r"\(ID: ([a-f0-9]+)\)", selected_package)
        if not match:
            messagebox.showerror("Error", "Could not determine package ID")
            return
        
        package_id = match.group(1)
        self.selected_edit_package_id = package_id
        
        # Get the package directly from MongoDB
        try:
            client = self.db.db.client
            db = client['proton']
            package = db.scraping_packages.find_one({"_id": ObjectId(package_id)})
            
            if not package:
                messagebox.showerror("Error", f"Package with ID {package_id} not found")
                return
            
            # Fill the form with package data
            self.edit_name_var.set(package.get("name", ""))
            self.edit_desc_var.set(package.get("description", ""))
            
            # Set RSS feeds
            self.edit_feeds_text.delete("1.0", tk.END)
            feeds = package.get("rss_feeds", [])
            self.edit_feeds_text.insert(tk.END, "\n".join(feeds))
            
            # Set schedule interval
            self.edit_interval_var.set(package.get("schedule_interval", "1h"))
            
            # Set max articles
            self.edit_max_articles_var.set(str(package.get("max_articles_per_run", 10)))
            
            # Set status
            self.edit_status_var.set(package.get("status", "active"))
            
            # Update status
            self.status_var.set(f"Loaded package {package.get('name')} for editing")
            
        except Exception as e:
            import traceback
            print(f"ERROR loading package for edit: {str(e)}")
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to load package: {str(e)}")

    def update_package(self):
        """Update the package with edited values"""
        if not self.selected_edit_package_id:
            messagebox.showerror("Error", "No package selected for editing")
            return
        
        # Validate input
        name = self.edit_name_var.get().strip()
        if not name:
            messagebox.showerror("Error", "Package name is required")
            return
        
        feeds_text = self.edit_feeds_text.get("1.0", tk.END).strip()
        if not feeds_text:
            messagebox.showerror("Error", "At least one RSS feed is required")
            return
        
        # Parse RSS feeds
        rss_feeds = [feed.strip() for feed in feeds_text.split("\n") if feed.strip()]
        
        # Prepare update data
        update_data = {
            "name": name,
            "description": self.edit_desc_var.get().strip(),
            "rss_feeds": rss_feeds,
            "schedule_interval": self.edit_interval_var.get(),
            "max_articles_per_run": int(self.edit_max_articles_var.get()),
            "status": self.edit_status_var.get(),
            "date_updated": datetime.datetime.utcnow()
        }
        
        # Update in database
        try:
            client = self.db.db.client
            db = client['proton']
            result = db.scraping_packages.update_one(
                {"_id": ObjectId(self.selected_edit_package_id)},
                {"$set": update_data}
            )
            
            if result.modified_count > 0:
                messagebox.showinfo("Success", f"Package '{name}' updated successfully")
                
                # Refresh package list
                self.load_packages()
                
                # Clear selection
                self.selected_edit_package_id = None
                self.edit_package_var.set("")
                
                # Clear form
                self.edit_name_var.set("")
                self.edit_desc_var.set("")
                self.edit_feeds_text.delete("1.0", tk.END)
                self.edit_interval_var.set("1h")
                self.edit_max_articles_var.set("10")
                self.edit_status_var.set("active")
                
                # Update status
                self.status_var.set(f"Package '{name}' updated successfully")
            else:
                messagebox.showwarning("Warning", "No changes were made to the package")
            
        except Exception as e:
            import traceback
            print(f"ERROR updating package: {str(e)}")
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to update package: {str(e)}")

def main():
    """Main entry point"""
    root = tk.Tk()
    app = ScrapingPackageGUI(root)
    
    # Handle window close
    root.protocol("WM_DELETE_WINDOW", app.close)
    
    # Start the main loop
    root.mainloop()

if __name__ == "__main__":
    main()
</file>

<file path="deploy/scraping_package_gui/scraping_package_model.py">
"""
MongoDB model for scraping packages
"""

import os
import sys
import logging
import datetime
from typing import Dict, List, Any, Optional
from bson.objectid import ObjectId

# Import ProtonDB
from proton_db_setup import ProtonDB

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ScrapingPackageModel:
    """MongoDB model for scraping packages"""
    
    def __init__(self):
        """Initialize connection to the database"""
        self.db = ProtonDB()
    
    def create_package(self, package_data: Dict[str, Any]) -> Optional[str]:
        """
        Create a new scraping package
        
        Args:
            package_data: Dictionary containing package configuration
            
        Returns:
            Optional[str]: ID of the created package or None if failed
        """
        try:
            # Set default fields if not provided
            if "status" not in package_data:
                package_data["status"] = "active"
                
            if "date_created" not in package_data:
                package_data["date_created"] = datetime.datetime.utcnow()
                
            if "date_updated" not in package_data:
                package_data["date_updated"] = datetime.datetime.utcnow()
                
            if "article_count" not in package_data:
                package_data["article_count"] = 0
                
            if "articles_last_run" not in package_data:
                package_data["articles_last_run"] = 0
                
            # Validate required fields
            required_fields = ["name", "rss_feeds", "schedule_interval"]
            for field in required_fields:
                if field not in package_data:
                    logger.error(f"Missing required field: {field}")
                    return None
            
            # Insert into database
            result = self.db.db.scraping_packages.insert_one(package_data)
            package_id = str(result.inserted_id)
            logger.info(f"Created scraping package with ID: {package_id}")
            
            return package_id
            
        except Exception as e:
            logger.error(f"Failed to create scraping package: {e}")
            return None
    
    def get_package(self, package_id: str) -> Optional[Dict[str, Any]]:
        """
        Get a scraping package by ID
        
        Args:
            package_id: ID of the package to retrieve
            
        Returns:
            Optional[Dict[str, Any]]: Package data or None if not found
        """
        try:
            package = self.db.db.scraping_packages.find_one({"_id": ObjectId(package_id)})
            return package
        except Exception as e:
            logger.error(f"Failed to get package {package_id}: {e}")
            return None
    
    def get_active_packages(self) -> List[Dict[str, Any]]:
        """
        Get all active scraping packages
        
        Returns:
            List[Dict[str, Any]]: List of active package documents
        """
        try:
            packages = list(self.db.db.scraping_packages.find({"status": "active"}))
            return packages
        except Exception as e:
            logger.error(f"Failed to get active packages: {e}")
            return []
    
    def update_package(self, package_id: str, updates: Dict[str, Any]) -> bool:
        """
        Update a scraping package
        
        Args:
            package_id: ID of the package to update
            updates: Dictionary of fields to update
            
        Returns:
            bool: True if updated successfully, False otherwise
        """
        try:
            # Add update timestamp
            updates["date_updated"] = datetime.datetime.utcnow()
            
            # Update the package
            result = self.db.db.scraping_packages.update_one(
                {"_id": ObjectId(package_id)},
                {"$set": updates}
            )
            
            if result.modified_count > 0:
                logger.info(f"Updated package {package_id}")
                return True
            else:
                logger.warning(f"No changes made to package {package_id}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to update package {package_id}: {e}")
            return False
    
    def activate_package(self, package_id: str) -> bool:
        """
        Activate a scraping package
        
        Args:
            package_id: ID of the package to activate
            
        Returns:
            bool: True if activated successfully, False otherwise
        """
        return self.update_package(package_id, {"status": "active"})
    
    def deactivate_package(self, package_id: str) -> bool:
        """
        Deactivate a scraping package
        
        Args:
            package_id: ID of the package to deactivate
            
        Returns:
            bool: True if deactivated successfully, False otherwise
        """
        return self.update_package(package_id, {"status": "inactive"})
    
    def delete_package(self, package_id: str) -> bool:
        """
        Delete a scraping package
        
        Args:
            package_id: ID of the package to delete
            
        Returns:
            bool: True if deleted successfully, False otherwise
        """
        try:
            result = self.db.db.scraping_packages.delete_one({"_id": ObjectId(package_id)})
            
            if result.deleted_count > 0:
                logger.info(f"Deleted package {package_id}")
                return True
            else:
                logger.warning(f"Package {package_id} not found")
                return False
                
        except Exception as e:
            logger.error(f"Failed to delete package {package_id}: {e}")
            return False
    
    def update_package_stats(self, package_id: str, stats: Dict[str, Any]) -> bool:
        """
        Update package statistics
        
        Args:
            package_id: ID of the package to update
            stats: Dictionary of statistics to update
            
        Returns:
            bool: True if updated successfully, False otherwise
        """
        try:
            # Build update document
            update_doc = {
                "$set": {
                    "last_run": datetime.datetime.utcnow(),
                    "date_updated": datetime.datetime.utcnow()
                }
            }
            
            # Add an $inc for article count if articles were processed
            if "articles_processed" in stats and stats["articles_processed"] > 0:
                update_doc["$inc"] = {
                    "article_count": stats["articles_processed"],
                    "articles_last_run": stats["articles_processed"]
                }
            else:
                update_doc["$set"]["articles_last_run"] = 0
            
            # Add any other stats
            for key, value in stats.items():
                if key != "articles_processed":
                    update_doc["$set"][f"stats.{key}"] = value
            
            # Update the package
            result = self.db.db.scraping_packages.update_one(
                {"_id": ObjectId(package_id)},
                update_doc
            )
            
            if result.modified_count > 0:
                logger.info(f"Updated stats for package {package_id}")
                return True
            else:
                logger.warning(f"No changes made to package stats {package_id}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to update package stats {package_id}: {e}")
            return False
    
    def close(self):
        """Close the database connection"""
        self.db.close()

# Example usage
if __name__ == "__main__":
    package_model = ScrapingPackageModel()
    
    try:
        # Create a test package
        test_package = {
            "name": "Tech News",
            "description": "Technology news from various sources",
            "rss_feeds": [
                "https://feeds.feedburner.com/TechCrunch/",
                "https://www.wired.com/feed/rss"
            ],
            "project_ids": [ObjectId()],
            "schedule_interval": "1h",
            "max_articles_per_run": 5,
            "status": "active"
        }
        
        package_id = package_model.create_package(test_package)
        if package_id:
            print(f"Created test package with ID: {package_id}")
            
            # Get the package
            package = package_model.get_package(package_id)
            print(f"Retrieved package: {package['name']}")
            
            # Update package
            package_model.update_package(package_id, {"description": "Updated description"})
            
            # Update stats
            package_model.update_package_stats(package_id, {"articles_processed": 5, "average_relevance": 0.8})
            
            # Deactivate package
            package_model.deactivate_package(package_id)
            
            # Delete test package (comment out to keep it)
            package_model.delete_package(package_id)
    
    finally:
        package_model.close()
</file>

<file path="deploy/scraping_package_gui/scraping_package_scheduler.py">
"""
Scraping Package Scheduler for Proton CRM

This module provides functionality to schedule and execute RSS feed scraping packages.
It combines multiple RSS feeds, extracts articles, and stores them in the MongoDB database.
"""

import os
import sys
import time
import datetime
import logging
import feedparser
import schedule
import threading
import queue
from typing import Dict, List, Optional, Any
from bson.objectid import ObjectId
import requests
from urllib.parse import urlparse
import numpy as np
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize

# Add parent directory to path to import ProtonDB
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from proton_db_setup import ProtonDB

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global queue for scheduled jobs
job_queue = queue.Queue()

class ScrapingPackage:
    """
    Represents a scraping package configuration that combines multiple RSS feeds
    """

    def __init__(self, package_id: str, project_ids: List[str], name: str,
                 description: str, rss_feeds: List[str], schedule_interval: str = "1h",
                 max_articles_per_run: int = 10, calculate_embeddings: bool = True):
        """
        Initialize a scraping package

        Args:
            package_id: MongoDB ObjectId of the scraping package
            project_ids: List of project IDs this package is associated with
            name: Name of the scraping package
            description: Description of the package
            rss_feeds: List of RSS feed URLs to scrape
            schedule_interval: How often to run the scraper (e.g., "30m", "1h", "1d")
            max_articles_per_run: Maximum number of articles to process per run
            calculate_embeddings: Whether to calculate vector embeddings for articles
        """
        self.package_id = package_id
        self.project_ids = project_ids
        self.name = name
        self.description = description
        self.rss_feeds = rss_feeds
        self.schedule_interval = schedule_interval
        self.max_articles_per_run = max_articles_per_run
        self.calculate_embeddings = calculate_embeddings

        # Load embedding model if needed
        if self.calculate_embeddings:
            try:
                from vector_embeddings import EmbeddingGenerator
                # Try to use OpenAI first, will fall back to local if API key not available
                self.embedding_generator = EmbeddingGenerator(model_type="openai")
                logger.info(f"Loaded embedding model for package {self.name} (type: {self.embedding_generator.model_type})")
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")
                self.calculate_embeddings = False

        # Initialize database connection
        self.db = ProtonDB()

        # Initialize NLTK for text processing
        try:
            # Download necessary NLTK resources
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            logger.info("Successfully downloaded NLTK resources")
        except Exception as e:
            logger.warning(f"Failed to download NLTK resources: {e}")
            logger.info("Will attempt to download resources again when needed")

    def parse_feeds(self) -> List[Dict[str, Any]]:
        """
        Parse all RSS feeds and combine entries

        Returns:
            List[Dict[str, Any]]: Combined list of feed entries
        """
        all_entries = []

        for feed_url in self.rss_feeds:
            try:
                logger.info(f"Parsing RSS feed: {feed_url}")
                feed = feedparser.parse(feed_url)

                # Skip feeds with errors
                if hasattr(feed, 'bozo_exception'):
                    logger.warning(f"Feed error for {feed_url}: {feed.bozo_exception}")
                    continue

                # Extract feed entries
                for entry in feed.entries:
                    try:
                        # Basic validation
                        if not hasattr(entry, 'link') or not entry.link:
                            continue

                        # Build entry data
                        entry_data = {
                            "title": getattr(entry, 'title', 'Untitled'),
                            "link": entry.link,
                            "summary": getattr(entry, 'summary', None),
                            "published": getattr(entry, 'published', None),
                            "source_feed": feed_url,
                            "source_name": feed.feed.get('title', urlparse(feed_url).netloc)
                        }

                        # Try to extract published date
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                # Convert struct_time to datetime
                                entry_data['published_date'] = datetime.datetime(*entry.published_parsed[:6])
                            except Exception as e:
                                logger.debug(f"Failed to parse date: {e}")

                        all_entries.append(entry_data)
                    except Exception as e:
                        logger.warning(f"Error processing feed entry: {e}")

                logger.info(f"Extracted {len(feed.entries)} entries from {feed_url}")

            except Exception as e:
                logger.error(f"Failed to parse feed {feed_url}: {e}")

        # Sort by published date (newest first) if available
        all_entries.sort(
            key=lambda x: x.get('published_date', datetime.datetime.min),
            reverse=True
        )

        # Limit to max_articles_per_run
        return all_entries[:self.max_articles_per_run]

    def fetch_full_content(self, entry: Dict[str, Any]) -> Dict[str, Any]:
        """
        Fetch the full content of an article from its URL

        Args:
            entry: Feed entry with basic metadata

        Returns:
            Dict[str, Any]: Entry with additional content and metadata
        """
        url = entry["link"]
        try:
            # Add a small delay to be considerate to the target server
            time.sleep(1)

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # Parse with BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')

            # Extract article content
            article_content = ""
            main_content = soup.find('article') or soup.find('main') or soup.find('div', class_='content')

            if main_content:
                # Get all paragraphs from the main content
                paragraphs = main_content.find_all('p')
                article_content = ' '.join([p.get_text().strip() for p in paragraphs])
            else:
                # Fallback: get all paragraphs from the body
                paragraphs = soup.find_all('p')
                article_content = ' '.join([p.get_text().strip() for p in paragraphs])

            # Extract author information if available
            author_elem = soup.find('meta', attrs={'name': 'author'}) or soup.find('a', rel='author')
            author = author_elem.get('content', '') if hasattr(author_elem, 'get') and author_elem.get('content') else (
                author_elem.get_text().strip() if author_elem else "Unknown"
            )

            # Extract image if available
            image_url = None
            image_elem = soup.find('meta', property='og:image')
            if image_elem and image_elem.get('content'):
                image_url = image_elem.get('content')
            else:
                # Try to find the first large image in the article
                images = main_content.find_all('img') if main_content else soup.find_all('img')
                for img in images:
                    if img.get('src') and (img.get('width') is None or int(img.get('width') or 0) >= 300):
                        image_url = img.get('src')
                        if not image_url.startswith('http'):
                            # Handle relative URLs
                            base_url = '/'.join(url.split('/')[:3])  # Get the domain part
                            image_url = f"{base_url}/{image_url.lstrip('/')}"
                        break

            # Add package info to content to improve relevance
            package_info = f"This article is part of the '{self.name}' package. {self.description}"
            enhanced_content = f"{package_info}\n\n{article_content}"

            # Extract keywords
            keywords = self._extract_keywords(enhanced_content)

            # Generate summary
            summary = self._generate_summary(article_content)

            # Update entry with full content and metadata
            entry.update({
                "content": enhanced_content,
                "original_content": article_content,  # Keep the original content too
                "summary": summary if summary else entry.get("summary", ""),
                "author": author,
                "keywords": keywords,
                "image_url": image_url,
                "package_name": self.name  # Add package name as metadata
            })

            # Calculate vector embedding if enabled
            if self.calculate_embeddings and hasattr(self, 'embedding_generator'):
                try:
                    # Combine title, package name, description, and content for embedding
                    text_to_embed = f"{entry['title']}. {self.name}. {self.description}. {article_content}"
                    embedding = self.embedding_generator.get_embedding(text_to_embed)
                    entry["vector_embedding"] = embedding
                    entry["embedding_model"] = self.embedding_generator.model_type
                    logger.info(f"Generated {self.embedding_generator.model_type} embedding for article: {entry['title']}")
                except Exception as e:
                    logger.warning(f"Failed to generate embedding: {e}")

            # Calculate relevance score (basic implementation)
            entry["relevance_scores"] = self._calculate_relevance_scores(entry)

            return entry

        except Exception as e:
            logger.error(f"Error fetching content from {url}: {e}")
            # Return the original entry with minimal data
            entry["content"] = f"This article is part of the '{self.name}' package. {self.description}\n\n" + entry.get("summary", "")
            entry["original_content"] = entry.get("summary", "")
            entry["package_name"] = self.name
            entry["relevance_scores"] = {"overall": 0.5}  # Default score
            return entry

    def _extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """
        Extract keywords from text

        Args:
            text: Text to extract keywords from
            max_keywords: Maximum number of keywords to extract

        Returns:
            List[str]: List of keywords
        """
        try:
            # Make sure NLTK resources are downloaded
            try:
                nltk.download('punkt', quiet=True)
                nltk.download('stopwords', quiet=True)
            except Exception as e:
                logger.warning(f"Failed to download NLTK resources: {e}")

            from nltk.tokenize import word_tokenize
            from nltk.corpus import stopwords

            # Add package name and keywords from description as additional keywords
            additional_keywords = []

            # Add package name as a keyword
            name_words = self.name.lower().split()
            additional_keywords.extend([word for word in name_words if len(word) > 3])

            # Add words from description as keywords
            if self.description:
                desc_words = self.description.lower().split()
                additional_keywords.extend([word for word in desc_words if len(word) > 3])

            # Tokenize the text
            try:
                tokens = word_tokenize(text.lower())
                logger.info(f"Successfully tokenized text with NLTK (found {len(tokens)} tokens)")
            except Exception as e:
                logger.warning(f"NLTK tokenization failed: {e}, falling back to simple split")
                tokens = text.lower().split()

            # Remove stopwords and non-alphabetic tokens
            try:
                stop_words = set(stopwords.words('english'))
                logger.info("Successfully loaded NLTK stopwords")
            except Exception as e:
                logger.warning(f"Failed to load NLTK stopwords: {e}, using basic stopwords")
                stop_words = set(['the', 'and', 'a', 'to', 'in', 'of', 'is', 'that', 'it', 'for', 'with', 'as', 'on',
                                 'this', 'be', 'are', 'was', 'were', 'has', 'have', 'had', 'do', 'does', 'did',
                                 'but', 'or', 'by', 'not', 'what', 'all', 'their', 'there', 'when', 'up', 'use', 'how',
                                 'out', 'if', 'so', 'no', 'such', 'they', 'then', 'than'])

            filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 3]
            logger.info(f"Filtered to {len(filtered_tokens)} meaningful tokens")

            # Count frequency of each word
            word_freq = {}
            for word in filtered_tokens:
                if word in word_freq:
                    word_freq[word] += 1
                else:
                    word_freq[word] = 1

            # Give higher weight to package name and description words
            for word in additional_keywords:
                if word in word_freq:
                    word_freq[word] += 3  # Boost the frequency
                else:
                    word_freq[word] = 3   # Add with higher initial frequency

            # Sort by frequency and get top keywords
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            keywords = [word for word, _ in sorted_words[:max_keywords]]

            logger.info(f"Extracted {len(keywords)} keywords: {', '.join(keywords[:5])}...")
            return keywords
        except Exception as e:
            logger.warning(f"Error extracting keywords: {e}")
            # Return at least the package name as a keyword
            fallback_keywords = [word for word in self.name.lower().split() if len(word) > 3]
            logger.info(f"Using fallback keywords: {fallback_keywords}")
            return fallback_keywords

    def _generate_summary(self, text: str, max_sentences: int = 3) -> str:
        """
        Generate a simple summary from text (first few sentences)

        Args:
            text: Text to summarize
            max_sentences: Maximum number of sentences in the summary

        Returns:
            str: Summary text
        """
        try:
            # Make sure NLTK resources are downloaded
            try:
                nltk.download('punkt', quiet=True)
            except Exception as e:
                logger.warning(f"Failed to download NLTK punkt: {e}")

            # Try NLTK sentence tokenization
            try:
                sentences = sent_tokenize(text)
                logger.info(f"Successfully tokenized text into {len(sentences)} sentences with NLTK")
            except Exception as e:
                logger.warning(f"NLTK sentence tokenization failed: {e}, falling back to simple split")
                # Simple fallback for sentence tokenization
                sentences = []
                for sentence in text.split('.'):
                    if len(sentence.strip()) > 10:  # Ignore very short fragments
                        sentences.append(sentence.strip() + '.')
                logger.info(f"Fallback tokenization found {len(sentences)} sentences")

            # Get the first few sentences for the summary
            if sentences:
                summary_sentences = sentences[:max_sentences]
                summary = ' '.join(summary_sentences)
                logger.info(f"Generated summary of {len(summary)} characters from {len(summary_sentences)} sentences")
                return summary
            else:
                # If no sentences were found, return a portion of the text
                logger.warning("No sentences found in text, using text prefix as summary")
                return text[:250] + "..."
        except Exception as e:
            logger.warning(f"Error generating summary: {e}")
            return text[:250] + "..."  # Fallback to first 250 chars

    def _calculate_relevance_scores(self, entry: Dict[str, Any]) -> Dict[str, float]:
        """
        Calculate relevance scores for an article

        Args:
            entry: Article data

        Returns:
            Dict[str, float]: Relevance scores
        """
        scores = {
            "overall": 0.75,  # Default score
            "recency": 0.0,   # Will be calculated based on publish date
            "package_fit": 0.8  # Default package fit score
        }

        # Calculate recency score (1.0 for today, decreasing with age)
        if "published_date" in entry and entry["published_date"]:
            now = datetime.datetime.now()
            age_days = (now - entry["published_date"]).days

            # Exponential decay: score = exp(-age_days/14)
            # This gives ~0.95 for today, ~0.5 for 2 weeks old, ~0.25 for 1 month old
            recency = np.exp(-age_days/14) if age_days >= 0 else 0.95
            scores["recency"] = min(1.0, max(0.0, recency))
        else:
            # If no date, assume it's not very recent
            scores["recency"] = 0.5

        # Calculate package fit score (if we have content to analyze)
        if "original_content" in entry and entry["original_content"]:
            # Look for package name and description keywords in the content
            package_keywords = set()

            # Add package name words
            for word in self.name.lower().split():
                if len(word) > 3:  # Only consider longer words
                    package_keywords.add(word)

            # Add description words
            if self.description:
                for word in self.description.lower().split():
                    if len(word) > 3:  # Only consider longer words
                        package_keywords.add(word)

            # Count matches in content
            content_lower = entry["original_content"].lower()
            match_count = sum(1 for word in package_keywords if word in content_lower)

            # Calculate package fit score
            if package_keywords:
                # Score is ratio of matched keywords to total keywords, with a minimum of 0.5
                package_fit = max(0.5, min(1.0, match_count / len(package_keywords)))
                scores["package_fit"] = package_fit

        # Adjust overall score based on recency and package fit
        scores["overall"] = 0.3 * scores["recency"] + 0.7 * scores["package_fit"]

        # Add persona-specific scores (placeholder for now)
        scores["persona_specific"] = {
            "general": scores["overall"]  # Default persona
        }

        return scores

    def store_article(self, entry: Dict[str, Any]) -> Optional[str]:
        """
        Store an article in the database

        Args:
            entry: Article data from RSS feed + full content

        Returns:
            Optional[str]: ID of the inserted article or None if failed
        """
        try:
            # Prepare article data for MongoDB
            article_data = {
                "title": entry["title"],
                "content": entry["content"],
                "original_content": entry.get("original_content", ""),  # Store original content
                "summary": entry["summary"],
                "source_url": entry["link"],
                "source_name": entry["source_name"],
                "author": entry.get("author", "Unknown"),
                "published_date": entry.get("published_date"),
                "date_scraped": datetime.datetime.now(datetime.timezone.utc),
                "scraping_package_id": ObjectId(self.package_id),
                "scraping_package_name": self.name,  # Store package name
                "project_ids": [ObjectId(project_id) for project_id in self.project_ids],
                "keywords": entry.get("keywords", []),
                "content_type": "news",  # Default to news
                "status": "active",
                "image_url": entry.get("image_url"),
                "relevance_scores": entry.get("relevance_scores", {"overall": 0.5})
            }

            # Add vector embedding if available
            if "vector_embedding" in entry:
                article_data["vector_embedding"] = entry["vector_embedding"]

            # Store in database
            article_id = self.db.insert_article(article_data)
            return article_id

        except Exception as e:
            logger.error(f"Failed to store article '{entry.get('title')}': {e}")
            return None

    def run(self) -> List[str]:
        """
        Execute the scraping package: fetch feeds, extract content, and store in database

        Returns:
            List[str]: List of article IDs that were processed
        """
        start_time = time.time()
        logger.info(f"Running scraping package '{self.name}' (ID: {self.package_id})")

        try:
            # Parse all RSS feeds and combine entries
            entries = self.parse_feeds()
            logger.info(f"Found {len(entries)} entries across {len(self.rss_feeds)} feeds")

            # Process each entry
            article_ids = []
            for entry in entries:
                try:
                    # Fetch full content
                    full_entry = self.fetch_full_content(entry)

                    # Store in database
                    article_id = self.store_article(full_entry)
                    if article_id:
                        article_ids.append(article_id)
                        logger.info(f"Processed article: {full_entry['title']} (ID: {article_id})")
                except Exception as e:
                    logger.error(f"Error processing entry {entry.get('link')}: {e}")

            duration = time.time() - start_time
            logger.info(f"Scraping package '{self.name}' completed in {duration:.2f}s. Processed {len(article_ids)} articles.")
            return article_ids

        except Exception as e:
            logger.error(f"Failed to run scraping package '{self.name}': {e}")
            return []
        finally:
            # Close the database connection
            self.db.close()

    def _generate_embeddings(self, content: str) -> List[float]:
        """
        Generate vector embeddings for the article content

        Args:
            content: Article content to generate embeddings for

        Returns:
            List[float]: Vector embeddings
        """
        # If we already have an embedding generator, use it
        if hasattr(self, 'embedding_generator'):
            try:
                return self.embedding_generator.get_embedding(content)
            except Exception as e:
                logger.error(f"Error generating embeddings with existing generator: {e}")

        # Otherwise create a new one
        try:
            # Use the EmbeddingGenerator class
            from vector_embeddings import EmbeddingGenerator

            # Check for preference in package settings
            embedding_model = os.getenv("EMBEDDING_MODEL", "local").lower()

            # Create embedding generator
            embedder = EmbeddingGenerator(model_type=embedding_model)

            # Generate and return embedding
            embedding = embedder.get_embedding(content)
            return embedding
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            # Return empty list on error
            return []


class ScrapingScheduler:
    """
    Scheduler for managing and executing scraping packages at specified intervals
    """

    def __init__(self):
        """Initialize the scheduler"""
        self.packages = {}  # Dict of package_id -> ScrapingPackage
        self.running = False
        self.worker_thread = None

    def add_package(self, package: ScrapingPackage) -> None:
        """
        Add a scraping package to the scheduler

        Args:
            package: ScrapingPackage to add
        """
        self.packages[package.package_id] = package

        # Convert interval string to schedule
        interval = package.schedule_interval.lower()
        if interval.endswith('m'):
            minutes = int(interval[:-1])
            schedule.every(minutes).minutes.do(self._queue_job, package.package_id)
        elif interval.endswith('h'):
            hours = int(interval[:-1])
            schedule.every(hours).hours.do(self._queue_job, package.package_id)
        elif interval.endswith('d'):
            days = int(interval[:-1])
            schedule.every(days).days.do(self._queue_job, package.package_id)
        else:
            # Default to hourly if format not recognized
            schedule.every(1).hours.do(self._queue_job, package.package_id)

        logger.info(f"Added scraping package '{package.name}' (ID: {package.package_id}) to scheduler with interval {package.schedule_interval}")

    def remove_package(self, package_id: str) -> bool:
        """
        Remove a scraping package from the scheduler

        Args:
            package_id: ID of the package to remove

        Returns:
            bool: True if package was removed, False otherwise
        """
        if package_id in self.packages:
            # Remove all jobs for this package from the schedule
            schedule.clear(package_id)
            del self.packages[package_id]
            logger.info(f"Removed scraping package (ID: {package_id}) from scheduler")
            return True
        return False

    def _queue_job(self, package_id: str) -> None:
        """
        Add a job to the queue

        Args:
            package_id: ID of the package to run
        """
        job_queue.put(package_id)
        logger.debug(f"Queued scraping package (ID: {package_id})")

    def _worker(self) -> None:
        """Worker thread that processes the job queue"""
        while self.running:
            try:
                # Run pending scheduled tasks
                schedule.run_pending()

                # Process queue
                if not job_queue.empty():
                    package_id = job_queue.get(block=False)

                    if package_id in self.packages:
                        package = self.packages[package_id]
                        try:
                            package.run()
                        except Exception as e:
                            logger.error(f"Error running package {package_id}: {e}")
                    else:
                        logger.warning(f"Unknown package ID: {package_id}")

                    job_queue.task_done()

                # Sleep briefly
                time.sleep(1)

            except Exception as e:
                logger.error(f"Error in scheduler worker: {e}")
                time.sleep(5)  # Sleep longer after an error

    def start(self) -> None:
        """Start the scheduler worker thread"""
        if not self.running:
            self.running = True
            self.worker_thread = threading.Thread(target=self._worker, daemon=True)
            self.worker_thread.start()
            logger.info("Scraping scheduler started")

    def stop(self) -> None:
        """Stop the scheduler worker thread"""
        self.running = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5)
            self.worker_thread = None
        logger.info("Scraping scheduler stopped")

    def run_package_now(self, package_id: str) -> None:
        """
        Run a package immediately

        Args:
            package_id: ID of the package to run
        """
        if package_id in self.packages:
            self._queue_job(package_id)
            logger.info(f"Scheduled immediate run for package (ID: {package_id})")
        else:
            logger.warning(f"Cannot run unknown package (ID: {package_id})")


# Example usage
if __name__ == "__main__":
    # Test RSS feeds
    test_feeds = [
        "https://www.theverge.com/rss/index.xml",
        "https://feeds.feedburner.com/TechCrunch/",
        "https://www.wired.com/feed/rss"
    ]

    # Create test project and package IDs
    test_project_id = str(ObjectId())
    test_package_id = str(ObjectId())

    try:
        # Initialize the scraping package
        package = ScrapingPackage(
            package_id=test_package_id,
            project_ids=[test_project_id],
            name="Tech News Package",
            description="Combines tech news from The Verge, TechCrunch, and Wired",
            rss_feeds=test_feeds,
            schedule_interval="1h",
            max_articles_per_run=5
        )

        # Initialize the scheduler
        scheduler = ScrapingScheduler()
        scheduler.add_package(package)

        # Start the scheduler
        scheduler.start()

        # Run the package immediately for testing
        scheduler.run_package_now(test_package_id)

        # Keep the script running
        logger.info("Scheduler running. Press Ctrl+C to exit.")
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("Stopping scheduler...")
            scheduler.stop()
            logger.info("Done.")

    except Exception as e:
        logger.error(f"Error in main: {e}")
</file>

<file path="templates/article.html">
{% extends "base.html" %}

{% block title %}{{ article.title }} - Proton CRM{% endblock %}

{% block content %}
    <div class="mb-3">
        <a href="{{ url_for('list_articles') }}" class="btn btn-outline-secondary btn-sm">
            &larr; Back to Articles
        </a>
        {% if package %}
            <a href="{{ url_for('view_package', package_id=article.scraping_package_id) }}" class="btn btn-outline-info btn-sm">
                View Package: {{ package.name }}
            </a>
        {% endif %}
    </div>

    <article class="mb-5">
        <h1>{{ article.title }}</h1>
        
        <div class="mb-3 text-muted">
            {% if article.source_name %}
                <span>
                    Source: {% if article.source_url %}
                        <a href="{{ article.source_url }}" target="_blank">{{ article.source_name }}</a>
                    {% else %}
                        {{ article.source_name }}
                    {% endif %}
                </span> 
            {% endif %}
            
            {% if article.display_date %}
                <span>‚Ä¢ Published: {{ article.display_date }}</span>
            {% endif %}
            
            {% if article.author %}
                <span>‚Ä¢ Author: {{ article.author }}</span>
            {% endif %}
        </div>
        
        {% if article.scraping_package_name %}
            <div class="mb-3">
                <span class="badge bg-secondary">Package: {{ article.scraping_package_name }}</span>
                
                {% if article.relevance_scores %}
                    <span class="badge bg-primary">
                        Relevance: {{ (article.relevance_scores.overall * 100)|int }}%
                    </span>
                    
                    {% if article.relevance_scores.package_fit is defined %}
                        <span class="badge bg-info text-dark">
                            Package Fit: {{ (article.relevance_scores.package_fit * 100)|int }}%
                        </span>
                    {% endif %}
                    
                    {% if article.relevance_scores.recency is defined %}
                        <span class="badge bg-success">
                            Recency: {{ (article.relevance_scores.recency * 100)|int }}%
                        </span>
                    {% endif %}
                {% endif %}
            </div>
        {% endif %}
        
        {% if article.keywords %}
            <div class="mb-4">
                <h5>Keywords</h5>
                <div>
                    {% for keyword in article.keywords %}
                        <span class="badge bg-light text-dark keyword-pill">{{ keyword }}</span>
                    {% endfor %}
                </div>
            </div>
        {% endif %}
        
        {% if article.summary %}
            <div class="card mb-4">
                <div class="card-header">Summary</div>
                <div class="card-body">
                    <p class="card-text">{{ article.summary }}</p>
                </div>
            </div>
        {% endif %}
        
        <div class="content mb-4">
            <h5>Content</h5>
            <div class="mt-3">
                {{ article.display_content }}
            </div>
        </div>
        
        {% if article.entities %}
            <div class="mb-4">
                <h5>Entities</h5>
                <div>
                    {% for entity in article.entities %}
                        <span class="badge bg-light text-dark keyword-pill" title="{{ entity.type }}">
                            {{ entity.text }}
                        </span>
                    {% endfor %}
                </div>
            </div>
        {% endif %}
    </article>
{% endblock %}
</file>

<file path="templates/articles.html">
{% extends "base.html" %}

{% block title %}Articles - Proton CRM{% endblock %}

{% block content %}
    <h1>Articles</h1>
    
    <div class="row mb-4">
        <div class="col-md-8">
            <form action="{{ url_for('list_articles') }}" method="get" class="row g-3">
                <div class="col-md-4">
                    <input type="text" class="form-control" name="q" placeholder="Search articles..." value="{{ search_query or '' }}">
                </div>
                <div class="col-md-3">
                    <select class="form-select" name="package">
                        <option value="">All Packages</option>
                        {% for pkg in packages %}
                            <option value="{{ pkg._id }}" {% if package_id and package_id == pkg._id|string %}selected{% endif %}>
                                {{ pkg.name }}
                            </option>
                        {% endfor %}
                    </select>
                </div>
                <div class="col-md-3">
                    <select class="form-select" name="sort">
                        <option value="date" {% if sort_by == 'date' %}selected{% endif %}>Sort by Date</option>
                        <option value="relevance" {% if sort_by == 'relevance' %}selected{% endif %}>Sort by Relevance</option>
                        <option value="title" {% if sort_by == 'title' %}selected{% endif %}>Sort by Title</option>
                    </select>
                </div>
                <div class="col-md-2">
                    <button type="submit" class="btn btn-primary w-100">Filter</button>
                </div>
            </form>
        </div>
        <div class="col-md-4 text-end">
            <p>{{ total_articles }} articles found</p>
        </div>
    </div>
    
    {% if articles %}
        {% for article in articles %}
            <div class="card article-card">
                <div class="card-body">
                    <div class="d-flex justify-content-between align-items-start mb-2">
                        <h5 class="card-title">
                            <a href="{{ url_for('view_article', article_id=article._id) }}">{{ article.title }}</a>
                        </h5>
                        {% if article.relevance_scores and article.relevance_scores.overall is defined %}
                            <span class="badge bg-primary relevance-score">
                                {{ (article.relevance_scores.overall * 100)|int }}% Relevance
                            </span>
                        {% endif %}
                    </div>
                    
                    <h6 class="card-subtitle mb-2 text-muted">
                        {% if article.source_name %}from {{ article.source_name }}{% endif %}
                        {% if article.display_date %} ‚Ä¢ {{ article.display_date }}{% endif %}
                        {% if article.scraping_package_name %}
                            ‚Ä¢ <span class="badge bg-secondary package-badge">{{ article.scraping_package_name }}</span>
                        {% endif %}
                    </h6>
                    
                    <p class="card-text">{{ article.content|truncate_html }}</p>
                    
                    {% if article.keywords %}
                        <div class="mt-2">
                            {% for keyword in article.keywords[:8] %}
                                <span class="badge bg-light text-dark keyword-pill">{{ keyword }}</span>
                            {% endfor %}
                            {% if article.keywords|length > 8 %}
                                <span class="badge bg-light text-dark">+{{ article.keywords|length - 8 }} more</span>
                            {% endif %}
                        </div>
                    {% endif %}
                </div>
            </div>
        {% endfor %}
        
        <!-- Pagination -->
        {% if total_pages > 1 %}
            <nav aria-label="Page navigation">
                <ul class="pagination justify-content-center">
                    <!-- Previous page link -->
                    <li class="page-item {% if current_page == 1 %}disabled{% endif %}">
                        <a class="page-link" href="{{ url_for('list_articles', page=current_page-1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                            Previous
                        </a>
                    </li>
                    
                    <!-- Page numbers -->
                    {% set start_page = [current_page - 2, 1]|max %}
                    {% set end_page = [start_page + 4, total_pages]|min %}
                    {% set start_page = [end_page - 4, 1]|max %}
                    
                    {% if start_page > 1 %}
                        <li class="page-item">
                            <a class="page-link" href="{{ url_for('list_articles', page=1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">1</a>
                        </li>
                        {% if start_page > 2 %}
                            <li class="page-item disabled"><span class="page-link">...</span></li>
                        {% endif %}
                    {% endif %}
                    
                    {% for page_num in range(start_page, end_page + 1) %}
                        <li class="page-item {% if page_num == current_page %}active{% endif %}">
                            <a class="page-link" href="{{ url_for('list_articles', page=page_num, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                                {{ page_num }}
                            </a>
                        </li>
                    {% endfor %}
                    
                    {% if end_page < total_pages %}
                        {% if end_page < total_pages - 1 %}
                            <li class="page-item disabled"><span class="page-link">...</span></li>
                        {% endif %}
                        <li class="page-item">
                            <a class="page-link" href="{{ url_for('list_articles', page=total_pages, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                                {{ total_pages }}
                            </a>
                        </li>
                    {% endif %}
                    
                    <!-- Next page link -->
                    <li class="page-item {% if current_page == total_pages %}disabled{% endif %}">
                        <a class="page-link" href="{{ url_for('list_articles', page=current_page+1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                            Next
                        </a>
                    </li>
                </ul>
            </nav>
        {% endif %}
    {% else %}
        <div class="alert alert-info">No articles found. Try changing your search criteria.</div>
    {% endif %}
{% endblock %}
</file>

<file path="templates/basic_index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agent Newsletter Generator</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .form-group { margin-bottom: 1rem; }
        #loading { display: none; }
        #streaming-window { display: none; }
        #result { display: none; }
        .card { margin-bottom: 20px; }
        
        /* This is the key styling for the streaming content */
        #streaming-content {
            height: 400px;
            overflow-y: auto;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 0.25rem;
            padding: 1rem;
            font-family: monospace;
            line-height: 1.6;
            font-size: 14px;
            color: #333;
            white-space: pre-line; /* This preserves line breaks */
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <h1>Agent Newsletter Generator</h1>
        </header>
        
        <div class="row">
            <div class="col-md-12">
                <div class="card">
                    <div class="card-header">
                        <h2>Generate Newsletter with AI Agent</h2>
                    </div>
                    <div class="card-body">
                        <form id="newsletter-form" method="post" enctype="multipart/form-data">
                            <div class="row">
                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="model_name"><strong>Model:</strong></label>
                                        <select class="form-control" id="model_name" name="model_name">
                                            {% for model in openai_models %}
                                                <option value="{{ model }}">OpenAI: {{ model }}</option>
                                            {% endfor %}
                                        </select>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="date_range"><strong>Date Range:</strong></label>
                                        <select class="form-control" id="date_range" name="date_range">
                                            <option value="all">All Time</option>
                                            <option value="7days">Last 7 Days</option>
                                            <option value="30days">Last 30 Days</option>
                                            <option value="90days">Last 90 Days</option>
                                        </select>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="package_ids"><strong>Scraping Packages:</strong></label>
                                        <select class="form-control" id="package_ids" name="package_ids" multiple size="5">
                                            {% for package in packages %}
                                                <option value="{{ package._id }}">{{ package.name|default('Unnamed Package') }}</option>
                                            {% endfor %}
                                        </select>
                                        <small class="form-text text-muted">Hold Ctrl/Cmd to select multiple packages. Leave empty to include all.</small>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="search_query"><strong>Search Query:</strong></label>
                                        <input type="text" class="form-control" id="search_query" name="search_query" placeholder="Enter search terms...">
                                        <small class="form-text text-muted">The agent will search for relevant articles using this query.</small>
                                    </div>
                                </div>
                                
                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="client_context"><strong>Client Context:</strong></label>
                                        <textarea class="form-control" id="client_context" name="client_context" rows="3" placeholder="Information about the client..."></textarea>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="project_context"><strong>Project Context:</strong></label>
                                        <textarea class="form-control" id="project_context" name="project_context" rows="3" placeholder="Information about the project..."></textarea>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="context_files"><strong>Additional Context Files:</strong></label>
                                        <input type="file" class="form-control" id="context_files" name="context_files" multiple>
                                        <small class="form-text text-muted">Upload TXT, PDF, MD, or DOCX files with additional context.</small>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="form-group mt-3">
                                <label for="prompt"><strong>Newsletter Instructions:</strong></label>
                                <textarea class="form-control" id="prompt" name="prompt" rows="5" placeholder="Detailed instructions for the newsletter..."></textarea>
                            </div>
                            
                            <div class="form-group mt-3">
                                <button type="submit" class="btn btn-primary" id="generate-btn">Generate Newsletter</button>
                            </div>
                        </form>
                        
                        <div id="loading" class="mt-4">
                            <div class="d-flex justify-content-center">
                                <div class="spinner-border text-primary" role="status">
                                    <span class="visually-hidden">Loading...</span>
                                </div>
                            </div>
                            <p class="text-center mt-2">Connecting to streaming service...</p>
                        </div>
                        
                        <!-- Streaming Window -->
                        <div id="streaming-window" class="mt-4">
                            <div class="card">
                                <div class="card-header">
                                    <h3>Agent Activity</h3>
                                </div>
                                <div class="card-body">
                                    <div id="streaming-content"></div>
                                </div>
                            </div>
                        </div>
                        
                        <div id="result" class="mt-4">
                            <div class="card">
                                <div class="card-header d-flex justify-content-between align-items-center">
                                    <h3>Generated Newsletter</h3>
                                    <button class="btn btn-sm btn-outline-secondary" id="copy-btn">Copy to Clipboard</button>
                                </div>
                                <div class="card-body">
                                    <div id="generated-content"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <footer class="mt-5 py-3 text-center text-secondary">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const form = document.getElementById('newsletter-form');
        const generateBtn = document.getElementById('generate-btn');
        const loading = document.getElementById('loading');
        const streamingWindow = document.getElementById('streaming-window');
        const streamingContent = document.getElementById('streaming-content');
        const result = document.getElementById('result');
        const generatedContent = document.getElementById('generated-content');
        const copyBtn = document.getElementById('copy-btn');
        
        form.addEventListener('submit', function(e) {
            e.preventDefault();
            
            // Clear previous content
            streamingContent.textContent = '';
            generatedContent.innerHTML = '';
            
            // Show loading indicator
            loading.style.display = 'block';
            streamingWindow.style.display = 'none';
            result.style.display = 'none';
            generateBtn.disabled = true;
            
            // Submit form data via AJAX
            const formData = new FormData(form);
            
            // First send the form data to start the generation process
            fetch('/generate', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    alert('Error: ' + data.error);
                    loading.style.display = 'none';
                    generateBtn.disabled = false;
                    return;
                }
                
                // Hide loading and show streaming window
                loading.style.display = 'none';
                streamingWindow.style.display = 'block';
                
                // Connect to the streaming endpoint
                const eventSource = new EventSource('/stream');
                
                // Collect the full content
                let fullContent = '';
                
                // Handle incoming messages
                eventSource.onmessage = function(event) {
                    if (event.data === '[DONE]') {
                        // End of stream
                        eventSource.close();
                        
                        // Show the final result
                        result.style.display = 'block';
                        generatedContent.innerHTML = formatContent(fullContent);
                        
                        // Re-enable the generate button
                        generateBtn.disabled = false;
                        
                        // Scroll to result
                        result.scrollIntoView({ behavior: 'smooth' });
                    } else if (event.data.trim() !== '') {
                        // Simply append the text - white-space: pre-line in CSS will handle line breaks
                        streamingContent.textContent += event.data;
                        streamingContent.scrollTop = streamingContent.scrollHeight;
                        
                        // Collect for final result
                        fullContent += event.data;
                    }
                };
                
                eventSource.onerror = function(error) {
                    console.error('EventSource error:', error);
                    eventSource.close();
                    generateBtn.disabled = false;
                    alert('Error in streaming. Please try again.');
                };
            })
            .catch(error => {
                console.error('Fetch error:', error);
                loading.style.display = 'none';
                generateBtn.disabled = false;
                alert('Error: ' + error);
            });
        });
        
        // Copy to clipboard functionality
        copyBtn.addEventListener('click', function() {
            const content = document.getElementById('generated-content').innerText;
            navigator.clipboard.writeText(content).then(() => {
                const originalText = copyBtn.textContent;
                copyBtn.textContent = 'Copied!';
                setTimeout(() => {
                    copyBtn.textContent = originalText;
                }, 2000);
            });
        });
        
        // Simple Markdown-like formatting
        function formatContent(text) {
            if (!text) return '';
            
            // Replace newlines with <br>
            text = text.replace(/\n/g, '<br>');
            
            // Format headers
            text = text.replace(/^# (.+)$/gm, '<h1>$1</h1>');
            text = text.replace(/^## (.+)$/gm, '<h2>$1</h2>');
            text = text.replace(/^### (.+)$/gm, '<h3>$1</h3>');
            
            // Format bold and italic
            text = text.replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>');
            text = text.replace(/\*(.+?)\*/g, '<em>$1</em>');
            
            // Format lists
            text = text.replace(/^- (.+)$/gm, '<li>$1</li>');
            text = text.replace(/(<li>.+<\/li>\n)+/g, '<ul>$&</ul>');
            
            return text;
        }
    });
    </script>
</body>
</html>
</file>

<file path="templates/fixed_index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agent Newsletter Generator</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .form-group { margin-bottom: 1rem; }
        #loading { display: none; }
        #streaming-window { display: none; }
        #result { display: none; }
        .card { margin-bottom: 20px; }
        
        /* This is the key styling for the streaming content */
        #streaming-content {
            height: 400px;
            overflow-y: auto;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 0.25rem;
            padding: 1rem;
            font-family: monospace;
            line-height: 1.6;
            font-size: 14px;
            color: #333;
            white-space: pre; /* Use pre instead of pre-line to preserve all whitespace */
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <h1>Agent Newsletter Generator</h1>
        </header>
        
        <div class="row">
            <div class="col-md-12">
                <div class="card">
                    <div class="card-header">
                        <h2>Generate Newsletter with AI Agent</h2>
                    </div>
                    <div class="card-body">
                        <form id="newsletter-form" method="post" enctype="multipart/form-data">
                            <div class="row">
                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="model_name"><strong>Model:</strong></label>
                                        <select class="form-control" id="model_name" name="model_name">
                                            {% for model in openai_models %}
                                                <option value="{{ model }}">OpenAI: {{ model }}</option>
                                            {% endfor %}
                                        </select>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="date_range"><strong>Date Range:</strong></label>
                                        <select class="form-control" id="date_range" name="date_range">
                                            <option value="all">All Time</option>
                                            <option value="7days">Last 7 Days</option>
                                            <option value="30days">Last 30 Days</option>
                                            <option value="90days">Last 90 Days</option>
                                        </select>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="package_ids"><strong>Scraping Packages:</strong></label>
                                        <select class="form-control" id="package_ids" name="package_ids" multiple size="5">
                                            {% for package in packages %}
                                                <option value="{{ package._id }}">{{ package.name|default('Unnamed Package') }}</option>
                                            {% endfor %}
                                        </select>
                                        <small class="form-text text-muted">Hold Ctrl/Cmd to select multiple packages. Leave empty to include all.</small>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="search_query"><strong>Search Query:</strong></label>
                                        <input type="text" class="form-control" id="search_query" name="search_query" placeholder="Enter search terms...">
                                        <small class="form-text text-muted">The agent will search for relevant articles using this query.</small>
                                    </div>
                                </div>
                                
                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="client_context"><strong>Client Context:</strong></label>
                                        <textarea class="form-control" id="client_context" name="client_context" rows="3" placeholder="Information about the client..."></textarea>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="project_context"><strong>Project Context:</strong></label>
                                        <textarea class="form-control" id="project_context" name="project_context" rows="3" placeholder="Information about the project..."></textarea>
                                    </div>
                                    
                                    <div class="form-group">
                                        <label for="context_files"><strong>Additional Context Files:</strong></label>
                                        <input type="file" class="form-control" id="context_files" name="context_files" multiple>
                                        <small class="form-text text-muted">Upload TXT, PDF, MD, or DOCX files with additional context.</small>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="form-group mt-3">
                                <label for="prompt"><strong>Newsletter Instructions:</strong></label>
                                <textarea class="form-control" id="prompt" name="prompt" rows="5" placeholder="Detailed instructions for the newsletter..."></textarea>
                            </div>
                            
                            <div class="form-group mt-3">
                                <button type="submit" class="btn btn-primary" id="generate-btn">Generate Newsletter</button>
                            </div>
                        </form>
                        
                        <div id="loading" class="mt-4">
                            <div class="d-flex justify-content-center">
                                <div class="spinner-border text-primary" role="status">
                                    <span class="visually-hidden">Loading...</span>
                                </div>
                            </div>
                            <p class="text-center mt-2">Connecting to streaming service...</p>
                        </div>
                        
                        <!-- Streaming Window -->
                        <div id="streaming-window" class="mt-4">
                            <div class="card">
                                <div class="card-header">
                                    <h3>Agent Activity</h3>
                                </div>
                                <div class="card-body">
                                    <div id="streaming-content"></div>
                                </div>
                            </div>
                        </div>
                        
                        <div id="result" class="mt-4">
                            <div class="card">
                                <div class="card-header d-flex justify-content-between align-items-center">
                                    <h3>Generated Newsletter</h3>
                                    <button class="btn btn-sm btn-outline-secondary" id="copy-btn">Copy to Clipboard</button>
                                </div>
                                <div class="card-body">
                                    <div id="generated-content"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <footer class="mt-5 py-3 text-center text-secondary">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const form = document.getElementById('newsletter-form');
        const generateBtn = document.getElementById('generate-btn');
        const loading = document.getElementById('loading');
        const streamingWindow = document.getElementById('streaming-window');
        const streamingContent = document.getElementById('streaming-content');
        const result = document.getElementById('result');
        const generatedContent = document.getElementById('generated-content');
        const copyBtn = document.getElementById('copy-btn');
        
        form.addEventListener('submit', function(e) {
            e.preventDefault();
            
            // Clear previous content
            streamingContent.textContent = '';
            generatedContent.innerHTML = '';
            
            // Show loading indicator
            loading.style.display = 'block';
            streamingWindow.style.display = 'none';
            result.style.display = 'none';
            generateBtn.disabled = true;
            
            // Submit form data via AJAX
            const formData = new FormData(form);
            
            // First send the form data to start the generation process
            fetch('/generate', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    alert('Error: ' + data.error);
                    loading.style.display = 'none';
                    generateBtn.disabled = false;
                    return;
                }
                
                // Hide loading and show streaming window
                loading.style.display = 'none';
                streamingWindow.style.display = 'block';
                
                // Connect to the streaming endpoint
                const eventSource = new EventSource('/stream');
                
                // Collect the full content
                let fullContent = '';
                
                // Handle incoming messages
                eventSource.onmessage = function(event) {
                    console.log('Received:', event.data);
                    
                    if (event.data === '[DONE]') {
                        // End of stream
                        eventSource.close();
                        console.log('Stream complete');
                        
                        // Show the final result
                        result.style.display = 'block';
                        generatedContent.innerHTML = formatContent(fullContent);
                        
                        // Re-enable the generate button
                        generateBtn.disabled = false;
                        
                        // Scroll to result
                        result.scrollIntoView({ behavior: 'smooth' });
                    } else if (event.data.trim() !== '') {
                        // Decode the SSE-encoded newlines
                        const decodedData = event.data.replace(/\\n/g, '\n');
                        
                        // Append to streaming content
                        streamingContent.textContent += decodedData;
                        
                        // Scroll to bottom
                        streamingContent.scrollTop = streamingContent.scrollHeight;
                        
                        // Collect for final result
                        fullContent += decodedData;
                    }
                };
                
                eventSource.onerror = function(error) {
                    console.error('EventSource error:', error);
                    eventSource.close();
                    generateBtn.disabled = false;
                    alert('Error in streaming. Please try again.');
                };
            })
            .catch(error => {
                console.error('Fetch error:', error);
                loading.style.display = 'none';
                generateBtn.disabled = false;
                alert('Error: ' + error);
            });
        });
        
        // Copy to clipboard functionality
        copyBtn.addEventListener('click', function() {
            const content = document.getElementById('generated-content').innerText;
            navigator.clipboard.writeText(content).then(() => {
                const originalText = copyBtn.textContent;
                copyBtn.textContent = 'Copied!';
                setTimeout(() => {
                    copyBtn.textContent = originalText;
                }, 2000);
            });
        });
        
        // Simple Markdown-like formatting
        function formatContent(text) {
            if (!text) return '';
            
            // Replace newlines with <br>
            text = text.replace(/\n/g, '<br>');
            
            // Format headers
            text = text.replace(/^# (.+)$/gm, '<h1>$1</h1>');
            text = text.replace(/^## (.+)$/gm, '<h2>$1</h2>');
            text = text.replace(/^### (.+)$/gm, '<h3>$1</h3>');
            
            // Format bold and italic
            text = text.replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>');
            text = text.replace(/\*(.+?)\*/g, '<em>$1</em>');
            
            // Format lists
            text = text.replace(/^- (.+)$/gm, '<li>$1</li>');
            text = text.replace(/(<li>.+<\/li>\n)+/g, '<ul>$&</ul>');
            
            return text;
        }
    });
    </script>
</body>
</html>
</file>

<file path="templates/index.html">
{% extends "base.html" %}

{% block title %}Agent Newsletter Generator{% endblock %}

{% block content %}
<div class="row">
    <div class="col-md-12">
        <div class="card">
            <div class="card-header">
                <h2>Generate Newsletter with AI Agent</h2>
            </div>
            <div class="card-body">
                <form id="newsletter-form" method="post" action="{{ url_for('generate') }}" enctype="multipart/form-data">
                    <div class="row">
                        <div class="col-md-6">
                            <div class="form-group">
                                <label for="model_name"><strong>Model:</strong></label>
                                <select class="form-control" id="model_name" name="model_name">
                                    {% for model in openai_models %}
                                        <option value="{{ model }}">OpenAI: {{ model }}</option>
                                    {% endfor %}
                                </select>
                            </div>

                            <div class="form-group">
                                <label for="date_range"><strong>Date Range:</strong></label>
                                <select class="form-control" id="date_range" name="date_range">
                                    <option value="all">All Time</option>
                                    <option value="7days">Last 7 Days</option>
                                    <option value="30days">Last 30 Days</option>
                                    <option value="90days">Last 90 Days</option>
                                </select>
                            </div>

                            <div class="form-group">
                                <label for="package_ids"><strong>Scraping Packages:</strong></label>
                                <select class="form-control" id="package_ids" name="package_ids" multiple size="5">
                                    {% for package in packages %}
                                        <option value="{{ package._id }}">{{ package.name|default('Unnamed Package') }}</option>
                                    {% endfor %}
                                </select>
                                <small class="form-text text-muted">Hold Ctrl/Cmd to select multiple packages. Leave empty to include all.</small>
                            </div>

                            <div class="form-group">
                                <label for="search_query"><strong>Search Query:</strong></label>
                                <input type="text" class="form-control" id="search_query" name="search_query" placeholder="Enter search terms...">
                                <small class="form-text text-muted">The agent will search for relevant articles using this query.</small>
                            </div>
                        </div>

                        <div class="col-md-6">
                            <div class="form-group">
                                <label for="client_context"><strong>Client Context:</strong></label>
                                <textarea class="form-control" id="client_context" name="client_context" rows="3" placeholder="Information about the client..."></textarea>
                            </div>

                            <div class="form-group">
                                <label for="project_context"><strong>Project Context:</strong></label>
                                <textarea class="form-control" id="project_context" name="project_context" rows="3" placeholder="Information about the project..."></textarea>
                            </div>

                            <div class="form-group">
                                <label for="context_files"><strong>Additional Context Files:</strong></label>
                                <input type="file" class="form-control" id="context_files" name="context_files" multiple>
                                <small class="form-text text-muted">Upload TXT, PDF, MD, or DOCX files with additional context.</small>
                            </div>
                        </div>
                    </div>

                    <div class="form-group mt-3">
                        <label for="prompt"><strong>Newsletter Instructions:</strong></label>
                        <textarea class="form-control" id="prompt" name="prompt" rows="5" placeholder="Detailed instructions for the newsletter..."></textarea>
                    </div>

                    <div class="form-group mt-3">
                        <button type="submit" class="btn btn-primary" id="generate-btn">Generate Newsletter</button>
                    </div>
                </form>

                <div id="loading" class="mt-4">
                    <div class="d-flex justify-content-center">
                        <div class="spinner-border text-primary" role="status">
                            <span class="visually-hidden">Loading...</span>
                        </div>
                    </div>
                    <p class="text-center mt-2">Generating newsletter... This may take a minute.</p>
                </div>

                <div id="result" class="mt-4">
                    <div class="card">
                        <div class="card-header d-flex justify-content-between align-items-center">
                            <h3>Generated Newsletter</h3>
                            <button class="btn btn-sm btn-outline-secondary" id="copy-btn">Copy to Clipboard</button>
                        </div>
                        <div class="card-body">
                            <div id="generated-content"></div>
                        </div>
                    </div>

                    <div class="card mt-3">
                        <div class="card-header">
                            <h3>Context Used <small class="text-muted">(<span id="articles-count">0</span> articles)</small></h3>
                        </div>
                        <div class="card-body">
                            <pre id="context-used" class="border p-3 bg-light" style="max-height: 300px; overflow-y: auto;"></pre>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
document.addEventListener('DOMContentLoaded', function() {
    const form = document.getElementById('newsletter-form');
    const generateBtn = document.getElementById('generate-btn');
    const loading = document.getElementById('loading');
    const result = document.getElementById('result');
    const generatedContent = document.getElementById('generated-content');
    const contextUsed = document.getElementById('context-used');
    const articlesCount = document.getElementById('articles-count');
    const copyBtn = document.getElementById('copy-btn');

    form.addEventListener('submit', function(e) {
        e.preventDefault();

        // Show loading indicator
        loading.style.display = 'block';
        result.style.display = 'none';
        generateBtn.disabled = true;

        // Submit form data via AJAX
        const formData = new FormData(form);

        fetch('/generate', {
            method: 'POST',
            body: formData
        })
        .then(response => response.json())
        .then(data => {
            // Hide loading indicator
            loading.style.display = 'none';
            generateBtn.disabled = false;

            if (data.error) {
                alert('Error: ' + data.error);
                return;
            }

            // Show result
            result.style.display = 'block';

            // Format and display generated content with Markdown
            generatedContent.innerHTML = formatContent(data.generated_content);

            // Display context used
            contextUsed.textContent = data.context_used;

            // Update articles count
            articlesCount.textContent = data.articles_count;

            // Scroll to result
            result.scrollIntoView({ behavior: 'smooth' });
        })
        .catch(error => {
            loading.style.display = 'none';
            generateBtn.disabled = false;
            alert('Error: ' + error);
        });
    });

    // Copy to clipboard functionality
    copyBtn.addEventListener('click', function() {
        const content = document.getElementById('generated-content').innerText;
        navigator.clipboard.writeText(content).then(() => {
            const originalText = copyBtn.textContent;
            copyBtn.textContent = 'Copied!';
            setTimeout(() => {
                copyBtn.textContent = originalText;
            }, 2000);
        });
    });

    // Simple Markdown-like formatting
    function formatContent(text) {
        if (!text) return '';

        // Replace newlines with <br>
        text = text.replace(/\n/g, '<br>');

        // Format headers
        text = text.replace(/^# (.+)$/gm, '<h1>$1</h1>');
        text = text.replace(/^## (.+)$/gm, '<h2>$1</h2>');
        text = text.replace(/^### (.+)$/gm, '<h3>$1</h3>');

        // Format bold and italic
        text = text.replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>');
        text = text.replace(/\*(.+?)\*/g, '<em>$1</em>');

        // Format lists
        text = text.replace(/^- (.+)$/gm, '<li>$1</li>');
        text = text.replace(/(<li>.+<\/li>\n)+/g, '<ul>$&</ul>');

        return text;
    }
});
</script>
{% endblock %}
</file>

<file path="templates/package.html">
{% extends "base.html" %}

{% block title %}{{ package.name }} - Proton CRM{% endblock %}

{% block content %}
    <div class="mb-3">
        <a href="{{ url_for('list_packages') }}" class="btn btn-outline-secondary btn-sm">
            &larr; Back to Packages
        </a>
    </div>

    <div class="card mb-4">
        <div class="card-body">
            <h1>{{ package.name }}</h1>
            <p class="mb-2">{{ package.description }}</p>
            
            <div class="mb-3">
                <span class="badge bg-{{ 'success' if package.status == 'active' else 'secondary' }}">
                    {{ package.status|title }}
                </span>
                <span class="badge bg-info text-dark">
                    Schedule: {{ package.schedule_interval }}
                </span>
                <span class="badge bg-info text-dark">
                    Max Articles: {{ package.max_articles_per_run }}
                </span>
            </div>
            
            <div class="row">
                <div class="col-md-6">
                    <h5>RSS Feeds</h5>
                    <ul class="list-group">
                        {% for feed in package.rss_feeds %}
                            <li class="list-group-item">{{ feed }}</li>
                        {% endfor %}
                    </ul>
                </div>
                <div class="col-md-6">
                    <h5>Statistics</h5>
                    <ul class="list-group">
                        <li class="list-group-item">Total Articles: {{ package.article_count or 0 }}</li>
                        <li class="list-group-item">Articles Last Run: {{ package.articles_last_run or 0 }}</li>
                        <li class="list-group-item">Last Run: {{ package.last_run_display }}</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    
    <h2>Recent Articles ({{ articles|length }})</h2>
    <div class="mb-2">
        <a href="{{ url_for('list_articles', package=package._id) }}" class="btn btn-primary">
            View All Articles from this Package
        </a>
    </div>
    
    {% if articles %}
        {% for article in articles %}
            <div class="card article-card">
                <div class="card-body">
                    <div class="d-flex justify-content-between align-items-start mb-2">
                        <h5 class="card-title">
                            <a href="{{ url_for('view_article', article_id=article._id) }}">{{ article.title }}</a>
                        </h5>
                        {% if article.relevance_scores and article.relevance_scores.overall is defined %}
                            <span class="badge bg-primary relevance-score">
                                {{ (article.relevance_scores.overall * 100)|int }}% Relevance
                            </span>
                        {% endif %}
                    </div>
                    
                    <h6 class="card-subtitle mb-2 text-muted">
                        {% if article.source_name %}from {{ article.source_name }}{% endif %}
                        {% if article.display_date %} ‚Ä¢ {{ article.display_date }}{% endif %}
                    </h6>
                    
                    <p class="card-text">{{ article.content|truncate_html }}</p>
                    
                    {% if article.keywords %}
                        <div class="mt-2">
                            {% for keyword in article.keywords[:8] %}
                                <span class="badge bg-light text-dark keyword-pill">{{ keyword }}</span>
                            {% endfor %}
                            {% if article.keywords|length > 8 %}
                                <span class="badge bg-light text-dark">+{{ article.keywords|length - 8 }} more</span>
                            {% endif %}
                        </div>
                    {% endif %}
                </div>
            </div>
        {% endfor %}
    {% else %}
        <div class="alert alert-info">No articles found for this package.</div>
    {% endif %}
{% endblock %}
</file>

<file path="templates/packages.html">
{% extends "base.html" %}

{% block title %}Packages - Proton CRM{% endblock %}

{% block content %}
    <h1>Scraping Packages</h1>
    
    <div class="row mb-4">
        <div class="col">
            <p>Viewing {{ packages|length }} scraping packages</p>
        </div>
    </div>
    
    <div class="row">
        {% for package in packages %}
            <div class="col-md-4 mb-4">
                <div class="card h-100">
                    <div class="card-body">
                        <h5 class="card-title">
                            <a href="{{ url_for('view_package', package_id=package._id) }}">{{ package.name }}</a>
                        </h5>
                        <h6 class="card-subtitle mb-2 text-muted">
                            {{ package.status }} ‚Ä¢ {{ package.rss_feeds|length }} feeds
                        </h6>
                        <p class="card-text">{{ package.description }}</p>
                    </div>
                    <div class="card-footer">
                        <div class="d-flex justify-content-between align-items-center">
                            <span>{{ package.article_count or 0 }} articles</span>
                            <small class="text-muted">Last run: {{ package.last_run_display }}</small>
                        </div>
                    </div>
                </div>
            </div>
        {% endfor %}
    </div>
{% endblock %}
</file>

<file path="templates/simple_index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agent Newsletter Generator</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .form-group { margin-bottom: 1rem; }
        #loading { display: none; }
        #streaming-window { display: none; }
        #result { display: none; }
        .card { margin-bottom: 20px; }
        #streaming-content {
            height: 300px;
            overflow-y: auto;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 0.25rem;
            padding: 1rem;
            font-family: monospace;
            line-height: 1.5;
            font-size: 14px;
            color: #333;
            text-align: left;
        }
        /* Add spacing between paragraphs */
        #streaming-content br {
            display: block;
            margin-top: 0.5em;
            content: "";
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <h1>Agent Newsletter Generator</h1>
        </header>

        <div class="row">
            <div class="col-md-12">
                <div class="card">
                    <div class="card-header">
                        <h2>Generate Newsletter with AI Agent</h2>
                    </div>
                    <div class="card-body">
                        <form id="newsletter-form" method="post">
                            <div class="row">
                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="model_name"><strong>Model:</strong></label>
                                        <select class="form-control" id="model_name" name="model_name">
                                            {% for model in openai_models %}
                                                <option value="{{ model }}">OpenAI: {{ model }}</option>
                                            {% endfor %}
                                        </select>
                                    </div>

                                    <div class="form-group">
                                        <label for="date_range"><strong>Date Range:</strong></label>
                                        <select class="form-control" id="date_range" name="date_range">
                                            <option value="all">All Time</option>
                                            <option value="7days">Last 7 Days</option>
                                            <option value="30days">Last 30 Days</option>
                                            <option value="90days">Last 90 Days</option>
                                        </select>
                                    </div>

                                    <div class="form-group">
                                        <label for="package_ids"><strong>Scraping Packages:</strong></label>
                                        <select class="form-control" id="package_ids" name="package_ids" multiple size="5">
                                            {% for package in packages %}
                                                <option value="{{ package._id }}">{{ package.name|default('Unnamed Package') }}</option>
                                            {% endfor %}
                                        </select>
                                        <small class="form-text text-muted">Hold Ctrl/Cmd to select multiple packages. Leave empty to include all.</small>
                                    </div>

                                    <div class="form-group">
                                        <label for="search_query"><strong>Search Query:</strong></label>
                                        <input type="text" class="form-control" id="search_query" name="search_query" placeholder="Enter search terms...">
                                        <small class="form-text text-muted">The agent will search for relevant articles using this query.</small>
                                    </div>
                                </div>

                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="client_context"><strong>Client Context:</strong></label>
                                        <textarea class="form-control" id="client_context" name="client_context" rows="3" placeholder="Information about the client..."></textarea>
                                    </div>

                                    <div class="form-group">
                                        <label for="project_context"><strong>Project Context:</strong></label>
                                        <textarea class="form-control" id="project_context" name="project_context" rows="3" placeholder="Information about the project..."></textarea>
                                    </div>
                                </div>
                            </div>

                            <div class="form-group mt-3">
                                <label for="prompt"><strong>Newsletter Instructions:</strong></label>
                                <textarea class="form-control" id="prompt" name="prompt" rows="5" placeholder="Detailed instructions for the newsletter..."></textarea>
                            </div>

                            <div class="form-group mt-3">
                                <button type="submit" class="btn btn-primary" id="generate-btn">Generate Newsletter</button>
                            </div>
                        </form>

                        <div id="loading" class="mt-4">
                            <div class="d-flex justify-content-center">
                                <div class="spinner-border text-primary" role="status">
                                    <span class="visually-hidden">Loading...</span>
                                </div>
                            </div>
                            <p class="text-center mt-2">Connecting to streaming service...</p>
                        </div>

                        <!-- Streaming Window -->
                        <div id="streaming-window" class="mt-4">
                            <div class="card">
                                <div class="card-header">
                                    <h3>Agent Activity</h3>
                                </div>
                                <div class="card-body">
                                    <div id="streaming-content"></div>
                                </div>
                            </div>
                        </div>

                        <div id="result" class="mt-4">
                            <div class="card">
                                <div class="card-header d-flex justify-content-between align-items-center">
                                    <h3>Generated Newsletter</h3>
                                    <button class="btn btn-sm btn-outline-secondary" id="copy-btn">Copy to Clipboard</button>
                                </div>
                                <div class="card-body">
                                    <div id="generated-content"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <footer class="mt-5 py-3 text-center text-secondary">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const form = document.getElementById('newsletter-form');
        const generateBtn = document.getElementById('generate-btn');
        const loading = document.getElementById('loading');
        const streamingWindow = document.getElementById('streaming-window');
        const streamingContent = document.getElementById('streaming-content');
        const result = document.getElementById('result');
        const generatedContent = document.getElementById('generated-content');
        const copyBtn = document.getElementById('copy-btn');

        form.addEventListener('submit', function(e) {
            e.preventDefault();

            // Clear previous content
            streamingContent.textContent = '';
            generatedContent.innerHTML = '';

            // Show loading indicator
            loading.style.display = 'block';
            streamingWindow.style.display = 'none';
            result.style.display = 'none';
            generateBtn.disabled = true;

            // Submit form data via AJAX
            const formData = new FormData(form);

            // First send the form data to start the generation process
            fetch('/generate', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    alert('Error: ' + data.error);
                    loading.style.display = 'none';
                    generateBtn.disabled = false;
                    return;
                }

                // Hide loading and show streaming window
                loading.style.display = 'none';
                streamingWindow.style.display = 'block';

                // Connect to the streaming endpoint
                const eventSource = new EventSource('/stream');

                // Collect the full content
                let fullContent = '';

                // Handle incoming messages
                eventSource.onmessage = function(event) {
                    console.log('Received event:', event.data);

                    if (event.data === '[DONE]') {
                        // End of stream
                        console.log('Stream complete');
                        eventSource.close();

                        // Show the final result
                        result.style.display = 'block';
                        generatedContent.innerHTML = formatContent(fullContent);

                        // Re-enable the generate button
                        generateBtn.disabled = false;

                        // Scroll to result
                        result.scrollIntoView({ behavior: 'smooth' });
                    } else if (event.data.trim() !== '') {
                        // Process the content to ensure line breaks are preserved
                        const formattedContent = event.data.replace(/\n/g, '<br>');

                        // Append to streaming content using innerHTML to preserve HTML formatting
                        const tempDiv = document.createElement('div');
                        tempDiv.innerHTML = formattedContent;

                        // Create a text node for plain text content
                        const textNode = document.createTextNode(event.data);

                        // Append the formatted content to the streaming window
                        streamingContent.innerHTML += formattedContent;
                        streamingContent.scrollTop = streamingContent.scrollHeight;

                        // Collect for final result (use the original text)
                        fullContent += event.data;

                        // Add a small delay to improve readability of streaming content
                        setTimeout(() => {
                            streamingContent.scrollTop = streamingContent.scrollHeight;
                        }, 10);
                    }
                };

                eventSource.onerror = function(error) {
                    console.error('EventSource error:', error);
                    eventSource.close();
                    generateBtn.disabled = false;
                    alert('Error in streaming. Please try again.');
                };
            })
            .catch(error => {
                console.error('Fetch error:', error);
                loading.style.display = 'none';
                generateBtn.disabled = false;
                alert('Error: ' + error);
            });
        });

        // Copy to clipboard functionality
        copyBtn.addEventListener('click', function() {
            const content = document.getElementById('generated-content').innerText;
            navigator.clipboard.writeText(content).then(() => {
                const originalText = copyBtn.textContent;
                copyBtn.textContent = 'Copied!';
                setTimeout(() => {
                    copyBtn.textContent = originalText;
                }, 2000);
            });
        });

        // Simple Markdown-like formatting
        function formatContent(text) {
            if (!text) return '';

            // Replace newlines with <br>
            text = text.replace(/\n/g, '<br>');

            // Format headers
            text = text.replace(/^# (.+)$/gm, '<h1>$1</h1>');
            text = text.replace(/^## (.+)$/gm, '<h2>$1</h2>');
            text = text.replace(/^### (.+)$/gm, '<h3>$1</h3>');

            // Format bold and italic
            text = text.replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>');
            text = text.replace(/\*(.+?)\*/g, '<em>$1</em>');

            // Format lists
            text = text.replace(/^- (.+)$/gm, '<li>$1</li>');
            text = text.replace(/(<li>.+<\/li>\n)+/g, '<ul>$&</ul>');

            return text;
        }
    });
    </script>
</body>
</html>
</file>

<file path="templates/test_streaming.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Streaming</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        
        h1 {
            color: #333;
        }
        
        #output {
            border: 1px solid #ccc;
            padding: 10px;
            min-height: 200px;
            background-color: #f9f9f9;
            white-space: pre;  /* Preserve line breaks */
            font-family: monospace;
            overflow-y: auto;
            margin-bottom: 20px;
        }
        
        button {
            padding: 10px 15px;
            background-color: #4CAF50;
            color: white;
            border: none;
            cursor: pointer;
        }
        
        button:hover {
            background-color: #45a049;
        }
    </style>
</head>
<body>
    <h1>Test Streaming with Line Breaks</h1>
    
    <div id="output"></div>
    
    <button id="start-btn">Start Streaming</button>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const outputDiv = document.getElementById('output');
            const startBtn = document.getElementById('start-btn');
            
            startBtn.addEventListener('click', function() {
                // Clear previous content
                outputDiv.textContent = '';
                
                // Connect to the streaming endpoint
                const eventSource = new EventSource('/stream');
                
                // Handle incoming messages
                eventSource.onmessage = function(event) {
                    console.log('Received:', event.data);
                    
                    if (event.data === '[DONE]') {
                        // End of stream
                        eventSource.close();
                        console.log('Stream complete');
                    } else if (event.data.trim() !== '') {
                        // Decode the SSE-encoded newlines
                        const decodedData = event.data.replace(/\\n/g, '\n');
                        
                        // Append to output
                        outputDiv.textContent += decodedData;
                        
                        // Scroll to bottom
                        outputDiv.scrollTop = outputDiv.scrollHeight;
                    }
                };
                
                eventSource.onerror = function(error) {
                    console.error('EventSource error:', error);
                    eventSource.close();
                };
            });
        });
    </script>
</body>
</html>
</file>

<file path="xx/rag_tester/templates/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Newsletter Tester</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css">
    <style>
        body {
            padding-top: 20px;
            padding-bottom: 40px;
        }
        .form-section {
            margin-bottom: 30px;
        }
        .result-section {
            margin-top: 30px;
        }
        .loading {
            display: none;
            text-align: center;
            margin: 20px 0;
        }
        .spinner-border {
            width: 3rem;
            height: 3rem;
        }
        #generatedContent {
            white-space: pre-wrap;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            padding: 15px;
            border-radius: 5px;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
        }
        .context-toggle {
            cursor: pointer;
            color: #0d6efd;
            text-decoration: underline;
        }
        #contextUsed {
            white-space: pre-wrap;
            font-family: monospace;
            font-size: 0.9em;
            padding: 15px;
            border-radius: 5px;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            display: none;
        }
        .flash-messages {
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="mb-4">RAG Newsletter Testing Environment</h1>
        
        <!-- Flash Messages -->
        <div class="flash-messages">
            {% with messages = get_flashed_messages(with_categories=true) %}
                {% if messages %}
                    {% for category, message in messages %}
                        <div class="alert alert-{{ category if category != 'message' else 'info' }} alert-dismissible fade show" role="alert">
                            {{ message }}
                            <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
                        </div>
                    {% endfor %}
                {% endif %}
            {% endwith %}
        </div>
        
        <form id="generationForm" method="post" action="/generate" enctype="multipart/form-data">
            <div class="row">
                <!-- Left Column: Configuration -->
                <div class="col-md-6">
                    <!-- Model Selection -->
                    <div class="form-section card">
                        <div class="card-header bg-primary text-white">
                            <h5 class="mb-0">Model Selection</h5>
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label class="form-label">Model Provider</label>
                                <div class="form-check">
                                    <input class="form-check-input" type="radio" name="model_provider" id="providerOpenAI" value="openai" checked>
                                    <label class="form-check-label" for="providerOpenAI">
                                        OpenAI
                                    </label>
                                </div>
                                <div class="form-check">
                                    <input class="form-check-input" type="radio" name="model_provider" id="providerAnthropic" value="anthropic">
                                    <label class="form-check-label" for="providerAnthropic">
                                        Anthropic (Claude)
                                    </label>
                                </div>
                            </div>
                            
                            <div class="mb-3" id="openaiModelsSection">
                                <label for="openaiModel" class="form-label">OpenAI Model</label>
                                <select class="form-select" id="openaiModel" name="model_name">
                                    {% for model in openai_models %}
                                        <option value="{{ model }}">{{ model }}</option>
                                    {% else %}
                                        <option value="gpt-4o" disabled>No OpenAI models available (API key missing)</option>
                                    {% endfor %}
                                </select>
                            </div>
                            
                            <div class="mb-3" id="anthropicModelsSection" style="display: none;">
                                <label for="anthropicModel" class="form-label">Anthropic Model</label>
                                <select class="form-select" id="anthropicModel" name="model_name">
                                    {% for model in anthropic_models %}
                                        <option value="{{ model }}">{{ model }}</option>
                                    {% else %}
                                        <option value="claude-3-opus-20240229" disabled>No Anthropic models available (API key missing)</option>
                                    {% endfor %}
                                </select>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Data Filtering -->
                    <div class="form-section card mt-4">
                        <div class="card-header bg-primary text-white">
                            <h5 class="mb-0">Data Filtering</h5>
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label for="dateRange" class="form-label">Date Range</label>
                                <select class="form-select" id="dateRange" name="date_range">
                                    <option value="all">All Time</option>
                                    <option value="7days">Last 7 Days</option>
                                    <option value="30days">Last 30 Days</option>
                                    <option value="90days">Last 90 Days</option>
                                </select>
                            </div>
                            
                            <div class="mb-3">
                                <label class="form-label">Scraping Packages</label>
                                <div class="card" style="max-height: 200px; overflow-y: auto;">
                                    <div class="card-body p-2">
                                        {% for package in packages %}
                                            <div class="form-check">
                                                <input class="form-check-input" type="checkbox" name="package_ids" id="package{{ package._id }}" value="{{ package._id }}">
                                                <label class="form-check-label" for="package{{ package._id }}">
                                                    {{ package.name }}
                                                </label>
                                            </div>
                                        {% else %}
                                            <p class="text-muted">No packages available</p>
                                        {% endfor %}
                                    </div>
                                </div>
                            </div>
                            
                            <div class="mb-3">
                                <label for="searchQuery" class="form-label">Search Query (for RAG)</label>
                                <input type="text" class="form-control" id="searchQuery" name="search_query" placeholder="Enter search terms for relevant articles">
                            </div>
                        </div>
                    </div>
                    
                    <!-- Context Files -->
                    <div class="form-section card mt-4">
                        <div class="card-header bg-primary text-white">
                            <h5 class="mb-0">Additional Context</h5>
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label for="contextFiles" class="form-label">Upload Context Files</label>
                                <input class="form-control" type="file" id="contextFiles" name="context_files" multiple>
                                <div class="form-text">Supported formats: TXT, PDF, MD, DOCX</div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <!-- Right Column: Prompt and Generation -->
                <div class="col-md-6">
                    <!-- Client and Project Context -->
                    <div class="form-section card">
                        <div class="card-header bg-primary text-white">
                            <h5 class="mb-0">Client & Project Context</h5>
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label for="clientContext" class="form-label">Client Context</label>
                                <textarea class="form-control" id="clientContext" name="client_context" rows="3" placeholder="Enter information about the client..."></textarea>
                            </div>
                            
                            <div class="mb-3">
                                <label for="projectContext" class="form-label">Project Context</label>
                                <textarea class="form-control" id="projectContext" name="project_context" rows="3" placeholder="Enter information about the project..."></textarea>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Prompt -->
                    <div class="form-section card mt-4">
                        <div class="card-header bg-primary text-white">
                            <h5 class="mb-0">Newsletter Prompt</h5>
                        </div>
                        <div class="card-body">
                            <div class="mb-3">
                                <label for="prompt" class="form-label">Prompt Instructions</label>
                                <textarea class="form-control" id="prompt" name="prompt" rows="6" placeholder="Enter detailed instructions for the newsletter generation..."></textarea>
                            </div>
                            
                            <button type="submit" class="btn btn-primary w-100" id="generateBtn">
                                <i class="bi bi-lightning-charge"></i> Generate Newsletter
                            </button>
                        </div>
                    </div>
                </div>
            </div>
        </form>
        
        <!-- Loading Indicator -->
        <div class="loading" id="loadingIndicator">
            <div class="spinner-border text-primary" role="status">
                <span class="visually-hidden">Loading...</span>
            </div>
            <p class="mt-2">Generating newsletter content...</p>
        </div>
        
        <!-- Results Section -->
        <div class="result-section" id="resultsSection" style="display: none;">
            <div class="card">
                <div class="card-header bg-success text-white">
                    <h5 class="mb-0">Generated Newsletter</h5>
                </div>
                <div class="card-body">
                    <div id="generatedContent"></div>
                    
                    <div class="mt-3">
                        <p>
                            <span class="context-toggle" id="toggleContext">Show Context Used</span> 
                            <span id="articlesCount" class="badge bg-info ms-2"></span>
                        </p>
                        <div id="contextUsed"></div>
                    </div>
                    
                    <div class="mt-3">
                        <button class="btn btn-outline-primary" id="copyBtn">
                            <i class="bi bi-clipboard"></i> Copy to Clipboard
                        </button>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Model provider toggle
            const providerOpenAI = document.getElementById('providerOpenAI');
            const providerAnthropic = document.getElementById('providerAnthropic');
            const openaiModelsSection = document.getElementById('openaiModelsSection');
            const anthropicModelsSection = document.getElementById('anthropicModelsSection');
            const openaiModel = document.getElementById('openaiModel');
            const anthropicModel = document.getElementById('anthropicModel');
            
            providerOpenAI.addEventListener('change', function() {
                if (this.checked) {
                    openaiModelsSection.style.display = 'block';
                    anthropicModelsSection.style.display = 'none';
                    openaiModel.setAttribute('name', 'model_name');
                    anthropicModel.removeAttribute('name');
                }
            });
            
            providerAnthropic.addEventListener('change', function() {
                if (this.checked) {
                    openaiModelsSection.style.display = 'none';
                    anthropicModelsSection.style.display = 'block';
                    anthropicModel.setAttribute('name', 'model_name');
                    openaiModel.removeAttribute('name');
                }
            });
            
            // Form submission
            const generationForm = document.getElementById('generationForm');
            const loadingIndicator = document.getElementById('loadingIndicator');
            const resultsSection = document.getElementById('resultsSection');
            const generatedContent = document.getElementById('generatedContent');
            const contextUsed = document.getElementById('contextUsed');
            const articlesCount = document.getElementById('articlesCount');
            const toggleContext = document.getElementById('toggleContext');
            const copyBtn = document.getElementById('copyBtn');
            
            generationForm.addEventListener('submit', function(e) {
                e.preventDefault();
                
                // Show loading indicator
                loadingIndicator.style.display = 'block';
                resultsSection.style.display = 'none';
                
                // Create FormData object
                const formData = new FormData(generationForm);
                
                // Send AJAX request
                fetch('/generate', {
                    method: 'POST',
                    body: formData
                })
                .then(response => response.json())
                .then(data => {
                    // Hide loading indicator
                    loadingIndicator.style.display = 'none';
                    
                    if (data.error) {
                        alert('Error: ' + data.error);
                        return;
                    }
                    
                    // Display results
                    generatedContent.textContent = data.generated_content;
                    contextUsed.textContent = data.context_used;
                    articlesCount.textContent = data.articles_count + ' articles used';
                    resultsSection.style.display = 'block';
                    
                    // Scroll to results
                    resultsSection.scrollIntoView({ behavior: 'smooth' });
                })
                .catch(error => {
                    loadingIndicator.style.display = 'none';
                    alert('Error: ' + error.message);
                });
            });
            
            // Toggle context display
            toggleContext.addEventListener('click', function() {
                if (contextUsed.style.display === 'none') {
                    contextUsed.style.display = 'block';
                    toggleContext.textContent = 'Hide Context Used';
                } else {
                    contextUsed.style.display = 'none';
                    toggleContext.textContent = 'Show Context Used';
                }
            });
            
            // Copy to clipboard
            copyBtn.addEventListener('click', function() {
                navigator.clipboard.writeText(generatedContent.textContent)
                    .then(() => {
                        const originalText = copyBtn.innerHTML;
                        copyBtn.innerHTML = '<i class="bi bi-check"></i> Copied!';
                        setTimeout(() => {
                            copyBtn.innerHTML = originalText;
                        }, 2000);
                    })
                    .catch(err => {
                        alert('Failed to copy: ' + err);
                    });
            });
        });
    </script>
</body>
</html>
</file>

<file path="xx/rag_tester/.gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
env/

# Environment variables
.env

# Uploads folder
uploads/

# IDE files
.idea/
.vscode/
*.swp
*.swo

# Logs
*.log
</file>

<file path="xx/rag_tester/app.py">
"""
RAG Testing Environment for Newsletter Generation

This Flask application provides a UI for testing RAG-based newsletter generation
with different models, filters, and context options.
"""

import os
import datetime
import logging
from flask import Flask, render_template, request, jsonify, flash
from werkzeug.utils import secure_filename
from dotenv import load_dotenv
import pymongo
from bson.objectid import ObjectId
import openai
import anthropic
import PyPDF2
import io

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')

# Configure upload folder
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}

# MongoDB connection
MONGODB_URI = os.environ.get('MONGODB_URI')
MONGODB_DB_NAME = os.environ.get('MONGODB_DB_NAME', 'proton')

# Initialize API clients
openai_client = None
anthropic_client = None

def init_api_clients():
    """Initialize API clients for OpenAI and Anthropic"""
    global openai_client, anthropic_client

    # Initialize OpenAI client
    openai_api_key = os.environ.get('OPENAI_API_KEY')
    if openai_api_key:
        try:
            openai_client = openai.OpenAI(api_key=openai_api_key)
            logger.info("OpenAI client initialized")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")

    # Initialize Anthropic client
    anthropic_api_key = os.environ.get('ANTHROPIC_API_KEY')
    if anthropic_api_key:
        try:
            # Check Anthropic version
            anthropic_version = getattr(anthropic, '__version__', 'unknown')
            logger.info(f"Detected Anthropic version: {anthropic_version}")

            # Initialize client based on version
            anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)

            # Test if the client has the messages attribute (newer versions)
            if hasattr(anthropic_client, 'messages'):
                logger.info("Anthropic client initialized with messages API")
            else:
                # Test if the client has the completions attribute (older versions)
                if hasattr(anthropic_client, 'completions'):
                    logger.info("Anthropic client initialized with completions API")
                else:
                    logger.warning("Anthropic client initialized but API version unclear")
        except Exception as e:
            logger.error(f"Failed to initialize Anthropic client: {e}")

# We'll no longer use sample data since we're connecting to the real MongoDB database

def connect_to_mongodb():
    """Connect to MongoDB and return client and database"""
    if not MONGODB_URI:
        logger.error("MONGODB_URI environment variable not set")
        return None, None

    try:
        client = pymongo.MongoClient(MONGODB_URI)
        # Test the connection
        client.admin.command('ping')
        db = client[MONGODB_DB_NAME]

        # Log database info
        collections = db.list_collection_names()
        logger.info(f"Connected to MongoDB database: {MONGODB_DB_NAME}")
        logger.info(f"Available collections: {collections}")

        # Check if scraping_packages collection exists
        if 'scraping_packages' in collections:
            package_count = db.scraping_packages.count_documents({})
            logger.info(f"Found {package_count} scraping packages")
        else:
            logger.warning("No scraping_packages collection found")

        # Check if articles collection exists
        if 'articles' in collections:
            article_count = db.articles.count_documents({})
            logger.info(f"Found {article_count} articles")
        else:
            logger.warning("No articles collection found")

        return client, db
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        return None, None

def allowed_file(filename):
    """Check if file has an allowed extension"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_from_file(file):
    """Extract text from uploaded file"""
    filename = secure_filename(file.filename)
    file_ext = filename.rsplit('.', 1)[1].lower()

    if file_ext == 'txt' or file_ext == 'md':
        # Text files
        return file.read().decode('utf-8')
    elif file_ext == 'pdf':
        # PDF files
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    elif file_ext == 'docx':
        # DOCX files (requires python-docx package)
        try:
            import docx
            doc = docx.Document(io.BytesIO(file.read()))
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""

    return ""

def get_date_filter(date_range):
    """Get MongoDB date filter based on selected range"""
    now = datetime.datetime.now(datetime.timezone.utc)
    logger.info(f"Filtering by date range: {date_range}")

    if date_range == "7days":
        # Last 7 days
        start_date = now - datetime.timedelta(days=7)
        logger.info(f"Date filter: from {start_date} to {now}")
        return {"published_date": {"$gte": start_date}}
    elif date_range == "30days":
        # Last 30 days
        start_date = now - datetime.timedelta(days=30)
        logger.info(f"Date filter: from {start_date} to {now}")
        return {"published_date": {"$gte": start_date}}
    elif date_range == "90days":
        # Last 90 days
        start_date = now - datetime.timedelta(days=90)
        logger.info(f"Date filter: from {start_date} to {now}")
        return {"published_date": {"$gte": start_date}}
    else:
        # All time (no filter)
        logger.info("No date filter applied (all time)")
        return {}

def get_package_filter(package_ids):
    """Get MongoDB package filter based on selected packages"""
    if not package_ids:
        return {}

    # Convert string IDs to ObjectId
    object_ids = [ObjectId(pid) for pid in package_ids if ObjectId.is_valid(pid)]

    if not object_ids:
        return {}

    return {"scraping_package_id": {"$in": object_ids}}

def get_vector_search_filter(query, filters, db, limit=10):
    """
    Perform search with filters, with fallback options for non-Atlas MongoDB

    Args:
        query: Search query text
        filters: MongoDB filters to apply
        db: MongoDB database connection
        limit: Maximum number of results to return

    Returns:
        List of matching articles
    """
    try:
        # Log the filters being applied
        logger.info(f"Applying filters: {filters}")

        # Skip vector search for local MongoDB (requires Atlas)
        # Just use regular search methods

        # Try text search if available
        try:
            # Check if text index exists - use a more compatible approach
            try:
                indexes = list(db.articles.list_indexes())
                index_names = [idx.get("name") for idx in indexes]
                logger.info(f"Available indexes: {index_names}")
            except Exception as idx_error:
                logger.warning(f"Could not list indexes: {idx_error}")
                index_names = []

            has_text_index = any("text" in idx for idx in index_names)

            if has_text_index and query:
                logger.info("Using text search")
                # Create text search filter
                text_filter = {"$text": {"$search": query}}
                combined_filter = {**text_filter, **filters} if filters else text_filter

                # Perform the search
                results = list(db.articles.find(combined_filter).sort("published_date", -1).limit(limit))
                logger.info(f"Text search returned {len(results)} results")

                if results:
                    return results
        except Exception as text_error:
            logger.error(f"Error in text search: {text_error}")

        # If text search failed or returned no results, try regex search
        if query:
            logger.info("Using regex search")
            # Try searching in title and content fields
            relaxed_filter = {
                "$or": [
                    {"title": {"$regex": query, "$options": "i"}},
                    {"content": {"$regex": query, "$options": "i"}},
                    {"summary": {"$regex": query, "$options": "i"}}
                ]
            }
            if filters:
                # Combine with date and package filters
                combined_filter = {"$and": [relaxed_filter, filters]}
            else:
                combined_filter = relaxed_filter

            results = list(db.articles.find(combined_filter).sort("published_date", -1).limit(limit))
            logger.info(f"Regex search returned {len(results)} results")

            if results:
                return results

        # If all else fails or no query provided, just use the filters
        logger.info("Using filter-only search")
        results = list(db.articles.find(filters if filters else {}).sort("published_date", -1).limit(limit))
        logger.info(f"Filter-only search returned {len(results)} results")

        return results
    except Exception as e:
        logger.error(f"Error performing search: {e}")
        return []

def generate_with_openai(prompt, context, model="gpt-4o"):
    """Generate content using OpenAI"""
    if not openai_client:
        return "Error: OpenAI API key not configured"

    try:
        messages = [
            {"role": "system", "content": "You are a newsletter generation assistant that creates high-quality newsletters based on the provided context and instructions."},
            {"role": "user", "content": f"Context:\n{context}\n\nInstructions:\n{prompt}"}
        ]

        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.7,
            max_tokens=2000
        )

        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error generating with OpenAI: {e}")
        return f"Error generating content: {str(e)}"

def generate_with_anthropic(prompt, context, model="claude-3-opus-20240229"):
    """Generate content using Anthropic Claude"""
    if not anthropic_client:
        return "Error: Anthropic API key not configured"

    try:
        # For Claude 3 models, use direct API call to ensure we use the Messages API
        if "claude-3" in model:
            logger.info(f"Using direct API call for {model}")
            import requests

            # Get API key from environment
            api_key = os.environ.get('ANTHROPIC_API_KEY')
            if not api_key:
                return "Error: Anthropic API key not configured"

            # Set up headers and data
            headers = {
                "x-api-key": api_key,
                "content-type": "application/json",
                "anthropic-version": "2023-06-01"
            }

            data = {
                "model": model,
                "max_tokens": 2000,
                "temperature": 0.7,
                "system": "You are a newsletter generation assistant that creates high-quality newsletters based on the provided context and instructions.",
                "messages": [
                    {
                        "role": "user",
                        "content": f"Context:\n{context}\n\nInstructions:\n{prompt}"
                    }
                ]
            }

            # Make the API call
            logger.info("Making direct API call to Anthropic Messages API")
            response = requests.post("https://api.anthropic.com/v1/messages", headers=headers, json=data)

            # Process the response
            if response.status_code == 200:
                logger.info("Successfully received response from Anthropic API")
                return response.json()["content"][0]["text"]
            else:
                error_msg = f"API call failed: {response.status_code} - {response.text}"
                logger.error(error_msg)
                return f"Error generating content: {error_msg}"
        else:
            # Legacy API for older models
            logger.info(f"Using Anthropic legacy API with {model}")
            completion = anthropic_client.completions.create(
                model=model,
                max_tokens_to_sample=2000,
                temperature=0.7,
                prompt=f"{anthropic.HUMAN_PROMPT} I need you to generate a newsletter based on the following context and instructions.\n\nContext:\n{context}\n\nInstructions:\n{prompt}{anthropic.AI_PROMPT}",
            )
            return completion.completion
    except Exception as e:
        logger.error(f"Error generating with Anthropic: {e}")
        logger.error(f"Anthropic client type: {type(anthropic_client)}")
        logger.error(f"Anthropic version: {anthropic.__version__ if hasattr(anthropic, '__version__') else 'unknown'}")
        return f"Error generating content: {str(e)}"

@app.route('/')
def index():
    """Render the main page"""
    # Connect to MongoDB
    client, db = connect_to_mongodb()

    if client is None or db is None:
        flash("Failed to connect to MongoDB", "error")
        return render_template('index.html', packages=[])

    try:
        # Get all scraping packages
        try:
            # Get all packages without any status filter
            packages = list(db.scraping_packages.find({}))
            logger.info(f"Found {len(packages)} total scraping packages")

            # If no packages found, try to extract from articles
            if not packages:
                collections = db.list_collection_names()
                if 'articles' in collections:
                    logger.info("No packages found, trying to extract package info from articles collection")
                    pipeline = [
                        {"$group": {"_id": "$scraping_package_id", "name": {"$first": "$package_name"}}},
                        {"$project": {"_id": 1, "name": 1}}
                    ]
                    package_info = list(db.articles.aggregate(pipeline))
                    if package_info:
                        logger.info(f"Extracted {len(package_info)} packages from articles")
                        packages = package_info
        except Exception as pkg_error:
            logger.error(f"Error retrieving scraping packages: {pkg_error}")
            packages = []

        # Get available models
        openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"] if openai_client else []
        anthropic_models = ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"] if anthropic_client else []

        return render_template(
            'index.html',
            packages=packages,
            openai_models=openai_models,
            anthropic_models=anthropic_models
        )
    except Exception as e:
        logger.error(f"Error loading index page: {e}")
        flash(f"Error: {str(e)}", "error")
        return render_template('index.html', packages=[])
    finally:
        client.close()

@app.route('/generate', methods=['POST'])
def generate():
    """Generate newsletter content"""
    # Get form data
    model_provider = request.form.get('model_provider', 'openai')
    model_name = request.form.get('model_name', 'gpt-4o' if model_provider == 'openai' else 'claude-3-opus-20240229')
    date_range = request.form.get('date_range', 'all')
    package_ids = request.form.getlist('package_ids')
    search_query = request.form.get('search_query', '')
    client_context = request.form.get('client_context', '')
    project_context = request.form.get('project_context', '')
    prompt = request.form.get('prompt', '')

    # Connect to MongoDB
    client, db = connect_to_mongodb()

    if client is None or db is None:
        return jsonify({"error": "Failed to connect to MongoDB"}), 500

    try:
        # Build filters
        date_filter = get_date_filter(date_range)
        package_filter = get_package_filter(package_ids)

        # Combine filters
        combined_filter = {}
        if date_filter:
            combined_filter.update(date_filter)
        if package_filter:
            combined_filter.update(package_filter)

        # Get relevant articles
        articles = []
        if search_query:
            articles = get_vector_search_filter(search_query, combined_filter, db, limit=10)

        # Process uploaded files
        uploaded_context = ""
        if 'context_files' in request.files:
            files = request.files.getlist('context_files')
            for file in files:
                if file and file.filename and allowed_file(file.filename):
                    file_text = extract_text_from_file(file)
                    if file_text:
                        uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"

        # Build context
        context = ""

        # Add client and project context
        if client_context:
            context += f"CLIENT CONTEXT:\n{client_context}\n\n"
        if project_context:
            context += f"PROJECT CONTEXT:\n{project_context}\n\n"

        # Add articles
        if articles:
            context += "RELEVANT ARTICLES:\n"
            for i, article in enumerate(articles, 1):
                # Format date
                pub_date = article.get('published_date', 'Unknown date')
                if isinstance(pub_date, datetime.datetime):
                    pub_date = pub_date.strftime('%Y-%m-%d')

                # Get source URL
                source_url = article.get('source_url', '')

                # Get package name
                package_name = article.get('package_name', 'Unknown package')

                # Add article to context
                context += f"[{i}] {article.get('title', 'Untitled')}\n"
                context += f"Source: {article.get('source_name', 'Unknown source')}\n"
                if source_url:
                    context += f"URL: {source_url}\n"
                context += f"Date: {pub_date}\n"
                context += f"Package: {package_name}\n"
                context += f"Summary: {article.get('summary', 'No summary available')}\n"

                # Add a snippet of the content if available
                content = article.get('content', '')
                if content:
                    # Limit to first 300 characters
                    snippet = content[:300] + "..." if len(content) > 300 else content
                    context += f"Content snippet: {snippet}\n"

                context += "\n"

        # Add uploaded context
        if uploaded_context:
            context += "UPLOADED CONTEXT:\n" + uploaded_context

        # Generate content
        if model_provider == 'openai':
            generated_content = generate_with_openai(prompt, context, model_name)
        else:
            generated_content = generate_with_anthropic(prompt, context, model_name)

        return jsonify({
            "generated_content": generated_content,
            "context_used": context,
            "articles_count": len(articles)
        })

    except Exception as e:
        logger.error(f"Error generating content: {e}")
        return jsonify({"error": str(e)}), 500
    finally:
        client.close()

# Initialize API clients on startup
init_api_clients()

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
</file>

<file path="xx/rag_tester/README.md">
# RAG Newsletter Testing Environment

This is a web application for testing RAG-based newsletter generation with different models, filters, and context options.

## Features

- Switch between OpenAI and Anthropic Claude models
- Filter articles by date range (last 7 days, last 30 days, all time)
- Filter by scraping packages
- Add client and project context
- Upload additional context documents (TXT, PDF, MD, DOCX)
- Generate newsletters with RAG

## Setup

### Local Development

1. Clone the repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Create a `.env` file with the following variables:
   ```
   MONGODB_URI=your_mongodb_connection_string
   MONGODB_DB_NAME=proton
   OPENAI_API_KEY=your_openai_api_key
   ANTHROPIC_API_KEY=your_anthropic_api_key
   FLASK_SECRET_KEY=your_secret_key
   ```
4. Run the application:
   ```
   python app.py
   ```
5. Open your browser and navigate to `http://localhost:5000`

### Deployment on Render.com

1. Push the code to a Git repository
2. Create a new Web Service on Render.com
3. Connect your Git repository
4. Set the following:
   - Build Command: `pip install -r requirements.txt`
   - Start Command: `gunicorn app:app`
5. Add the environment variables:
   - `MONGODB_URI`: Your MongoDB connection string
   - `MONGODB_DB_NAME`: Your MongoDB database name (default: proton)
   - `OPENAI_API_KEY`: Your OpenAI API key
   - `ANTHROPIC_API_KEY`: Your Anthropic API key
   - `FLASK_SECRET_KEY`: A random string for Flask session security

## Usage

1. **Model Selection**: Choose between OpenAI and Anthropic Claude models
2. **Data Filtering**: 
   - Select a date range for articles
   - Choose which scraping packages to include
   - Enter a search query for RAG
3. **Additional Context**: Upload context files (TXT, PDF, MD, DOCX)
4. **Client & Project Context**: Add information about the client and project
5. **Newsletter Prompt**: Enter detailed instructions for the newsletter generation
6. Click "Generate Newsletter" to create the content

## Requirements

- Python 3.8+
- MongoDB with vector search capabilities
- OpenAI API key (for OpenAI models and embeddings)
- Anthropic API key (for Claude models)

## Notes

- The application uses OpenAI embeddings for vector search, even when generating with Claude
- Make sure your MongoDB instance has a vector index set up on the `vector_embedding` field
- For best results, provide detailed client and project context
</file>

<file path="xx/rag_tester/render.yaml">
services:
  # Web app for RAG testing
  - type: web
    name: proton-rag-tester
    runtime: python
    buildCommand: pip install -r requirements.txt
    startCommand: gunicorn app:app
    envVars:
      - key: MONGODB_URI
        sync: false
      - key: MONGODB_DB_NAME
        value: proton
      - key: OPENAI_API_KEY
        sync: false
      - key: ANTHROPIC_API_KEY
        sync: false
      - key: FLASK_SECRET_KEY
        generateValue: true
      - key: PYTHONUNBUFFERED
        value: true
</file>

<file path="xx/rag_tester/requirements.txt">
flask==2.3.3
werkzeug==2.3.7
pymongo==4.5.0
dnspython==2.4.2
python-dotenv==1.0.0
openai==1.3.5
anthropic==0.5.0
requests==2.31.0
PyPDF2==3.0.1
python-docx==0.8.11
gunicorn==21.2.0
markupsafe==2.1.3
</file>

<file path="xx/agent_newsletter_app.py">
"""
Agent-Based Newsletter Generator Web Interface

This Flask application provides a UI for testing agent-based newsletter generation
with different models, filters, and context options.
"""

import os
import datetime
import logging
from flask import Flask, render_template, request, jsonify, flash, Response, session
from werkzeug.utils import secure_filename
from dotenv import load_dotenv
import PyPDF2
import io

# Import our agent
from agent_newsletter_generator import NewsletterAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')

# Configure upload folder
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}

# Initialize global agent
agent = None

def init_agent():
    """Initialize the newsletter agent"""
    global agent
    try:
        agent = NewsletterAgent()
        logger.info("Newsletter agent initialized")
    except Exception as e:
        logger.error(f"Failed to initialize newsletter agent: {e}")
        agent = None

def allowed_file(filename):
    """Check if file has an allowed extension"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_from_file(file):
    """Extract text from uploaded file"""
    filename = secure_filename(file.filename)
    file_ext = filename.rsplit('.', 1)[1].lower()

    if file_ext == 'txt' or file_ext == 'md':
        # Text files
        return file.read().decode('utf-8')
    elif file_ext == 'pdf':
        # PDF files
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    elif file_ext == 'docx':
        # DOCX files (requires python-docx package)
        try:
            import docx
            doc = docx.Document(io.BytesIO(file.read()))
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""

    return ""

# Add context processor for current datetime
@app.context_processor
def inject_now():
    return {'now': datetime.datetime.now()}

@app.route('/')
def index():
    """Render the main page"""
    global agent

    if agent is None:
        init_agent()

    if agent is None:
        flash("Failed to initialize newsletter agent", "error")
        return render_template('index.html', packages=[])

    try:
        # Get all scraping packages
        packages = agent.get_scraping_packages()

        # Define available models
        openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"] if agent.openai_client else []

        return render_template(
            'index.html',
            packages=packages,
            openai_models=openai_models,
            anthropic_models=[]  # We're only using OpenAI for now
        )
    except Exception as e:
        logger.error(f"Error loading index page: {e}")
        flash(f"Error: {str(e)}", "error")
        return render_template('index.html', packages=[])

# Create a streaming route for the agent
@app.route('/generate-stream', methods=['GET', 'POST'])
def generate_stream():
    """Generate newsletter content using the agent with streaming"""
    global agent

    if agent is None:
        init_agent()

    if agent is None:
        return jsonify({"error": "Failed to initialize newsletter agent"}), 500

    # Check if this is a GET request (for SSE connection) or POST (for form submission)
    if request.method == 'GET':
        # This is the SSE connection
        # Create a generator function for streaming
        def generate_stream_content():
            # Queue for collecting chunks from the callback
            from queue import Queue
            chunk_queue = Queue()

            # Define the callback function for streaming
            def stream_callback(content):
                chunk_queue.put(content)

            try:
                # Initial message
                yield "data: Starting newsletter generation...\n\n"

                # Start the agent in a separate thread
                import threading

                # Get form data from the most recent POST request
                # This is a simplification - in a real app, you'd use session or a database
                # to store the form data between requests
                model_name = app.config.get('LAST_MODEL', 'gpt-4o')
                date_range = app.config.get('LAST_DATE_RANGE', 'all')
                package_ids = app.config.get('LAST_PACKAGE_IDS', [])
                search_query = app.config.get('LAST_SEARCH_QUERY', '')
                client_context = app.config.get('LAST_CLIENT_CONTEXT', '')
                project_context = app.config.get('LAST_PROJECT_CONTEXT', '')
                prompt = app.config.get('LAST_PROMPT', '')
                uploaded_context = app.config.get('LAST_UPLOADED_CONTEXT', '')

                def run_agent():
                    try:
                        agent.run_agent_with_query(
                            search_query=search_query,
                            date_range=date_range,
                            package_ids=package_ids,
                            client_context=client_context,
                            project_context=project_context,
                            prompt=prompt,
                            uploaded_context=uploaded_context,
                            model=model_name,
                            stream_callback=stream_callback
                        )
                        # Signal completion
                        chunk_queue.put(None)
                    except Exception as e:
                        logger.error(f"Thread error: {e}")
                        chunk_queue.put(f"Error: {str(e)}")
                        chunk_queue.put(None)

                # Start the agent thread
                agent_thread = threading.Thread(target=run_agent)
                agent_thread.daemon = True
                agent_thread.start()

                # Stream chunks as they become available
                while True:
                    try:
                        # Use a timeout to avoid blocking forever
                        chunk = chunk_queue.get(timeout=1.0)
                        if chunk is None:  # End signal
                            break
                        yield f"data: {chunk}\n\n"
                    except Exception as e:
                        # Check if the thread is still alive
                        if not agent_thread.is_alive():
                            logger.error(f"Agent thread died: {e}")
                            break
                        # Otherwise continue waiting
                        continue

                # Signal the end of the stream
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"Error in streaming: {e}")
                yield f"data: Error in streaming: {str(e)}\n\n"
                yield "data: [DONE]\n\n"

        return Response(generate_stream_content(), mimetype='text/event-stream')
    else:
        # This is the form submission
        # Get form data
        model_name = request.form.get('model_name', 'gpt-4o')
        date_range = request.form.get('date_range', 'all')
        package_ids = request.form.getlist('package_ids')
        search_query = request.form.get('search_query', '')
        client_context = request.form.get('client_context', '')
        project_context = request.form.get('project_context', '')
        prompt = request.form.get('prompt', '')

        # Process uploaded files
        uploaded_context = ""
        if 'context_files' in request.files:
            files = request.files.getlist('context_files')
            for file in files:
                if file and file.filename and allowed_file(file.filename):
                    file_text = extract_text_from_file(file)
                    if file_text:
                        uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"

        # Store the form data in the app config for the SSE connection to use
        app.config['LAST_MODEL'] = model_name
        app.config['LAST_DATE_RANGE'] = date_range
        app.config['LAST_PACKAGE_IDS'] = package_ids
        app.config['LAST_SEARCH_QUERY'] = search_query
        app.config['LAST_CLIENT_CONTEXT'] = client_context
        app.config['LAST_PROJECT_CONTEXT'] = project_context
        app.config['LAST_PROMPT'] = prompt
        app.config['LAST_UPLOADED_CONTEXT'] = uploaded_context

        # Return a success response
        return jsonify({"status": "success"})

@app.route('/generate', methods=['POST'])
def generate():
    """Generate newsletter content using the agent"""
    global agent

    if agent is None:
        init_agent()

    if agent is None:
        return jsonify({"error": "Failed to initialize newsletter agent"}), 500

    try:
        # Get form data
        model_name = request.form.get('model_name', 'gpt-4o')
        date_range = request.form.get('date_range', 'all')
        package_ids = request.form.getlist('package_ids')
        search_query = request.form.get('search_query', '')
        client_context = request.form.get('client_context', '')
        project_context = request.form.get('project_context', '')
        prompt = request.form.get('prompt', '')

        # Process uploaded files
        uploaded_context = ""
        if 'context_files' in request.files:
            files = request.files.getlist('context_files')
            for file in files:
                if file and file.filename and allowed_file(file.filename):
                    file_text = extract_text_from_file(file)
                    if file_text:
                        uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"

        # Run the agent
        result = agent.run_agent_with_query(
            search_query=search_query,
            date_range=date_range,
            package_ids=package_ids,
            client_context=client_context,
            project_context=project_context,
            prompt=prompt,
            uploaded_context=uploaded_context,
            model=model_name
        )

        return jsonify(result)

    except Exception as e:
        logger.error(f"Error generating content: {e}")
        return jsonify({"error": str(e)}), 500

# Create templates directory and files
def create_templates():
    """Create Flask templates for the application"""
    # Create templates directory if it doesn't exist
    if not os.path.exists('templates'):
        os.makedirs('templates')

    # Create base template
    with open('templates/base.html', 'w', encoding='utf-8') as f:
        f.write('''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{% block title %}Agent Newsletter Generator{% endblock %}</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .form-group { margin-bottom: 1rem; }
        #loading { display: none; }
        #result { display: none; }
        .card { margin-bottom: 20px; }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <h1>{% block header %}Agent Newsletter Generator{% endblock %}</h1>
        </header>

        {% with messages = get_flashed_messages() %}
          {% if messages %}
            {% for message in messages %}
              <div class="alert alert-info mb-3">{{ message }}</div>
            {% endfor %}
          {% endif %}
        {% endwith %}

        {% block content %}{% endblock %}

        <footer class="mt-5 py-3 text-center text-secondary">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    {% block scripts %}{% endblock %}
</body>
</html>''')

    # Create index template
    with open('templates/index.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}Agent Newsletter Generator{% endblock %}

{% block content %}
<div class="row">
    <div class="col-md-12">
        <div class="card">
            <div class="card-header">
                <h2>Generate Newsletter with AI Agent</h2>
            </div>
            <div class="card-body">
                <form id="newsletter-form" method="post" action="{{ url_for('generate') }}" enctype="multipart/form-data">
                    <div class="row">
                        <div class="col-md-6">
                            <div class="form-group">
                                <label for="model_name"><strong>Model:</strong></label>
                                <select class="form-control" id="model_name" name="model_name">
                                    {% for model in openai_models %}
                                        <option value="{{ model }}">OpenAI: {{ model }}</option>
                                    {% endfor %}
                                </select>
                            </div>

                            <div class="form-group">
                                <label for="date_range"><strong>Date Range:</strong></label>
                                <select class="form-control" id="date_range" name="date_range">
                                    <option value="all">All Time</option>
                                    <option value="7days">Last 7 Days</option>
                                    <option value="30days">Last 30 Days</option>
                                    <option value="90days">Last 90 Days</option>
                                </select>
                            </div>

                            <div class="form-group">
                                <label for="package_ids"><strong>Scraping Packages:</strong></label>
                                <select class="form-control" id="package_ids" name="package_ids" multiple size="5">
                                    {% for package in packages %}
                                        <option value="{{ package._id }}">{{ package.name|default('Unnamed Package') }}</option>
                                    {% endfor %}
                                </select>
                                <small class="form-text text-muted">Hold Ctrl/Cmd to select multiple packages. Leave empty to include all.</small>
                            </div>

                            <div class="form-group">
                                <label for="search_query"><strong>Search Query:</strong></label>
                                <input type="text" class="form-control" id="search_query" name="search_query" placeholder="Enter search terms...">
                                <small class="form-text text-muted">The agent will search for relevant articles using this query.</small>
                            </div>
                        </div>

                        <div class="col-md-6">
                            <div class="form-group">
                                <label for="client_context"><strong>Client Context:</strong></label>
                                <textarea class="form-control" id="client_context" name="client_context" rows="3" placeholder="Information about the client..."></textarea>
                            </div>

                            <div class="form-group">
                                <label for="project_context"><strong>Project Context:</strong></label>
                                <textarea class="form-control" id="project_context" name="project_context" rows="3" placeholder="Information about the project..."></textarea>
                            </div>

                            <div class="form-group">
                                <label for="context_files"><strong>Additional Context Files:</strong></label>
                                <input type="file" class="form-control" id="context_files" name="context_files" multiple>
                                <small class="form-text text-muted">Upload TXT, PDF, MD, or DOCX files with additional context.</small>
                            </div>
                        </div>
                    </div>

                    <div class="form-group mt-3">
                        <label for="prompt"><strong>Newsletter Instructions:</strong></label>
                        <textarea class="form-control" id="prompt" name="prompt" rows="5" placeholder="Detailed instructions for the newsletter..."></textarea>
                    </div>

                    <div class="form-group mt-3">
                        <button type="submit" class="btn btn-primary" id="generate-btn">Generate Newsletter</button>
                    </div>
                </form>

                <div id="loading" class="mt-4">
                    <div class="d-flex justify-content-center">
                        <div class="spinner-border text-primary" role="status">
                            <span class="visually-hidden">Loading...</span>
                        </div>
                    </div>
                    <p class="text-center mt-2">Generating newsletter... This may take a minute.</p>
                </div>

                <div id="result" class="mt-4">
                    <div class="card">
                        <div class="card-header d-flex justify-content-between align-items-center">
                            <h3>Generated Newsletter</h3>
                            <button class="btn btn-sm btn-outline-secondary" id="copy-btn">Copy to Clipboard</button>
                        </div>
                        <div class="card-body">
                            <div id="generated-content"></div>
                        </div>
                    </div>

                    <div class="card mt-3">
                        <div class="card-header">
                            <h3>Context Used <small class="text-muted">(<span id="articles-count">0</span> articles)</small></h3>
                        </div>
                        <div class="card-body">
                            <pre id="context-used" class="border p-3 bg-light" style="max-height: 300px; overflow-y: auto;"></pre>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
document.addEventListener('DOMContentLoaded', function() {
    const form = document.getElementById('newsletter-form');
    const generateBtn = document.getElementById('generate-btn');
    const loading = document.getElementById('loading');
    const result = document.getElementById('result');
    const generatedContent = document.getElementById('generated-content');
    const contextUsed = document.getElementById('context-used');
    const articlesCount = document.getElementById('articles-count');
    const copyBtn = document.getElementById('copy-btn');

    form.addEventListener('submit', function(e) {
        e.preventDefault();

        // Show loading indicator
        loading.style.display = 'block';
        result.style.display = 'none';
        generateBtn.disabled = true;

        // Submit form data via AJAX
        const formData = new FormData(form);

        fetch('/generate', {
            method: 'POST',
            body: formData
        })
        .then(response => response.json())
        .then(data => {
            // Hide loading indicator
            loading.style.display = 'none';
            generateBtn.disabled = false;

            if (data.error) {
                alert('Error: ' + data.error);
                return;
            }

            // Show result
            result.style.display = 'block';

            // Format and display generated content with Markdown
            generatedContent.innerHTML = formatContent(data.generated_content);

            // Display context used
            contextUsed.textContent = data.context_used;

            // Update articles count
            articlesCount.textContent = data.articles_count;

            // Scroll to result
            result.scrollIntoView({ behavior: 'smooth' });
        })
        .catch(error => {
            loading.style.display = 'none';
            generateBtn.disabled = false;
            alert('Error: ' + error);
        });
    });

    // Copy to clipboard functionality
    copyBtn.addEventListener('click', function() {
        const content = document.getElementById('generated-content').innerText;
        navigator.clipboard.writeText(content).then(() => {
            const originalText = copyBtn.textContent;
            copyBtn.textContent = 'Copied!';
            setTimeout(() => {
                copyBtn.textContent = originalText;
            }, 2000);
        });
    });

    // Simple Markdown-like formatting
    function formatContent(text) {
        if (!text) return '';

        // Replace newlines with <br>
        text = text.replace(/\\n/g, '<br>');

        // Format headers
        text = text.replace(/^# (.+)$/gm, '<h1>$1</h1>');
        text = text.replace(/^## (.+)$/gm, '<h2>$1</h2>');
        text = text.replace(/^### (.+)$/gm, '<h3>$1</h3>');

        // Format bold and italic
        text = text.replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>');
        text = text.replace(/\*(.+?)\*/g, '<em>$1</em>');

        // Format lists
        text = text.replace(/^- (.+)$/gm, '<li>$1</li>');
        text = text.replace(/(<li>.+<\/li>\\n)+/g, '<ul>$&</ul>');

        return text;
    }
});
</script>
{% endblock %}''')

    logger.info("Created templates for the agent newsletter app")

# Create templates on module import
create_templates()

# Initialize agent on startup
init_agent()

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5001))  # Changed to port 5001
    app.run(host='0.0.0.0', port=port, debug=True)
</file>

<file path="xx/AGENT_NEWSLETTER_DOCS.md">
# Agent Newsletter Generator Documentation

A sophisticated AI-powered newsletter generation system that uses LLM agents to create high-quality, context-aware newsletters based on articles stored in a MongoDB database.

## Overview

The Agent Newsletter Generator is a web application that allows users to generate professional newsletters using AI. The system:

1. Connects to a MongoDB database containing articles from various sources
2. Allows filtering by date range, scraping packages, and search queries
3. Supports both OpenAI and Anthropic models
4. Provides real-time streaming of agent thoughts during generation
5. Enables saving and importing newsletter configurations and outputs

## System Architecture

The system consists of the following components:

### Backend Components

- **final_app.py**: The main Flask application that handles HTTP requests, serves the UI, and manages the generation process
- **agent_newsletter_generator.py**: The core agent implementation that connects to MongoDB, retrieves articles, and generates newsletters using LLMs

### Frontend Components

- **templates/final_index.html**: The main UI template with form inputs and result display
- **static/**: Contains CSS and JavaScript files (if any)

### Data Flow

1. User submits form with newsletter parameters
2. Backend retrieves relevant articles from MongoDB based on filters
3. Agent processes articles and generates newsletter content
4. Results are streamed back to the UI in real-time
5. Final newsletter content is displayed and can be downloaded as JSON

## Installation

### Prerequisites

- Python 3.8+
- MongoDB (local or Atlas)
- OpenAI API key (for GPT models)
- Anthropic API key (for Claude models)

### Setup

1. Clone the repository:
   ```
   git clone <repository-url>
   cd <repository-directory>
   ```

2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

3. Create a `.env` file with the following variables:
   ```
   # MongoDB Connection
   MONGODB_URI=mongodb+srv://<username>:<password>@<cluster-url>/<database>?retryWrites=true&w=majority
   MONGODB_DB_NAME=proton
   
   # Local development flag (set to True to use DNS resolver for MongoDB Atlas connection)
   LOCAL=True
   
   # API Keys
   OPENAI_API_KEY=your_openai_api_key
   ANTHROPIC_API_KEY=your_anthropic_api_key
   
   # Flask Configuration
   FLASK_SECRET_KEY=your_secret_key
   PORT=5006
   ```

4. Run the application:
   ```
   python final_app.py
   ```

5. Access the application at http://localhost:5006

## Usage Guide

### Generating a Newsletter

1. **Select Model**: Choose from available OpenAI models (GPT-4o, GPT-4-turbo, GPT-3.5-turbo)
2. **Date Range**: Filter articles by time period (All Time, Last 7 Days, Last 30 Days, Last 90 Days)
3. **Scraping Packages**: Select specific content sources (hold Ctrl/Cmd to select multiple)
4. **Search Query**: Enter keywords to find relevant articles
5. **Client Context**: Provide information about the client (who the newsletter is for)
6. **Project Context**: Describe the newsletter's purpose and audience
7. **Additional Context Files**: Upload TXT, PDF, MD, or DOCX files with additional information
8. **Newsletter Instructions**: Provide detailed instructions for the newsletter format and content
9. Click "Generate Newsletter" to start the process

### Importing/Exporting Newsletters

1. **Export**: After generating a newsletter, click "Download JSON" to save the configuration and results
2. **Import**: Click "Import from JSON" to upload a previously saved newsletter JSON file

### JSON Format

The JSON format for import/export includes:

```json
{
  "inputs": {
    "model_name": "gpt-4o",
    "date_range": "30days",
    "package_ids": ["id1", "id2"],
    "search_query": "search terms",
    "client_context": "client information",
    "project_context": "project information",
    "prompt": "detailed instructions"
  },
  "outputs": {
    "content": "generated newsletter content",
    "context_used": "context information used",
    "articles_count": 5
  }
}
```

## MongoDB Schema

### Articles Collection

```
{
  "_id": ObjectId,
  "title": String,
  "content": String,
  "summary": String,
  "source_name": String,
  "source_url": String,
  "published_date": Date,
  "author": String,
  "package_name": String,
  "scraping_package_id": ObjectId,
  "vector_embedding": Array (optional)
}
```

### Scraping Packages Collection

```
{
  "_id": ObjectId,
  "name": String,
  "description": String,
  "status": String
}
```

## Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| MONGODB_URI | MongoDB connection string | Yes |
| MONGODB_DB_NAME | MongoDB database name | Yes |
| LOCAL | Use DNS resolver for MongoDB Atlas (True/False) | No |
| OPENAI_API_KEY | OpenAI API key | Yes |
| ANTHROPIC_API_KEY | Anthropic API key | No |
| FLASK_SECRET_KEY | Secret key for Flask sessions | Yes |
| PORT | Port to run the application on | No |

## Troubleshooting

### MongoDB Connection Issues

If you encounter DNS resolution issues with MongoDB Atlas, set `LOCAL=True` in your `.env` file. This enables a custom DNS resolver that uses Google's DNS servers (8.8.8.8 and 1.1.1.1).

### API Key Issues

If you see errors related to API keys:
1. Verify your API keys are correct and have sufficient credits
2. Check that the keys are properly set in the `.env` file
3. Restart the application after updating the keys

### Model Selection

If certain models are not appearing in the dropdown:
1. Ensure you have the correct API keys for that provider
2. Check that your API key has access to the models you want to use
3. Verify the model names in the code match the current model names from the provider

## Development Guide

### Adding New Models

To add support for new models:

1. Update the model list in `final_app.py`:
   ```python
   openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo", "new-model-name"]
   ```

2. Ensure the model is properly handled in the agent implementation.

### Extending the Agent

The `NewsletterAgent` class in `agent_newsletter_generator.py` can be extended with new capabilities:

1. Add new methods for additional functionality
2. Update the `run_agent_with_query` method to use the new capabilities
3. Modify the UI to expose the new features

### Customizing the UI

The UI template is in `templates/final_index.html`. To customize:

1. Modify the HTML structure for layout changes
2. Update the CSS styles for visual changes
3. Extend the JavaScript for new interactive features

## Agent Implementation Details

### NewsletterAgent Class

The `NewsletterAgent` class in `agent_newsletter_generator.py` is the core of the system. It:

1. Connects to MongoDB to retrieve articles
2. Filters articles based on user criteria
3. Formats the context for the LLM
4. Calls the appropriate LLM API (OpenAI or Anthropic)
5. Processes and returns the generated newsletter

Key methods:

- `__init__()`: Initializes the agent with API clients and database connection
- `_connect_to_mongodb()`: Establishes connection to MongoDB with DNS resolver support
- `get_scraping_packages()`: Retrieves available scraping packages
- `get_vector_search_filter()`: Performs vector search with filters
- `get_date_filter()`: Creates MongoDB date filters based on selected range
- `run_agent_with_query()`: Main method that orchestrates the newsletter generation process
- `generate_with_openai()`: Generates content using OpenAI models
- `generate_with_anthropic()`: Generates content using Anthropic models

### Context Building

The agent builds context for the LLM in this format:

```
CLIENT CONTEXT:
[Client information provided by the user]

PROJECT CONTEXT:
[Project information provided by the user]

RELEVANT ARTICLES:
[1] Article Title
Source: Source Name
URL: Source URL
Date: Publication Date
Package: Package Name
Summary: Article Summary
Content snippet: First 300 characters of content...

[2] Article Title
...
```

### Streaming Implementation

The system uses Server-Sent Events (SSE) to stream the agent's thoughts and the generated newsletter in real-time:

1. The client makes a request to `/generate`
2. The server starts a background thread to run the agent
3. The agent uses a callback function to send updates to a queue
4. The `/stream-thoughts` endpoint reads from this queue and sends events to the client
5. The client displays these updates in real-time

## Deployment

### Render.com Deployment

1. Create a new Web Service in Render
2. Connect your GitHub repository
3. Set the build command: `pip install -r requirements.txt`
4. Set the start command: `gunicorn final_app:app`
5. Add all environment variables from your `.env` file
6. Deploy the service

### Docker Deployment

A Dockerfile is provided for containerized deployment:

1. Build the Docker image:
   ```
   docker build -t newsletter-agent .
   ```

2. Run the container:
   ```
   docker run -p 5006:5006 --env-file .env newsletter-agent
   ```

## Recent Updates

### JSON Import/Export Feature

The system now supports importing and exporting newsletter configurations and results as JSON files:

1. **Export**: After generating a newsletter, users can download a JSON file containing:
   - All input parameters (model, date range, search query, etc.)
   - The generated newsletter content
   - Context information used
   - Number of articles used

2. **Import**: Users can upload a previously saved JSON file to:
   - Populate all form fields with the saved parameters
   - Display the previously generated newsletter
   - View the context information used

This feature enables:
- Saving work for later editing
- Sharing newsletters with team members
- Creating templates for different types of newsletters
- Archiving generated newsletters with their parameters

### DNS Resolution Fix

A fix has been implemented for DNS resolution issues when connecting to MongoDB Atlas:

1. Added a `LOCAL` environment variable that, when set to `True`, enables a custom DNS resolver
2. The resolver uses Google's DNS servers (8.8.8.8 and 1.1.1.1) for reliable resolution
3. This fix is specific to machines that have DNS resolution issues with MongoDB Atlas SRV records

## Future Enhancements

Planned enhancements for future versions:

1. **Multi-user Support**: Add user accounts and authentication
2. **Template System**: Save and reuse newsletter templates
3. **Scheduled Generation**: Automatically generate newsletters on a schedule
4. **Email Integration**: Send generated newsletters directly via email
5. **Analytics**: Track newsletter performance metrics
6. **Custom Styling**: Add options for custom CSS and branding
7. **More LLM Providers**: Add support for additional LLM providers
8. **Collaborative Editing**: Allow multiple users to collaborate on newsletters
9. **Version History**: Track changes to newsletters over time
10. **API Access**: Provide an API for programmatic access to the system
</file>

<file path="xx/agent_newsletter_generator.py">
"""
Agent-Based Newsletter Generator

This module provides an agent-based approach to newsletter generation,
replacing the RAG-based approach while maintaining the same interface.
The agent can query the MongoDB vector database, process context,
and generate newsletters based on user instructions.
"""

import os
import datetime
import logging
import json
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv
import pymongo
from bson.objectid import ObjectId
import openai
from openai import OpenAI
import dns.resolver

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

class NewsletterAgent:
    """
    Agent for generating newsletters based on MongoDB vector database
    and user instructions.
    """

    def __init__(self):
        """Initialize the newsletter agent with API clients and database connection"""
        # Initialize OpenAI client
        self.openai_api_key = os.environ.get('OPENAI_API_KEY')
        self.openai_client = None
        if self.openai_api_key:
            try:
                self.openai_client = OpenAI(api_key=self.openai_api_key)
                logger.info("OpenAI client initialized")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")

        # MongoDB connection
        self.mongodb_uri = os.environ.get('MONGODB_URI')
        self.mongodb_db_name = os.environ.get('MONGODB_DB_NAME', 'proton')
        self.mongo_client = None
        self.db = None

        # Connect to MongoDB
        self._connect_to_mongodb()

    def _connect_to_mongodb(self):
        """Connect to MongoDB database"""
        if not self.mongodb_uri:
            logger.error("MONGODB_URI environment variable not set")
            return

        try:
            # Check if we're in local development mode
            local_mode = os.environ.get('LOCAL', 'False').lower() in ('true', '1', 't')

            if local_mode and 'mongodb+srv' in self.mongodb_uri:
                logger.info("Using local DNS resolver for MongoDB Atlas connection")
                # Configure DNS resolver to use Google's DNS servers
                dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
                dns.resolver.default_resolver.nameservers = ['8.8.8.8', '1.1.1.1']

                # Set pymongo to use our DNS resolver
                pymongo.uri_parser.SRV_SCHEME_TXT = 'mongodb+srv'

            # Connect to MongoDB
            self.mongo_client = pymongo.MongoClient(self.mongodb_uri)
            # Test the connection
            self.mongo_client.admin.command('ping')
            self.db = self.mongo_client[self.mongodb_db_name]

            # Log database info
            collections = self.db.list_collection_names()
            logger.info(f"Connected to MongoDB database: {self.mongodb_db_name}")
            logger.info(f"Available collections: {collections}")

            # Check if scraping_packages collection exists
            if 'scraping_packages' in collections:
                package_count = self.db.scraping_packages.count_documents({})
                logger.info(f"Found {package_count} scraping packages")
            else:
                logger.warning("No scraping_packages collection found")

            # Check if articles collection exists
            if 'articles' in collections:
                article_count = self.db.articles.count_documents({})
                logger.info(f"Found {article_count} articles")
            else:
                logger.warning("No articles collection found")

        except Exception as e:
            logger.error(f"Failed to connect to MongoDB: {e}")
            self.mongo_client = None
            self.db = None

    def close(self):
        """Close MongoDB connection"""
        if self.mongo_client:
            self.mongo_client.close()
            self.mongo_client = None
            self.db = None

    def get_scraping_packages(self) -> List[Dict[str, Any]]:
        """Get all scraping packages from the database"""
        if self.db is None:
            logger.error("No database connection")
            return []

        try:
            # Get all packages without any status filter
            packages = list(self.db.scraping_packages.find({}))
            logger.info(f"Found {len(packages)} total scraping packages")

            # If no packages found, try to extract from articles
            if not packages:
                collections = self.db.list_collection_names()
                if 'articles' in collections:
                    logger.info("No packages found, trying to extract package info from articles collection")
                    pipeline = [
                        {"$group": {"_id": "$scraping_package_id", "name": {"$first": "$package_name"}}},
                        {"$project": {"_id": 1, "name": 1}}
                    ]
                    package_info = list(self.db.articles.aggregate(pipeline))
                    if package_info:
                        logger.info(f"Extracted {len(package_info)} packages from articles")
                        packages = package_info

            return packages
        except Exception as e:
            logger.error(f"Error retrieving scraping packages: {e}")
            return []

    def get_date_filter(self, date_range: str) -> Dict[str, Any]:
        """Get MongoDB date filter based on selected range"""
        now = datetime.datetime.now(datetime.timezone.utc)
        logger.info(f"Filtering by date range: {date_range}")

        if date_range == "7days":
            # Last 7 days
            start_date = now - datetime.timedelta(days=7)
            logger.info(f"Date filter: from {start_date} to {now}")
            return {"published_date": {"$gte": start_date}}
        elif date_range == "30days":
            # Last 30 days
            start_date = now - datetime.timedelta(days=30)
            logger.info(f"Date filter: from {start_date} to {now}")
            return {"published_date": {"$gte": start_date}}
        elif date_range == "90days":
            # Last 90 days
            start_date = now - datetime.timedelta(days=90)
            logger.info(f"Date filter: from {start_date} to {now}")
            return {"published_date": {"$gte": start_date}}
        else:
            # All time (no filter)
            logger.info("No date filter applied (all time)")
            return {}

    def get_package_filter(self, package_ids: List[str]) -> Dict[str, Any]:
        """Get MongoDB package filter based on selected packages"""
        if not package_ids:
            return {}

        # Convert string IDs to ObjectId
        object_ids = [ObjectId(pid) for pid in package_ids if ObjectId.is_valid(pid)]

        if not object_ids:
            return {}

        return {"scraping_package_id": {"$in": object_ids}}

    def search_articles(self, query: str, filters: Dict[str, Any], limit: int = 10) -> List[Dict[str, Any]]:
        """
        Search for articles using text search, regex, or filters

        Args:
            query: Search query text
            filters: MongoDB filters to apply
            limit: Maximum number of results to return

        Returns:
            List of matching articles
        """
        if self.db is None:
            logger.error("No database connection")
            return []

        try:
            # Log the filters being applied
            logger.info(f"Applying filters: {filters}")

            # Try text search if available
            try:
                # Check if text index exists
                try:
                    indexes = list(self.db.articles.list_indexes())
                    index_names = [idx.get("name") for idx in indexes]
                    logger.info(f"Available indexes: {index_names}")
                except Exception as idx_error:
                    logger.warning(f"Could not list indexes: {idx_error}")
                    index_names = []

                has_text_index = any("text" in idx for idx in index_names)

                if has_text_index and query:
                    logger.info("Using text search")
                    # Create text search filter
                    text_filter = {"$text": {"$search": query}}
                    combined_filter = {**text_filter, **filters} if filters else text_filter

                    # Perform the search
                    results = list(self.db.articles.find(combined_filter).sort("published_date", -1).limit(limit))
                    logger.info(f"Text search returned {len(results)} results")

                    if results:
                        return results
            except Exception as text_error:
                logger.error(f"Error in text search: {text_error}")

            # If text search failed or returned no results, try regex search
            if query:
                logger.info("Using regex search")
                # Try searching in title and content fields
                relaxed_filter = {
                    "$or": [
                        {"title": {"$regex": query, "$options": "i"}},
                        {"content": {"$regex": query, "$options": "i"}},
                        {"summary": {"$regex": query, "$options": "i"}}
                    ]
                }
                if filters:
                    # Combine with date and package filters
                    combined_filter = {"$and": [relaxed_filter, filters]}
                else:
                    combined_filter = relaxed_filter

                results = list(self.db.articles.find(combined_filter).sort("published_date", -1).limit(limit))
                logger.info(f"Regex search returned {len(results)} results")

                if results:
                    return results

            # If all else fails or no query provided, just use the filters
            logger.info("Using filter-only search")
            results = list(self.db.articles.find(filters if filters else {}).sort("published_date", -1).limit(limit))
            logger.info(f"Filter-only search returned {len(results)} results")

            return results
        except Exception as e:
            logger.error(f"Error performing search: {e}")
            return []

    def build_context(self,
                     articles: List[Dict[str, Any]],
                     client_context: str = "",
                     project_context: str = "",
                     uploaded_context: str = "") -> str:
        """
        Build context for the agent from articles and additional context

        Args:
            articles: List of articles to include in context
            client_context: Context about the client
            project_context: Context about the project
            uploaded_context: Context from uploaded files

        Returns:
            Formatted context string
        """
        context = ""

        # Add client and project context
        if client_context:
            context += f"CLIENT CONTEXT:\n{client_context}\n\n"
        if project_context:
            context += f"PROJECT CONTEXT:\n{project_context}\n\n"

        # Add articles
        if articles:
            context += "RELEVANT ARTICLES:\n"
            for i, article in enumerate(articles, 1):
                # Format date
                pub_date = article.get('published_date', 'Unknown date')
                if isinstance(pub_date, datetime.datetime):
                    pub_date = pub_date.strftime('%Y-%m-%d')

                # Get source URL
                source_url = article.get('source_url', '')

                # Get package name
                package_name = article.get('package_name', 'Unknown package')

                # Add article to context
                context += f"[{i}] {article.get('title', 'Untitled')}\n"
                context += f"Source: {article.get('source_name', 'Unknown source')}\n"
                if source_url:
                    context += f"URL: {source_url}\n"
                context += f"Date: {pub_date}\n"
                context += f"Package: {package_name}\n"
                context += f"Summary: {article.get('summary', 'No summary available')}\n"

                # Add a snippet of the content if available
                content = article.get('content', '')
                if content:
                    # Limit to first 300 characters
                    snippet = content[:300] + "..." if len(content) > 300 else content
                    context += f"Content snippet: {snippet}\n"

                context += "\n"

        # Add uploaded context
        if uploaded_context:
            context += "UPLOADED CONTEXT:\n" + uploaded_context

        return context

    def generate_newsletter(self,
                           prompt: str,
                           context: str,
                           model: str = "gpt-4o",
                           scraping_packages: Optional[List[str]] = None,
                           stream_callback = None) -> str:
        """Generate newsletter using the agent with optional streaming"""
        # Print a message to the console for debugging
        print(f"Generating newsletter with model {model}, streaming: {stream_callback is not None}")
        """
        Generate newsletter using the agent

        Args:
            prompt: User instructions for newsletter generation
            context: Context information including articles and additional context
            model: Model to use for generation
            scraping_packages: List of scraping package names (for agent context)
            stream_callback: Optional callback function for streaming responses

        Returns:
            Generated newsletter content
        """
        if self.openai_client is None:
            return "Error: OpenAI API key not configured"

        try:
            # Create system message with agent instructions
            system_message = """You are an advanced newsletter generation agent that creates high-quality newsletters based on provided context and instructions.

Your capabilities:
1. Analyze and extract relevant information from articles
2. Understand client and project context
3. Follow specific formatting and style instructions
4. Create engaging, well-structured newsletters

When generating a newsletter:
- Focus on the most relevant and recent information
- Maintain a consistent tone and style
- Include proper attribution for sources
- Structure the content logically with clear sections
- Adapt to the specific audience mentioned in the context
"""

            # Add information about scraping packages if provided
            if scraping_packages:
                system_message += "\n\nAvailable scraping packages:\n"
                for package in scraping_packages:
                    system_message += f"- {package}\n"

            # Create messages for the API call
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": f"Context:\n{context}\n\nInstructions:\n{prompt}"}
            ]

            # Log the agent's thought process
            logger.info("Agent is analyzing context and generating newsletter...")

            # If streaming is requested
            if stream_callback is not None:
                # Stream the agent's thought process
                stream_callback("Agent is analyzing context and preparing to generate newsletter...\n")
                stream_callback("Extracting relevant information from articles...\n")

                # Call the OpenAI API with streaming
                full_response = ""
                response = self.openai_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=4000,
                    stream=True
                )

                # Process the streaming response
                for chunk in response:
                    if chunk.choices and len(chunk.choices) > 0:
                        content = chunk.choices[0].delta.content
                        if content is not None:
                            # Check if content contains newlines and handle them properly
                            if '\n' in content:
                                # Replace single newlines with double newlines for better readability
                                formatted_content = content.replace('\n', '\n\n')
                                full_response += content  # Keep original content for the full response
                                stream_callback(formatted_content)
                            else:
                                full_response += content
                                stream_callback(content)

                return full_response
            else:
                # Call the OpenAI API without streaming
                response = self.openai_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=4000
                )

                return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error generating newsletter: {e}")
            error_message = f"Error generating newsletter: {str(e)}"
            if stream_callback is not None:
                stream_callback(error_message)
            return error_message

    def run_agent_with_query(self,
                            search_query: str,
                            date_range: str,
                            package_ids: List[str],
                            client_context: str,
                            project_context: str,
                            prompt: str,
                            uploaded_context: str = "",
                            model: str = "gpt-4o",
                            stream_callback = None) -> Dict[str, Any]:
        """
        Run the newsletter agent with a search query and filters

        Args:
            search_query: Query for searching articles
            date_range: Date range filter (7days, 30days, 90days, all)
            package_ids: List of package IDs to filter by
            client_context: Context about the client
            project_context: Context about the project
            prompt: User instructions for newsletter generation
            uploaded_context: Context from uploaded files
            model: Model to use for generation
            stream_callback: Optional callback function for streaming responses

        Returns:
            Dictionary with generated content, context used, and article count
        """
        try:
            # Stream the agent's thought process if streaming is enabled
            if stream_callback is not None:
                stream_callback("Agent starting newsletter generation process...\n\n")
                # Add a small delay to ensure messages are processed separately
                import time
                time.sleep(0.1)
                stream_callback("Building filters based on date range and packages...\n\n")

            # Build filters
            date_filter = self.get_date_filter(date_range)
            package_filter = self.get_package_filter(package_ids)

            # Combine filters
            combined_filter = {}
            if date_filter:
                combined_filter.update(date_filter)
            if package_filter:
                combined_filter.update(package_filter)

            # Get relevant articles
            articles = []
            if search_query:
                if stream_callback is not None:
                    stream_callback(f"Searching for articles related to: '{search_query}'...\n\n")
                    import time
                    time.sleep(0.1)
                articles = self.search_articles(search_query, combined_filter, limit=10)
                if stream_callback is not None:
                    stream_callback(f"Found {len(articles)} relevant articles.\n\n")
                    time.sleep(0.1)

            # Build context
            if stream_callback is not None:
                stream_callback("Building context from articles and user inputs...\n\n")
                import time
                time.sleep(0.1)
            context = self.build_context(
                articles=articles,
                client_context=client_context,
                project_context=project_context,
                uploaded_context=uploaded_context
            )

            # Get package names for agent context
            package_names = []
            try:
                if stream_callback is not None:
                    stream_callback("Retrieving scraping package information...\n\n")
                    time.sleep(0.1)
                packages = self.get_scraping_packages()
                package_names = [p.get('name', str(p.get('_id'))) for p in packages]
            except Exception as e:
                logger.error(f"Error getting package names: {e}")
                if stream_callback is not None:
                    stream_callback(f"Warning: Error retrieving package names: {e}\n\n")
                    time.sleep(0.1)

            # Generate newsletter
            if stream_callback is not None:
                stream_callback("\nStarting newsletter generation with OpenAI model...\n\n")
                time.sleep(0.1)
            generated_content = self.generate_newsletter(
                prompt=prompt,
                context=context,
                model=model,
                scraping_packages=package_names,
                stream_callback=stream_callback
            )

            # Completion message
            if stream_callback is not None:
                import time
                time.sleep(0.1)
                stream_callback("\n\nNewsletter generation complete!\n\n")
                time.sleep(0.1)

            return {
                "generated_content": generated_content,
                "context_used": context,
                "articles_count": len(articles)
            }

        except Exception as e:
            logger.error(f"Error running agent: {e}")
            error_message = f"Error generating newsletter: {str(e)}"
            if stream_callback is not None:
                stream_callback(f"\n{error_message}\n")
            return {
                "error": str(e),
                "generated_content": error_message,
                "context_used": "",
                "articles_count": 0
            }

# Example usage
if __name__ == "__main__":
    # Create agent
    agent = NewsletterAgent()

    # Example parameters
    search_query = "artificial intelligence"
    date_range = "30days"
    package_ids = []  # Empty list means all packages
    client_context = "Tech startup focused on AI solutions for healthcare"
    project_context = "Monthly newsletter highlighting AI advancements in healthcare"
    prompt = "Create a newsletter with 3-4 sections covering recent AI developments in healthcare. Include a brief introduction and conclusion."

    try:
        # Run agent
        result = agent.run_agent_with_query(
            search_query=search_query,
            date_range=date_range,
            package_ids=package_ids,
            client_context=client_context,
            project_context=project_context,
            prompt=prompt
        )

        # Print result
        print("Generated Newsletter:")
        print(result["generated_content"])
        print(f"\nArticles used: {result['articles_count']}")

    finally:
        # Close connections
        agent.close()
</file>

<file path="xx/agent_newsletter_README.md">
# Agent-Based Newsletter Generator

This application provides an agent-based approach to newsletter generation, replacing the RAG-based approach while maintaining the same interface. The agent can query the MongoDB vector database, process context, and generate newsletters based on user instructions.

## Features

- Uses an AI agent to generate newsletters instead of a simple RAG approach
- Connects to the same MongoDB database as the RAG system
- Filters articles by date range and scraping packages
- Supports search queries to find relevant articles
- Allows adding client and project context
- Supports uploading additional context files (TXT, PDF, MD, DOCX)
- Uses OpenAI models for generation

## Setup

1. Make sure you have all the required environment variables in your `.env` file:
   ```
   MONGODB_URI=your_mongodb_connection_string
   MONGODB_DB_NAME=proton
   OPENAI_API_KEY=your_openai_api_key
   FLASK_SECRET_KEY=your_secret_key
   ```

2. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

3. Run the application:
   ```
   python agent_newsletter_app.py
   ```

4. Open your browser and navigate to `http://localhost:5000`

## How It Works

1. **Agent Initialization**: The application initializes an AI agent that connects to your MongoDB database and OpenAI API.

2. **Article Search**: When you submit a search query, the agent searches for relevant articles in the database using text search, regex search, or filters.

3. **Context Building**: The agent builds context from the retrieved articles, client information, project details, and any uploaded files.

4. **Newsletter Generation**: The agent uses the OpenAI API to generate a newsletter based on the context and your instructions.

## Usage

1. **Model Selection**: Choose an OpenAI model from the dropdown menu.

2. **Data Filtering**:
   - Select a date range for articles (All Time, Last 7 Days, Last 30 Days, Last 90 Days)
   - Choose which scraping packages to include
   - Enter a search query to find relevant articles

3. **Context Information**:
   - Enter information about the client
   - Enter information about the project
   - Upload additional context files (TXT, PDF, MD, DOCX)

4. **Newsletter Instructions**: Enter detailed instructions for the newsletter generation.

5. Click "Generate Newsletter" to create the content.

## Differences from RAG Approach

The agent-based approach offers several advantages over the simple RAG approach:

1. **More Intelligent Processing**: The agent can better understand the relationships between articles and context.

2. **Better Context Integration**: The agent can more effectively integrate client and project context with the article information.

3. **Improved Structure**: The agent can create more coherent and well-structured newsletters.

4. **Adaptability**: The agent can adapt to different types of newsletter requirements and styles.

## Customization

You can customize the agent's behavior by modifying the `NewsletterAgent` class in `agent_newsletter_generator.py`. For example, you can:

- Change the system message to give the agent different instructions
- Adjust the search parameters for finding relevant articles
- Modify how context is built and formatted
- Change the temperature or other generation parameters

## Troubleshooting

- **MongoDB Connection Issues**: Make sure your MongoDB URI is correct and that your database is accessible.
- **OpenAI API Issues**: Verify that your OpenAI API key is valid and has sufficient quota.
- **Missing Articles**: Check that your database contains articles and that the scraping packages are correctly configured.
- **File Upload Issues**: Ensure that the uploaded files are in the supported formats and are not too large.
</file>

<file path="xx/agent_requirements.txt">
flask==3.0.0
pymongo==4.5.0
dnspython==2.4.2
python-dotenv>=0.21.1
openai>=1.3.0
PyPDF2==3.0.1
python-docx==1.0.1
gunicorn==21.2.0
markupsafe==2.1.3
</file>

<file path="xx/article_viewer.py">
"""
Article Viewer for Proton CRM

A simple Flask web app to browse articles in the database.
"""

import os
import sys
import logging
import datetime
from typing import List, Dict, Any
from flask import Flask, render_template, request, jsonify, redirect, url_for, flash
from markupsafe import Markup
from bson.objectid import ObjectId
import math

# Import MongoDB connection
from proton_db_setup import ProtonDB

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__, template_folder='templates')
app.secret_key = os.urandom(24)  # For flash messages

# Initialize global database connection
db = None

# Add context processor for current datetime
@app.context_processor
def inject_now():
    return {'now': datetime.datetime.now()}

def get_db():
    """Get database connection, creating it if it doesn't exist"""
    global db
    if db is None:
        db = ProtonDB()
    return db

def close_db(exception=None):
    """Close database connection"""
    global db
    if db is not None:
        db.close()
        db = None

@app.route('/')
def index():
    """Home page, redirect to articles list"""
    return redirect(url_for('list_articles'))

@app.route('/articles')
def list_articles():
    """List all articles with pagination"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 10, type=int)
    package_id = request.args.get('package')
    search_query = request.args.get('q')
    sort_by = request.args.get('sort', 'date')
    sort_order = request.args.get('order', 'desc')
    
    # Get database connection
    db_conn = get_db()
    
    # Build query
    query = {}
    if package_id:
        query['scraping_package_id'] = ObjectId(package_id)
    
    if search_query:
        query['$text'] = {'$search': search_query}
    
    # Get packages for filter dropdown
    packages = list(db_conn.db.scraping_packages.find({}, {'name': 1}))
    
    # Build sort query
    sort_query = []
    if sort_by == 'date':
        sort_query.append(('published_date', -1 if sort_order == 'desc' else 1))
    elif sort_by == 'relevance':
        sort_query.append(('relevance_scores.overall', -1 if sort_order == 'desc' else 1))
    elif sort_by == 'title':
        sort_query.append(('title', 1 if sort_order == 'asc' else -1))
    
    # Add secondary sort by date
    if sort_by != 'date':
        sort_query.append(('published_date', -1))
    
    # Count total articles
    total_articles = db_conn.db.articles.count_documents(query)
    total_pages = math.ceil(total_articles / per_page)
    
    # Adjust page number to be within bounds
    page = max(1, min(page, total_pages)) if total_pages > 0 else 1
    
    # Get articles
    skip = (page - 1) * per_page
    articles = list(db_conn.db.articles.find(query).sort(sort_query).skip(skip).limit(per_page))
    
    # Format dates for display
    for article in articles:
        if 'published_date' in article and article['published_date']:
            article['display_date'] = article['published_date'].strftime('%Y-%m-%d %H:%M')
        else:
            article['display_date'] = 'Unknown'
    
    # Render template
    return render_template(
        'articles.html',
        articles=articles,
        packages=packages,
        current_page=page,
        total_pages=total_pages,
        total_articles=total_articles,
        per_page=per_page,
        package_id=package_id,
        search_query=search_query,
        sort_by=sort_by,
        sort_order=sort_order,
        now=datetime.datetime.now()
    )

@app.route('/articles/<article_id>')
def view_article(article_id):
    """View a single article"""
    db_conn = get_db()
    article = db_conn.db.articles.find_one({'_id': ObjectId(article_id)})
    
    if not article:
        flash('Article not found')
        return redirect(url_for('list_articles'))
    
    # Get package info if available
    package = None
    if 'scraping_package_id' in article:
        package = db_conn.db.scraping_packages.find_one({'_id': article['scraping_package_id']})
    
    # Format date for display
    if 'published_date' in article and article['published_date']:
        article['display_date'] = article['published_date'].strftime('%Y-%m-%d %H:%M')
    else:
        article['display_date'] = 'Unknown'
    
    # Convert content to Markup to preserve HTML
    if 'content' in article:
        article['display_content'] = Markup(article['content'])
    
    return render_template('article.html', article=article, package=package, now=datetime.datetime.now())

@app.route('/packages')
def list_packages():
    """List all scraping packages"""
    db_conn = get_db()
    packages = list(db_conn.db.scraping_packages.find())
    
    # Format dates for display
    for package in packages:
        if 'last_run' in package and package['last_run']:
            package['last_run_display'] = package['last_run'].strftime('%Y-%m-%d %H:%M')
        else:
            package['last_run_display'] = 'Never'
    
    return render_template('packages.html', packages=packages, now=datetime.datetime.now())

@app.route('/packages/<package_id>')
def view_package(package_id):
    """View a single package and its articles"""
    db_conn = get_db()
    package = db_conn.db.scraping_packages.find_one({'_id': ObjectId(package_id)})
    
    if not package:
        flash('Package not found')
        return redirect(url_for('list_packages'))
    
    # Get recent articles for this package
    articles = list(db_conn.db.articles.find(
        {'scraping_package_id': ObjectId(package_id)}
    ).sort('published_date', -1).limit(20))
    
    # Format dates for display
    for article in articles:
        if 'published_date' in article and article['published_date']:
            article['display_date'] = article['published_date'].strftime('%Y-%m-%d %H:%M')
        else:
            article['display_date'] = 'Unknown'
    
    return render_template('package.html', package=package, articles=articles, now=datetime.datetime.now())

@app.template_filter('truncate_html')
def truncate_html(html, length=200):
    """Truncate HTML text to specified length while preserving some formatting"""
    if not html:
        return ""
    
    # Simple truncation for now
    text = html.replace('<p>', ' ').replace('</p>', ' ').replace('<br>', ' ')
    if len(text) <= length:
        return text
    
    return text[:length] + '...'

@app.teardown_appcontext
def shutdown_session(exception=None):
    """Close database connection when app context ends"""
    close_db()

if __name__ == '__main__':
    # Create templates directory if it doesn't exist
    if not os.path.exists('templates'):
        os.makedirs('templates')
    
    # Create templates
    with open('templates/base.html', 'w', encoding='utf-8') as f:
        f.write('''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{% block title %}Proton CRM Article Viewer{% endblock %}</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .article-card { margin-bottom: 20px; }
        .keyword-pill { margin-right: 5px; margin-bottom: 5px; }
        .relevance-score { font-size: 0.8rem; }
        .package-badge { font-size: 0.8rem; }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <nav class="navbar navbar-expand-lg navbar-light bg-light">
                <div class="container-fluid">
                    <a class="navbar-brand" href="{{ url_for('index') }}">Proton CRM</a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarNav">
                        <ul class="navbar-nav">
                            <li class="nav-item">
                                <a class="nav-link" href="{{ url_for('list_articles') }}">Articles</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="{{ url_for('list_packages') }}">Packages</a>
                            </li>
                        </ul>
                    </div>
                </div>
            </nav>
        </header>
        
        {% with messages = get_flashed_messages() %}
            {% if messages %}
                {% for message in messages %}
                    <div class="alert alert-info">{{ message }}</div>
                {% endfor %}
            {% endif %}
        {% endwith %}
        
        <main>
            {% block content %}{% endblock %}
        </main>
        
        <footer class="mt-5 pt-3 border-top text-muted">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    {% block scripts %}{% endblock %}
</body>
</html>''')
    
    with open('templates/articles.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}Articles - Proton CRM{% endblock %}

{% block content %}
    <h1>Articles</h1>
    
    <div class="row mb-4">
        <div class="col-md-8">
            <form action="{{ url_for('list_articles') }}" method="get" class="row g-3">
                <div class="col-md-4">
                    <input type="text" class="form-control" name="q" placeholder="Search articles..." value="{{ search_query or '' }}">
                </div>
                <div class="col-md-3">
                    <select class="form-select" name="package">
                        <option value="">All Packages</option>
                        {% for pkg in packages %}
                            <option value="{{ pkg._id }}" {% if package_id and package_id == pkg._id|string %}selected{% endif %}>
                                {{ pkg.name }}
                            </option>
                        {% endfor %}
                    </select>
                </div>
                <div class="col-md-3">
                    <select class="form-select" name="sort">
                        <option value="date" {% if sort_by == 'date' %}selected{% endif %}>Sort by Date</option>
                        <option value="relevance" {% if sort_by == 'relevance' %}selected{% endif %}>Sort by Relevance</option>
                        <option value="title" {% if sort_by == 'title' %}selected{% endif %}>Sort by Title</option>
                    </select>
                </div>
                <div class="col-md-2">
                    <button type="submit" class="btn btn-primary w-100">Filter</button>
                </div>
            </form>
        </div>
        <div class="col-md-4 text-end">
            <p>{{ total_articles }} articles found</p>
        </div>
    </div>
    
    {% if articles %}
        {% for article in articles %}
            <div class="card article-card">
                <div class="card-body">
                    <div class="d-flex justify-content-between align-items-start mb-2">
                        <h5 class="card-title">
                            <a href="{{ url_for('view_article', article_id=article._id) }}">{{ article.title }}</a>
                        </h5>
                        {% if article.relevance_scores and article.relevance_scores.overall is defined %}
                            <span class="badge bg-primary relevance-score">
                                {{ (article.relevance_scores.overall * 100)|int }}% Relevance
                            </span>
                        {% endif %}
                    </div>
                    
                    <h6 class="card-subtitle mb-2 text-muted">
                        {% if article.source_name %}from {{ article.source_name }}{% endif %}
                        {% if article.display_date %} ‚Ä¢ {{ article.display_date }}{% endif %}
                        {% if article.scraping_package_name %}
                            ‚Ä¢ <span class="badge bg-secondary package-badge">{{ article.scraping_package_name }}</span>
                        {% endif %}
                    </h6>
                    
                    <p class="card-text">{{ article.content|truncate_html }}</p>
                    
                    {% if article.keywords %}
                        <div class="mt-2">
                            {% for keyword in article.keywords[:8] %}
                                <span class="badge bg-light text-dark keyword-pill">{{ keyword }}</span>
                            {% endfor %}
                            {% if article.keywords|length > 8 %}
                                <span class="badge bg-light text-dark">+{{ article.keywords|length - 8 }} more</span>
                            {% endif %}
                        </div>
                    {% endif %}
                </div>
            </div>
        {% endfor %}
        
        <!-- Pagination -->
        {% if total_pages > 1 %}
            <nav aria-label="Page navigation">
                <ul class="pagination justify-content-center">
                    <!-- Previous page link -->
                    <li class="page-item {% if current_page == 1 %}disabled{% endif %}">
                        <a class="page-link" href="{{ url_for('list_articles', page=current_page-1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                            Previous
                        </a>
                    </li>
                    
                    <!-- Page numbers -->
                    {% set start_page = [current_page - 2, 1]|max %}
                    {% set end_page = [start_page + 4, total_pages]|min %}
                    {% set start_page = [end_page - 4, 1]|max %}
                    
                    {% if start_page > 1 %}
                        <li class="page-item">
                            <a class="page-link" href="{{ url_for('list_articles', page=1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">1</a>
                        </li>
                        {% if start_page > 2 %}
                            <li class="page-item disabled"><span class="page-link">...</span></li>
                        {% endif %}
                    {% endif %}
                    
                    {% for page_num in range(start_page, end_page + 1) %}
                        <li class="page-item {% if page_num == current_page %}active{% endif %}">
                            <a class="page-link" href="{{ url_for('list_articles', page=page_num, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                                {{ page_num }}
                            </a>
                        </li>
                    {% endfor %}
                    
                    {% if end_page < total_pages %}
                        {% if end_page < total_pages - 1 %}
                            <li class="page-item disabled"><span class="page-link">...</span></li>
                        {% endif %}
                        <li class="page-item">
                            <a class="page-link" href="{{ url_for('list_articles', page=total_pages, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                                {{ total_pages }}
                            </a>
                        </li>
                    {% endif %}
                    
                    <!-- Next page link -->
                    <li class="page-item {% if current_page == total_pages %}disabled{% endif %}">
                        <a class="page-link" href="{{ url_for('list_articles', page=current_page+1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">
                            Next
                        </a>
                    </li>
                </ul>
            </nav>
        {% endif %}
    {% else %}
        <div class="alert alert-info">No articles found. Try changing your search criteria.</div>
    {% endif %}
{% endblock %}''')
    
    with open('templates/article.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}{{ article.title }} - Proton CRM{% endblock %}

{% block content %}
    <div class="mb-3">
        <a href="{{ url_for('list_articles') }}" class="btn btn-outline-secondary btn-sm">
            &larr; Back to Articles
        </a>
        {% if package %}
            <a href="{{ url_for('view_package', package_id=article.scraping_package_id) }}" class="btn btn-outline-info btn-sm">
                View Package: {{ package.name }}
            </a>
        {% endif %}
    </div>

    <article class="mb-5">
        <h1>{{ article.title }}</h1>
        
        <div class="mb-3 text-muted">
            {% if article.source_name %}
                <span>
                    Source: {% if article.source_url %}
                        <a href="{{ article.source_url }}" target="_blank">{{ article.source_name }}</a>
                    {% else %}
                        {{ article.source_name }}
                    {% endif %}
                </span> 
            {% endif %}
            
            {% if article.display_date %}
                <span>‚Ä¢ Published: {{ article.display_date }}</span>
            {% endif %}
            
            {% if article.author %}
                <span>‚Ä¢ Author: {{ article.author }}</span>
            {% endif %}
        </div>
        
        {% if article.scraping_package_name %}
            <div class="mb-3">
                <span class="badge bg-secondary">Package: {{ article.scraping_package_name }}</span>
                
                {% if article.relevance_scores %}
                    <span class="badge bg-primary">
                        Relevance: {{ (article.relevance_scores.overall * 100)|int }}%
                    </span>
                    
                    {% if article.relevance_scores.package_fit is defined %}
                        <span class="badge bg-info text-dark">
                            Package Fit: {{ (article.relevance_scores.package_fit * 100)|int }}%
                        </span>
                    {% endif %}
                    
                    {% if article.relevance_scores.recency is defined %}
                        <span class="badge bg-success">
                            Recency: {{ (article.relevance_scores.recency * 100)|int }}%
                        </span>
                    {% endif %}
                {% endif %}
            </div>
        {% endif %}
        
        {% if article.keywords %}
            <div class="mb-4">
                <h5>Keywords</h5>
                <div>
                    {% for keyword in article.keywords %}
                        <span class="badge bg-light text-dark keyword-pill">{{ keyword }}</span>
                    {% endfor %}
                </div>
            </div>
        {% endif %}
        
        {% if article.summary %}
            <div class="card mb-4">
                <div class="card-header">Summary</div>
                <div class="card-body">
                    <p class="card-text">{{ article.summary }}</p>
                </div>
            </div>
        {% endif %}
        
        <div class="content mb-4">
            <h5>Content</h5>
            <div class="mt-3">
                {{ article.display_content }}
            </div>
        </div>
        
        {% if article.entities %}
            <div class="mb-4">
                <h5>Entities</h5>
                <div>
                    {% for entity in article.entities %}
                        <span class="badge bg-light text-dark keyword-pill" title="{{ entity.type }}">
                            {{ entity.text }}
                        </span>
                    {% endfor %}
                </div>
            </div>
        {% endif %}
    </article>
{% endblock %}''')
    
    with open('templates/packages.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}Packages - Proton CRM{% endblock %}

{% block content %}
    <h1>Scraping Packages</h1>
    
    <div class="row mb-4">
        <div class="col">
            <p>Viewing {{ packages|length }} scraping packages</p>
        </div>
    </div>
    
    <div class="row">
        {% for package in packages %}
            <div class="col-md-4 mb-4">
                <div class="card h-100">
                    <div class="card-body">
                        <h5 class="card-title">
                            <a href="{{ url_for('view_package', package_id=package._id) }}">{{ package.name }}</a>
                        </h5>
                        <h6 class="card-subtitle mb-2 text-muted">
                            {{ package.status }} ‚Ä¢ {{ package.rss_feeds|length }} feeds
                        </h6>
                        <p class="card-text">{{ package.description }}</p>
                    </div>
                    <div class="card-footer">
                        <div class="d-flex justify-content-between align-items-center">
                            <span>{{ package.article_count or 0 }} articles</span>
                            <small class="text-muted">Last run: {{ package.last_run_display }}</small>
                        </div>
                    </div>
                </div>
            </div>
        {% endfor %}
    </div>
{% endblock %}''')
    
    with open('templates/package.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}{{ package.name }} - Proton CRM{% endblock %}

{% block content %}
    <div class="mb-3">
        <a href="{{ url_for('list_packages') }}" class="btn btn-outline-secondary btn-sm">
            &larr; Back to Packages
        </a>
    </div>

    <div class="card mb-4">
        <div class="card-body">
            <h1>{{ package.name }}</h1>
            <p class="mb-2">{{ package.description }}</p>
            
            <div class="mb-3">
                <span class="badge bg-{{ 'success' if package.status == 'active' else 'secondary' }}">
                    {{ package.status|title }}
                </span>
                <span class="badge bg-info text-dark">
                    Schedule: {{ package.schedule_interval }}
                </span>
                <span class="badge bg-info text-dark">
                    Max Articles: {{ package.max_articles_per_run }}
                </span>
            </div>
            
            <div class="row">
                <div class="col-md-6">
                    <h5>RSS Feeds</h5>
                    <ul class="list-group">
                        {% for feed in package.rss_feeds %}
                            <li class="list-group-item">{{ feed }}</li>
                        {% endfor %}
                    </ul>
                </div>
                <div class="col-md-6">
                    <h5>Statistics</h5>
                    <ul class="list-group">
                        <li class="list-group-item">Total Articles: {{ package.article_count or 0 }}</li>
                        <li class="list-group-item">Articles Last Run: {{ package.articles_last_run or 0 }}</li>
                        <li class="list-group-item">Last Run: {{ package.last_run_display }}</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    
    <h2>Recent Articles ({{ articles|length }})</h2>
    <div class="mb-2">
        <a href="{{ url_for('list_articles', package=package._id) }}" class="btn btn-primary">
            View All Articles from this Package
        </a>
    </div>
    
    {% if articles %}
        {% for article in articles %}
            <div class="card article-card">
                <div class="card-body">
                    <div class="d-flex justify-content-between align-items-start mb-2">
                        <h5 class="card-title">
                            <a href="{{ url_for('view_article', article_id=article._id) }}">{{ article.title }}</a>
                        </h5>
                        {% if article.relevance_scores and article.relevance_scores.overall is defined %}
                            <span class="badge bg-primary relevance-score">
                                {{ (article.relevance_scores.overall * 100)|int }}% Relevance
                            </span>
                        {% endif %}
                    </div>
                    
                    <h6 class="card-subtitle mb-2 text-muted">
                        {% if article.source_name %}from {{ article.source_name }}{% endif %}
                        {% if article.display_date %} ‚Ä¢ {{ article.display_date }}{% endif %}
                    </h6>
                    
                    <p class="card-text">{{ article.content|truncate_html }}</p>
                    
                    {% if article.keywords %}
                        <div class="mt-2">
                            {% for keyword in article.keywords[:8] %}
                                <span class="badge bg-light text-dark keyword-pill">{{ keyword }}</span>
                            {% endfor %}
                            {% if article.keywords|length > 8 %}
                                <span class="badge bg-light text-dark">+{{ article.keywords|length - 8 }} more</span>
                            {% endif %}
                        </div>
                    {% endif %}
                </div>
            </div>
        {% endfor %}
    {% else %}
        <div class="alert alert-info">No articles found for this package.</div>
    {% endif %}
{% endblock %}''')
    
    print("Created article viewer and templates")
    
    # Run the Flask app
    app.run(debug=True, host='0.0.0.0', port=5000)
</file>

<file path="xx/basic_app.py">
"""
Basic Streaming Newsletter Generator App

A simplified version of the agent newsletter app with better streaming support.
"""

import os
import datetime
import logging
import threading
import queue
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv
import PyPDF2
import io

# Import our agent
from agent_newsletter_generator import NewsletterAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')

# Configure upload folder
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}

# Global variables for streaming
stream_queue = queue.Queue()
agent = None

def init_agent():
    """Initialize the newsletter agent"""
    global agent
    try:
        agent = NewsletterAgent()
        logger.info("Newsletter agent initialized")
    except Exception as e:
        logger.error(f"Failed to initialize newsletter agent: {e}")
        agent = None

def allowed_file(filename):
    """Check if file has an allowed extension"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_from_file(file):
    """Extract text from uploaded file"""
    filename = file.filename
    file_ext = filename.rsplit('.', 1)[1].lower() if '.' in filename else ''
    
    if file_ext == 'txt' or file_ext == 'md':
        # Text files
        return file.read().decode('utf-8')
    elif file_ext == 'pdf':
        # PDF files
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    elif file_ext == 'docx':
        # DOCX files (requires python-docx package)
        try:
            import docx
            doc = docx.Document(io.BytesIO(file.read()))
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""
    
    return ""

# Initialize agent on startup
init_agent()

@app.route('/')
def index():
    """Render the main page"""
    global agent
    
    if agent is None:
        init_agent()
    
    try:
        # Get all scraping packages
        packages = agent.get_scraping_packages() if agent else []
        
        # Define available models
        openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"] if agent and agent.openai_client else []
        
        return render_template(
            'basic_index.html',
            packages=packages,
            openai_models=openai_models,
            now=datetime.datetime.now()
        )
    except Exception as e:
        logger.error(f"Error loading index page: {e}")
        return render_template('basic_index.html', packages=[], openai_models=[], now=datetime.datetime.now())

@app.route('/stream')
def stream():
    """Stream the agent's output"""
    def generate():
        while True:
            # Get message from queue
            try:
                message = stream_queue.get(timeout=1.0)
                if message == "[DONE]":
                    yield "data: [DONE]\n\n"
                    break
                else:
                    # Send the message as is - no need to escape or modify
                    yield f"data: {message}\n\n"
            except queue.Empty:
                # Send a keep-alive message
                yield "data: \n\n"
                continue
            except Exception as e:
                logger.error(f"Error in stream: {e}")
                yield f"data: Error: {str(e)}\n\n"
                yield "data: [DONE]\n\n"
                break
    
    return Response(generate(), mimetype='text/event-stream')

@app.route('/generate', methods=['POST'])
def generate():
    """Generate newsletter content using the agent"""
    global agent, stream_queue
    
    if agent is None:
        init_agent()
    
    if agent is None:
        return jsonify({"error": "Failed to initialize newsletter agent"}), 500
    
    # Clear the queue
    while not stream_queue.empty():
        try:
            stream_queue.get_nowait()
        except queue.Empty:
            break
    
    # Get form data
    model_name = request.form.get('model_name', 'gpt-4o')
    date_range = request.form.get('date_range', 'all')
    package_ids = request.form.getlist('package_ids')
    search_query = request.form.get('search_query', '')
    client_context = request.form.get('client_context', '')
    project_context = request.form.get('project_context', '')
    prompt = request.form.get('prompt', '')
    
    # Process uploaded files
    uploaded_context = ""
    if 'context_files' in request.files:
        files = request.files.getlist('context_files')
        for file in files:
            if file and file.filename and allowed_file(file.filename):
                file_text = extract_text_from_file(file)
                if file_text:
                    uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"
    
    # Define the callback function for streaming
    def stream_callback(content):
        stream_queue.put(content)
    
    # Run the agent in a separate thread
    def run_agent():
        try:
            # Put initial message in queue
            stream_queue.put("Starting newsletter generation...\n\n")
            
            # Run the agent
            result = agent.run_agent_with_query(
                search_query=search_query,
                date_range=date_range,
                package_ids=package_ids,
                client_context=client_context,
                project_context=project_context,
                prompt=prompt,
                uploaded_context=uploaded_context,
                model=model_name,
                stream_callback=stream_callback
            )
            
            # Signal the end of the stream
            stream_queue.put("[DONE]")
            
            # Return the result
            return result
        except Exception as e:
            logger.error(f"Error running agent: {e}")
            stream_queue.put(f"Error: {str(e)}")
            stream_queue.put("[DONE]")
            return {
                "error": str(e),
                "generated_content": f"Error generating newsletter: {str(e)}",
                "context_used": "",
                "articles_count": 0
            }
    
    # Start the agent thread
    agent_thread = threading.Thread(target=run_agent)
    agent_thread.daemon = True
    agent_thread.start()
    
    # Return success response
    return jsonify({"status": "success"})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5003))  # Use a different port
    app.run(host='0.0.0.0', port=port, debug=True)
</file>

<file path="xx/final_app.py">
"""
Final Newsletter Generator App

A refined version of the agent newsletter app that:
1. Properly handles line breaks without being excessive
2. Separates agent thoughts from the generated newsletter
"""

import os
import datetime
import logging
import threading
import queue
import re
import json
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv
import PyPDF2
import io

# Import our agent
from agent_newsletter_generator import NewsletterAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')

# Configure upload folder
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}

# Global variables for streaming
agent_thoughts_queue = queue.Queue()
newsletter_content = ""
context_used = ""
articles_count = 0
agent = None

def init_agent():
    """Initialize the newsletter agent"""
    global agent
    try:
        agent = NewsletterAgent()
        logger.info("Newsletter agent initialized")
    except Exception as e:
        logger.error(f"Failed to initialize newsletter agent: {e}")
        agent = None

def allowed_file(filename):
    """Check if file has an allowed extension"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_from_file(file):
    """Extract text from uploaded file"""
    filename = file.filename
    file_ext = filename.rsplit('.', 1)[1].lower() if '.' in filename else ''

    if file_ext == 'txt' or file_ext == 'md':
        # Text files
        return file.read().decode('utf-8')
    elif file_ext == 'pdf':
        # PDF files
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    elif file_ext == 'docx':
        # DOCX files (requires python-docx package)
        try:
            import docx
            doc = docx.Document(io.BytesIO(file.read()))
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""

    return ""

# Initialize agent on startup
init_agent()

@app.route('/')
def index():
    """Render the main page"""
    global agent

    if agent is None:
        init_agent()

    try:
        # Get all scraping packages
        packages = agent.get_scraping_packages() if agent else []

        # Define available models
        openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"] if agent and agent.openai_client else []

        return render_template(
            'final_index.html',
            packages=packages,
            openai_models=openai_models,
            now=datetime.datetime.now()
        )
    except Exception as e:
        logger.error(f"Error loading index page: {e}")
        return render_template('final_index.html', packages=[], openai_models=[], now=datetime.datetime.now())

@app.route('/stream-thoughts')
def stream_thoughts():
    """Stream the agent's thoughts and process"""
    def generate():
        while True:
            # Get message from queue
            try:
                message = agent_thoughts_queue.get(timeout=1.0)
                if message == "[DONE]":
                    yield "data: [DONE]\n\n"
                    break
                else:
                    # Explicitly encode newlines for SSE
                    message_sse = message.replace("\n", "\\n")
                    yield f"data: {message_sse}\n\n"
            except queue.Empty:
                # Send a keep-alive message
                yield "data: \n\n"
                continue
            except Exception as e:
                logger.error(f"Error in stream: {e}")
                yield f"data: Error: {str(e)}\n\n"
                yield "data: [DONE]\n\n"
                break

    return Response(generate(), mimetype='text/event-stream')

@app.route('/get-newsletter')
def get_newsletter():
    """Get the generated newsletter content and context information"""
    global newsletter_content, context_used, articles_count
    return jsonify({
        "content": newsletter_content,
        "context_used": context_used,
        "articles_count": articles_count
    })

@app.route('/import-json', methods=['POST'])
def import_json():
    """Import newsletter data from JSON file"""
    global newsletter_content, context_used, articles_count

    try:
        # Get the uploaded JSON file
        if 'json_file' not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files['json_file']
        if file.filename == '':
            return jsonify({"error": "No file selected"}), 400

        # Read and parse the JSON data
        try:
            json_data = json.loads(file.read().decode('utf-8'))
        except Exception as e:
            logger.error(f"Error parsing JSON file: {e}")
            return jsonify({"error": f"Invalid JSON file: {str(e)}"}), 400

        # Validate the JSON structure
        if 'inputs' not in json_data or 'outputs' not in json_data:
            return jsonify({"error": "Invalid JSON format: missing 'inputs' or 'outputs' sections"}), 400

        # Extract the data
        inputs = json_data.get('inputs', {})
        outputs = json_data.get('outputs', {})

        # Update the global variables
        newsletter_content = outputs.get('content', '')
        context_used = outputs.get('context_used', '')
        articles_count = outputs.get('articles_count', 0)

        # Return the imported data
        return jsonify({
            "status": "success",
            "message": "JSON data imported successfully",
            "inputs": inputs,
            "outputs": {
                "content": newsletter_content,
                "context_used": context_used,
                "articles_count": articles_count
            }
        })
    except Exception as e:
        logger.error(f"Error importing JSON data: {e}")
        return jsonify({"error": f"Error importing JSON data: {str(e)}"}), 500

@app.route('/generate', methods=['POST'])
def generate():
    """Generate newsletter content using the agent"""
    global agent, agent_thoughts_queue, newsletter_content

    if agent is None:
        init_agent()

    if agent is None:
        return jsonify({"error": "Failed to initialize newsletter agent"}), 500

    # Clear the queue and reset all content
    while not agent_thoughts_queue.empty():
        try:
            agent_thoughts_queue.get_nowait()
        except queue.Empty:
            break

    # Reset all global variables
    global newsletter_content, context_used, articles_count
    newsletter_content = ""
    context_used = ""
    articles_count = 0

    # Get form data
    model_name = request.form.get('model_name', 'gpt-4o')
    date_range = request.form.get('date_range', 'all')
    package_ids = request.form.getlist('package_ids')
    search_query = request.form.get('search_query', '')
    client_context = request.form.get('client_context', '')
    project_context = request.form.get('project_context', '')
    prompt = request.form.get('prompt', '')

    # Process uploaded files
    uploaded_context = ""
    if 'context_files' in request.files:
        files = request.files.getlist('context_files')
        for file in files:
            if file and file.filename and allowed_file(file.filename):
                file_text = extract_text_from_file(file)
                if file_text:
                    uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"

    # Define the callback function for streaming
    def stream_callback(content):
        global newsletter_content

        # Check if this is part of the actual newsletter
        # We'll consider content that starts with a title-like format to be the newsletter
        if re.match(r'^#+ |^\*\*[^*]+\*\*|^[A-Z][^a-z]+:', content) and not content.startswith("Agent"):
            # This is likely part of the newsletter
            newsletter_content += content
            # Also send to the thoughts queue but mark it as newsletter content
            agent_thoughts_queue.put(content)
        else:
            # This is agent thoughts/process
            agent_thoughts_queue.put(content)

    # Run the agent in a separate thread
    def run_agent():
        # Declare globals at the beginning of the function
        global newsletter_content, context_used, articles_count

        try:
            # Put initial message in queue
            agent_thoughts_queue.put("Starting newsletter generation...\n")

            # Run the agent
            result = agent.run_agent_with_query(
                search_query=search_query,
                date_range=date_range,
                package_ids=package_ids,
                client_context=client_context,
                project_context=project_context,
                prompt=prompt,
                uploaded_context=uploaded_context,
                model=model_name,
                stream_callback=stream_callback
            )

            # Store the context information and article count

            # If we didn't capture any newsletter content, use the generated_content from the result
            if not newsletter_content and "generated_content" in result:
                newsletter_content = result["generated_content"]

            # Store the context used and article count
            if "context_used" in result:
                context_used = result["context_used"]

            if "articles_count" in result:
                articles_count = result["articles_count"]

            # Signal the end of the stream
            agent_thoughts_queue.put("[DONE]")

            # Return the result
            return result
        except Exception as e:
            logger.error(f"Error running agent: {e}")
            agent_thoughts_queue.put(f"Error: {str(e)}")
            agent_thoughts_queue.put("[DONE]")

            # Update global variables for error case
            newsletter_content = f"Error generating newsletter: {str(e)}"
            context_used = ""
            articles_count = 0

            return {
                "error": str(e),
                "generated_content": newsletter_content,
                "context_used": context_used,
                "articles_count": articles_count
            }

    # Start the agent thread
    agent_thread = threading.Thread(target=run_agent)
    agent_thread.daemon = True
    agent_thread.start()

    # Return success response
    return jsonify({"status": "success"})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5006))  # Use a different port
    app.run(host='0.0.0.0', port=port, debug=True)
</file>

<file path="xx/fixed_app.py">
"""
Fixed Newsletter Generator App

A fixed version of the agent newsletter app with proper line break handling.
"""

import os
import datetime
import logging
import threading
import queue
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv
import PyPDF2
import io

# Import our agent
from agent_newsletter_generator import NewsletterAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')

# Configure upload folder
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}

# Global variables for streaming
stream_queue = queue.Queue()
agent = None

def init_agent():
    """Initialize the newsletter agent"""
    global agent
    try:
        agent = NewsletterAgent()
        logger.info("Newsletter agent initialized")
    except Exception as e:
        logger.error(f"Failed to initialize newsletter agent: {e}")
        agent = None

def allowed_file(filename):
    """Check if file has an allowed extension"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_from_file(file):
    """Extract text from uploaded file"""
    filename = file.filename
    file_ext = filename.rsplit('.', 1)[1].lower() if '.' in filename else ''
    
    if file_ext == 'txt' or file_ext == 'md':
        # Text files
        return file.read().decode('utf-8')
    elif file_ext == 'pdf':
        # PDF files
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    elif file_ext == 'docx':
        # DOCX files (requires python-docx package)
        try:
            import docx
            doc = docx.Document(io.BytesIO(file.read()))
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""
    
    return ""

# Initialize agent on startup
init_agent()

@app.route('/')
def index():
    """Render the main page"""
    global agent
    
    if agent is None:
        init_agent()
    
    try:
        # Get all scraping packages
        packages = agent.get_scraping_packages() if agent else []
        
        # Define available models
        openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"] if agent and agent.openai_client else []
        
        return render_template(
            'fixed_index.html',
            packages=packages,
            openai_models=openai_models,
            now=datetime.datetime.now()
        )
    except Exception as e:
        logger.error(f"Error loading index page: {e}")
        return render_template('fixed_index.html', packages=[], openai_models=[], now=datetime.datetime.now())

@app.route('/stream')
def stream():
    """Stream the agent's output"""
    def generate():
        while True:
            # Get message from queue
            try:
                message = stream_queue.get(timeout=1.0)
                if message == "[DONE]":
                    yield "data: [DONE]\n\n"
                    break
                else:
                    # Explicitly encode newlines for SSE
                    message_sse = message.replace("\n", "\\n")
                    yield f"data: {message_sse}\n\n"
            except queue.Empty:
                # Send a keep-alive message
                yield "data: \n\n"
                continue
            except Exception as e:
                logger.error(f"Error in stream: {e}")
                yield f"data: Error: {str(e)}\n\n"
                yield "data: [DONE]\n\n"
                break
    
    return Response(generate(), mimetype='text/event-stream')

@app.route('/generate', methods=['POST'])
def generate():
    """Generate newsletter content using the agent"""
    global agent, stream_queue
    
    if agent is None:
        init_agent()
    
    if agent is None:
        return jsonify({"error": "Failed to initialize newsletter agent"}), 500
    
    # Clear the queue
    while not stream_queue.empty():
        try:
            stream_queue.get_nowait()
        except queue.Empty:
            break
    
    # Get form data
    model_name = request.form.get('model_name', 'gpt-4o')
    date_range = request.form.get('date_range', 'all')
    package_ids = request.form.getlist('package_ids')
    search_query = request.form.get('search_query', '')
    client_context = request.form.get('client_context', '')
    project_context = request.form.get('project_context', '')
    prompt = request.form.get('prompt', '')
    
    # Process uploaded files
    uploaded_context = ""
    if 'context_files' in request.files:
        files = request.files.getlist('context_files')
        for file in files:
            if file and file.filename and allowed_file(file.filename):
                file_text = extract_text_from_file(file)
                if file_text:
                    uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"
    
    # Define the callback function for streaming
    def stream_callback(content):
        stream_queue.put(content)
    
    # Run the agent in a separate thread
    def run_agent():
        try:
            # Put initial message in queue
            stream_queue.put("Starting newsletter generation...\n\n")
            
            # Run the agent
            result = agent.run_agent_with_query(
                search_query=search_query,
                date_range=date_range,
                package_ids=package_ids,
                client_context=client_context,
                project_context=project_context,
                prompt=prompt,
                uploaded_context=uploaded_context,
                model=model_name,
                stream_callback=stream_callback
            )
            
            # Signal the end of the stream
            stream_queue.put("[DONE]")
            
            # Return the result
            return result
        except Exception as e:
            logger.error(f"Error running agent: {e}")
            stream_queue.put(f"Error: {str(e)}")
            stream_queue.put("[DONE]")
            return {
                "error": str(e),
                "generated_content": f"Error generating newsletter: {str(e)}",
                "context_used": "",
                "articles_count": 0
            }
    
    # Start the agent thread
    agent_thread = threading.Thread(target=run_agent)
    agent_thread.daemon = True
    agent_thread.start()
    
    # Return success response
    return jsonify({"status": "success"})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5005))  # Use a different port
    app.run(host='0.0.0.0', port=port, debug=True)
</file>

<file path="xx/scraper_example.py">
"""
Example script for scraping articles and storing them in MongoDB using the Proton schema
"""

import sys
import os
import datetime
import logging
from typing import Dict, List, Any, Optional
import requests
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from bson.objectid import ObjectId
import hashlib
import time

# Add parent directory to path to import ProtonDB
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from proton_db_setup import ProtonDB

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize NLTK resources (uncomment these lines for first-time setup)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Try to create a fallback for punkt_tab issue
try:
    nltk.download('punkt_tab')
except:
    print("Note: punkt_tab may not be available. Using punkt instead.")

class SimpleArticleScraper:
    """
    A simple article scraper example for the Proton CRM system
    """
    
    def __init__(self, project_id: str, scraping_package_id: str):
        """
        Initialize the scraper
        
        Args:
            project_id: MongoDB ObjectId of the project
            scraping_package_id: MongoDB ObjectId of the scraping package
        """
        self.project_id = project_id
        self.scraping_package_id = scraping_package_id
        self.db = ProtonDB()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
    def extract_article_from_url(self, url: str) -> Dict[str, Any]:
        """
        Extract article content from a URL
        
        Args:
            url: URL of the article to scrape
            
        Returns:
            Dict[str, Any]: Dictionary containing article data
        """
        try:
            # Add a small delay to be considerate to the target server
            time.sleep(1)
            
            # Fetch the URL content
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # Parse with BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract basic article information (this is a simplified example)
            # In a real implementation, you'd want more sophisticated parsing specific to the site
            title = soup.find('h1')
            title_text = title.get_text().strip() if title else "No title found"
            
            # Try to find the article content
            # This is a simplified approach - real implementations would be more sophisticated
            article_content = ""
            main_content = soup.find('article') or soup.find('main') or soup.find('div', class_='content')
            
            if main_content:
                # Get all paragraphs from the main content
                paragraphs = main_content.find_all('p')
                article_content = ' '.join([p.get_text().strip() for p in paragraphs])
            else:
                # Fallback: get all paragraphs from the body
                paragraphs = soup.find_all('p')
                article_content = ' '.join([p.get_text().strip() for p in paragraphs])
            
            # Extract author information if available
            author_elem = soup.find('meta', attrs={'name': 'author'}) or soup.find('a', rel='author')
            author = author_elem.get('content', '') if hasattr(author_elem, 'get') and author_elem.get('content') else (
                author_elem.get_text().strip() if author_elem else "Unknown"
            )
            
            # Try to find the publication date
            # This is simplified - real implementations would have more robust date extraction
            date_elem = soup.find('meta', property='article:published_time') or soup.find('time')
            published_date = None
            if date_elem:
                date_str = date_elem.get('content', '') or date_elem.get('datetime', '')
                if date_str:
                    try:
                        published_date = datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                    except (ValueError, TypeError):
                        published_date = None
            
            # Extract main image if available
            image_url = None
            image_elem = soup.find('meta', property='og:image')
            if image_elem and image_elem.get('content'):
                image_url = image_elem.get('content')
            else:
                # Try to find the first large image in the article
                images = main_content.find_all('img') if main_content else soup.find_all('img')
                for img in images:
                    if img.get('src') and (img.get('width') is None or int(img.get('width') or 0) >= 300):
                        image_url = img.get('src')
                        if not image_url.startswith('http'):
                            # Handle relative URLs
                            base_url = '/'.join(url.split('/')[:3])  # Get the domain part
                            image_url = f"{base_url}/{image_url.lstrip('/')}"
                        break
            
            # Extract the domain name for source_name
            domain_parts = url.split('//')[-1].split('/')[0].split('.')
            source_name = domain_parts[-2] if len(domain_parts) >= 2 else domain_parts[0]
            source_name = source_name.capitalize()
            
            # Extract keywords from content
            keywords = self._extract_keywords(article_content)
            
            # Extract named entities
            entities = self._extract_entities(article_content)
            
            # Generate a simple summary (first 2-3 sentences)
            summary = self._generate_summary(article_content)
            
            # Prepare the article data dictionary
            article_data = {
                "title": title_text,
                "content": article_content,
                "summary": summary,
                "source_url": url,
                "source_name": source_name,
                "author": author,
                "published_date": published_date,
                "date_scraped": datetime.datetime.utcnow(),
                "scraping_package_id": ObjectId(self.scraping_package_id),
                "project_ids": [ObjectId(self.project_id)],
                "keywords": keywords,
                "entities": entities,
                "content_type": "news",  # Default to news
                "status": "active",
                "image_url": image_url
            }
            
            return article_data
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Error fetching URL {url}: {e}")
            raise
        except Exception as e:
            logger.error(f"Error processing article from URL {url}: {e}")
            raise
    
    def _extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """
        Extract keywords from text
        
        Args:
            text: Text to extract keywords from
            max_keywords: Maximum number of keywords to extract
            
        Returns:
            List[str]: List of keywords
        """
        try:
            # Tokenize the text - handle punkt_tab resource issue
            try:
                tokens = word_tokenize(text.lower())
            except:
                # Fallback simple tokenization if NLTK tokenization fails
                tokens = text.lower().split()
            
            # Remove stopwords and non-alphabetic tokens
            try:
                stop_words = set(stopwords.words('english'))
            except:
                # Basic stopwords if NLTK stopwords are unavailable
                stop_words = set(['the', 'and', 'a', 'to', 'in', 'of', 'is', 'that', 'it', 'for', 'with', 'as', 'on', 'by', 'this', 'be', 'are'])
                
            filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 3]
            
            # Count frequency of each word
            word_freq = {}
            for word in filtered_tokens:
                if word in word_freq:
                    word_freq[word] += 1
                else:
                    word_freq[word] = 1
            
            # Sort by frequency and get top keywords
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            keywords = [word for word, _ in sorted_words[:max_keywords]]
            
            return keywords
        except Exception as e:
            logger.warning(f"Error extracting keywords: {e}")
            return []
    
    def _extract_entities(self, text: str) -> List[Dict[str, str]]:
        """
        Extract named entities from text
        
        Args:
            text: Text to extract entities from
            
        Returns:
            List[Dict[str, str]]: List of entities with text and type
        """
        try:
            # Tokenize and tag parts of speech - handle punkt_tab resource issue
            try:
                tokens = word_tokenize(text)
            except:
                tokens = text.split()
                
            try:
                pos_tags = nltk.pos_tag(tokens)
                
                # Extract named entities
                named_entities = nltk.ne_chunk(pos_tags)
                
                # Process the named entities tree to extract entities
                entities = []
                
                for chunk in named_entities:
                    if hasattr(chunk, 'label'):
                        # Start of a named entity
                        entity_type = chunk.label()
                        entity_text = ' '.join([token for token, pos in chunk.leaves()])
                        entities.append({
                            "text": entity_text,
                            "type": entity_type
                        })
                
                # Remove duplicates while preserving order
                unique_entities = []
                seen = set()
                for entity in entities:
                    key = f"{entity['text'].lower()}|{entity['type']}"
                    if key not in seen:
                        seen.add(key)
                        unique_entities.append(entity)
                
                return unique_entities[:20]  # Limit to top 20 entities
            except:
                # Very basic fallback entity extraction - just grab capitalized words
                # This is not ideal but provides something if NLTK functions fail
                words = text.split()
                entities = []
                for word in words:
                    if word and word[0].isupper() and len(word) > 1:
                        if {'text': word, 'type': 'UNKNOWN'} not in entities:
                            entities.append({'text': word, 'type': 'UNKNOWN'})
                return entities[:20]
        except Exception as e:
            logger.warning(f"Error extracting entities: {e}")
            return []
    
    def _generate_summary(self, text: str, max_sentences: int = 3) -> str:
        """
        Generate a simple summary from text (first few sentences)
        
        Args:
            text: Text to summarize
            max_sentences: Maximum number of sentences in the summary
            
        Returns:
            str: Summary text
        """
        try:
            # Try NLTK sentence tokenization
            try:
                sentences = sent_tokenize(text)
            except:
                # Simple fallback for sentence tokenization
                # This is not perfect but works for basic cases
                sentences = []
                for sentence in text.split('.'):
                    if len(sentence.strip()) > 10:  # Ignore very short fragments
                        sentences.append(sentence.strip() + '.')
            
            summary = ' '.join(sentences[:max_sentences])
            return summary
        except Exception as e:
            logger.warning(f"Error generating summary: {e}")
            return text[:250] + "..."  # Fallback to first 250 chars
    
    def scrape_and_store_articles(self, urls: List[str]) -> List[str]:
        """
        Scrape articles from URLs and store them in MongoDB
        
        Args:
            urls: List of URLs to scrape
            
        Returns:
            List[str]: List of stored article IDs
        """
        article_ids = []
        for url in urls:
            try:
                logger.info(f"Scraping article from URL: {url}")
                article_data = self.extract_article_from_url(url)
                
                # Store in database
                article_id = self.db.insert_article(article_data)
                article_ids.append(article_id)
                logger.info(f"Stored article '{article_data['title']}' with ID: {article_id}")
                
            except Exception as e:
                logger.error(f"Failed to process URL {url}: {e}")
        
        return article_ids
    
    def close(self):
        """
        Close the database connection
        """
        self.db.close()


if __name__ == "__main__":
    # Example URLs to scrape
    # In practice, these would come from RSS feeds, site maps, or other sources
    example_urls = [
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://www.python.org/about/"
    ]
    
    # Example project and scraping package IDs (replace with real IDs in production)
    project_id = str(ObjectId())  # For testing only - use a real project ID
    scraping_package_id = str(ObjectId())  # For testing only - use a real package ID
    
    try:
        # Initialize scraper
        scraper = SimpleArticleScraper(project_id, scraping_package_id)
        
        # Scrape and store articles
        article_ids = scraper.scrape_and_store_articles(example_urls)
        
        # Report results
        logger.info(f"Successfully stored {len(article_ids)} articles")
        
        # Clean up
        scraper.close()
        
    except Exception as e:
        logger.error(f"Scraper error: {e}")
</file>

<file path="xx/simple_streaming_app.py">
"""
Simple Streaming Newsletter Generator App

This is a simplified version of the agent newsletter app with better streaming support.
"""

import os
import datetime
import logging
import threading
import queue
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv

# Import our agent
from agent_newsletter_generator import NewsletterAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')

# Global variables for streaming
stream_queue = queue.Queue()
agent = None

def init_agent():
    """Initialize the newsletter agent"""
    global agent
    try:
        agent = NewsletterAgent()
        logger.info("Newsletter agent initialized")
    except Exception as e:
        logger.error(f"Failed to initialize newsletter agent: {e}")
        agent = None

# Initialize agent on startup
init_agent()

@app.route('/')
def index():
    """Render the main page"""
    global agent

    if agent is None:
        init_agent()

    try:
        # Get all scraping packages
        packages = agent.get_scraping_packages() if agent else []

        # Define available models
        openai_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"] if agent and agent.openai_client else []

        return render_template(
            'simple_index.html',
            packages=packages,
            openai_models=openai_models,
            now=datetime.datetime.now()
        )
    except Exception as e:
        logger.error(f"Error loading index page: {e}")
        return render_template('simple_index.html', packages=[], now=datetime.datetime.now())

@app.route('/stream')
def stream():
    """Stream the agent's output"""
    def generate():
        while True:
            # Get message from queue
            try:
                message = stream_queue.get(timeout=1.0)
                if message == "[DONE]":
                    yield "data: [DONE]\n\n"
                    break
                else:
                    # Ensure message is properly escaped for SSE
                    escaped_message = message.replace('\n', '\n')
                    yield f"data: {escaped_message}\n\n"
            except queue.Empty:
                # Send a keep-alive message
                yield "data: \n\n"
                continue
            except Exception as e:
                logger.error(f"Error in stream: {e}")
                yield f"data: Error: {str(e)}\n\n"
                yield "data: [DONE]\n\n"
                break

    return Response(generate(), mimetype='text/event-stream')

@app.route('/generate', methods=['POST'])
def generate():
    """Generate newsletter content using the agent"""
    global agent, stream_queue

    if agent is None:
        init_agent()

    if agent is None:
        return jsonify({"error": "Failed to initialize newsletter agent"}), 500

    # Clear the queue
    while not stream_queue.empty():
        try:
            stream_queue.get_nowait()
        except queue.Empty:
            break

    # Get form data
    model_name = request.form.get('model_name', 'gpt-4o')
    date_range = request.form.get('date_range', 'all')
    package_ids = request.form.getlist('package_ids')
    search_query = request.form.get('search_query', '')
    client_context = request.form.get('client_context', '')
    project_context = request.form.get('project_context', '')
    prompt = request.form.get('prompt', '')

    # Define the callback function for streaming
    def stream_callback(content):
        stream_queue.put(content)

    # Run the agent in a separate thread
    def run_agent():
        try:
            # Put initial message in queue
            stream_queue.put("Starting newsletter generation...\n\n")

            # Run the agent
            result = agent.run_agent_with_query(
                search_query=search_query,
                date_range=date_range,
                package_ids=package_ids,
                client_context=client_context,
                project_context=project_context,
                prompt=prompt,
                model=model_name,
                stream_callback=stream_callback
            )

            # Signal the end of the stream
            stream_queue.put("[DONE]")

            # Return the result
            return result
        except Exception as e:
            logger.error(f"Error running agent: {e}")
            stream_queue.put(f"Error: {str(e)}")
            stream_queue.put("[DONE]")
            return {
                "error": str(e),
                "generated_content": f"Error generating newsletter: {str(e)}",
                "context_used": "",
                "articles_count": 0
            }

    # Start the agent thread
    agent_thread = threading.Thread(target=run_agent)
    agent_thread.daemon = True
    agent_thread.start()

    # Return success response
    return jsonify({"status": "success"})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5002))  # Use a different port
    app.run(host='0.0.0.0', port=port, debug=True)
</file>

<file path="xx/test_agent.py">
#!/usr/bin/env python3
"""
Test script for the Newsletter Agent

This script allows you to test the newsletter agent from the command line
without running the full web application.
"""

import argparse
import sys
import logging
from agent_newsletter_generator import NewsletterAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Main function to run the agent test"""
    parser = argparse.ArgumentParser(description='Test the Newsletter Agent')
    parser.add_argument('--query', type=str, default='', help='Search query for articles')
    parser.add_argument('--date-range', type=str, default='all', 
                        choices=['all', '7days', '30days', '90days'],
                        help='Date range for articles')
    parser.add_argument('--client-context', type=str, default='', help='Context about the client')
    parser.add_argument('--project-context', type=str, default='', help='Context about the project')
    parser.add_argument('--prompt', type=str, required=True, help='Instructions for newsletter generation')
    parser.add_argument('--model', type=str, default='gpt-4o', help='OpenAI model to use')
    parser.add_argument('--output', type=str, help='Output file for the generated newsletter')
    
    args = parser.parse_args()
    
    # Create agent
    logger.info("Initializing newsletter agent...")
    agent = NewsletterAgent()
    
    try:
        # Run agent
        logger.info("Running agent with query: %s", args.query)
        result = agent.run_agent_with_query(
            search_query=args.query,
            date_range=args.date_range,
            package_ids=[],  # Empty list means all packages
            client_context=args.client_context,
            project_context=args.project_context,
            prompt=args.prompt,
            model=args.model
        )
        
        # Print result
        if "error" in result:
            logger.error("Error: %s", result["error"])
            return 1
        
        # Print or save the generated content
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(result["generated_content"])
            logger.info("Newsletter saved to %s", args.output)
        else:
            print("\n" + "="*80)
            print("GENERATED NEWSLETTER:")
            print("="*80)
            print(result["generated_content"])
            print("\n" + "="*80)
        
        logger.info("Articles used: %d", result["articles_count"])
        return 0
    
    except Exception as e:
        logger.error("Error running agent: %s", str(e))
        return 1
    
    finally:
        # Close connections
        agent.close()

if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="xx/test_embeddings.py">
"""
Test script for vector embeddings and NLP processing

This script tests both OpenAI and local embeddings, as well as NLP processing
to verify that all components are working correctly.
"""

import os
import sys
import logging
import time
from typing import List, Dict, Any

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_vector_embeddings():
    """Test both OpenAI and local vector embeddings"""
    
    try:
        from vector_embeddings import EmbeddingGenerator
        
        test_texts = [
            "This is a test of the embedding system for Proton CRM.",
            "Vector embeddings are used for semantic search and content recommendation.",
            "Natural language processing helps extract meaning from unstructured text data."
        ]
        
        logger.info("=" * 50)
        logger.info("TESTING VECTOR EMBEDDINGS")
        logger.info("=" * 50)
        
        # Test OpenAI embeddings
        logger.info("\nTesting OpenAI embeddings:")
        try:
            openai_embedder = EmbeddingGenerator(model_type="openai")
            logger.info(f"Initialized OpenAI embedder (actual model type: {openai_embedder.model_type})")
            
            start_time = time.time()
            openai_embedding = openai_embedder.get_embedding(test_texts[0])
            duration = time.time() - start_time
            
            logger.info(f"OpenAI embedding generated in {duration:.2f} seconds")
            logger.info(f"Embedding dimension: {len(openai_embedding)}")
            logger.info(f"First 5 values: {openai_embedding[:5]}")
            
            # Test batch embeddings
            start_time = time.time()
            batch_embeddings = openai_embedder.batch_get_embeddings(test_texts)
            duration = time.time() - start_time
            
            logger.info(f"Batch embeddings ({len(batch_embeddings)}) generated in {duration:.2f} seconds")
            logger.info(f"All have same dimensions: {all(len(emb) == len(openai_embedding) for emb in batch_embeddings)}")
            
            logger.info("‚úÖ OpenAI embeddings test passed")
        except Exception as e:
            logger.error(f"‚ùå OpenAI embeddings test failed: {e}")
        
        # Test local embeddings
        logger.info("\nTesting local embeddings:")
        try:
            local_embedder = EmbeddingGenerator(model_type="local")
            logger.info(f"Initialized local embedder (model: {local_embedder.model_type})")
            
            start_time = time.time()
            local_embedding = local_embedder.get_embedding(test_texts[0])
            duration = time.time() - start_time
            
            logger.info(f"Local embedding generated in {duration:.2f} seconds")
            logger.info(f"Embedding dimension: {len(local_embedding)}")
            logger.info(f"First 5 values: {local_embedding[:5]}")
            
            # Test batch embeddings
            start_time = time.time()
            batch_embeddings = local_embedder.batch_get_embeddings(test_texts)
            duration = time.time() - start_time
            
            logger.info(f"Batch embeddings ({len(batch_embeddings)}) generated in {duration:.2f} seconds")
            logger.info(f"All have same dimensions: {all(len(emb) == len(local_embedding) for emb in batch_embeddings)}")
            
            logger.info("‚úÖ Local embeddings test passed")
        except Exception as e:
            logger.error(f"‚ùå Local embeddings test failed: {e}")
            
    except Exception as e:
        logger.error(f"Error in embeddings test: {e}")

def test_nlp_processing():
    """Test NLP processing (keyword extraction and summarization)"""
    
    try:
        from scraping_package_scheduler import ScrapingPackage
        from bson.objectid import ObjectId
        
        logger.info("\n" + "=" * 50)
        logger.info("TESTING NLP PROCESSING")
        logger.info("=" * 50)
        
        # Create a test package
        test_package = ScrapingPackage(
            package_id=str(ObjectId()),
            project_ids=[str(ObjectId())],
            name="NLP Test Package",
            description="Testing NLP processing capabilities",
            rss_feeds=["https://example.com/feed"],
            calculate_embeddings=False  # Disable embeddings for this test
        )
        
        # Test text for NLP processing
        test_text = """
        Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction
        between computers and humans through natural language. The ultimate objective of NLP is to read, decipher,
        understand, and make sense of human language in a valuable way.
        
        NLP combines computational linguistics‚Äîrule-based modeling of human language‚Äîwith statistical, machine learning,
        and deep learning models. These technologies enable computers to process human language in the form of text or
        voice data and to 'understand' its full meaning, complete with the speaker or writer's intent and sentiment.
        
        NLP drives computer programs that translate text from one language to another, respond to spoken commands,
        and summarize large volumes of text rapidly‚Äîeven in real time. There's a good chance you've interacted with
        NLP in the form of voice-operated GPS systems, digital assistants, speech-to-text dictation software,
        customer service chatbots, and other consumer conveniences.
        
        But NLP also plays a growing role in enterprise solutions that help streamline business operations, increase
        employee productivity, and simplify mission-critical business processes.
        """
        
        # Test keyword extraction
        logger.info("\nTesting keyword extraction:")
        try:
            start_time = time.time()
            keywords = test_package._extract_keywords(test_text)
            duration = time.time() - start_time
            
            logger.info(f"Keywords extracted in {duration:.2f} seconds")
            logger.info(f"Found {len(keywords)} keywords: {', '.join(keywords)}")
            logger.info("‚úÖ Keyword extraction test passed")
        except Exception as e:
            logger.error(f"‚ùå Keyword extraction test failed: {e}")
        
        # Test summary generation
        logger.info("\nTesting summary generation:")
        try:
            start_time = time.time()
            summary = test_package._generate_summary(test_text)
            duration = time.time() - start_time
            
            logger.info(f"Summary generated in {duration:.2f} seconds")
            logger.info(f"Summary length: {len(summary)} characters")
            logger.info(f"Summary: {summary}")
            logger.info("‚úÖ Summary generation test passed")
        except Exception as e:
            logger.error(f"‚ùå Summary generation test failed: {e}")
            
    except Exception as e:
        logger.error(f"Error in NLP processing test: {e}")

if __name__ == "__main__":
    # Test vector embeddings
    test_vector_embeddings()
    
    # Test NLP processing
    test_nlp_processing()
    
    logger.info("\n" + "=" * 50)
    logger.info("ALL TESTS COMPLETED")
    logger.info("=" * 50)
</file>

<file path="xx/test_scraping.py">
"""
Test script for the scraping package functionality

This script tests the scraping package with NLP processing and vector embeddings
to verify that everything is working correctly.
"""

import os
import sys
import time
import logging
from bson.objectid import ObjectId
import traceback

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add parent directory to path to import ProtonDB
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from proton_db_setup import ProtonDB
from scraping_package_scheduler import ScrapingPackage

def test_scraping_with_embeddings():
    """Test the scraping package with NLP and embeddings"""

    # Test RSS feeds (using popular tech news sites)
    test_feeds = [
        "https://www.theverge.com/rss/index.xml",
        "https://feeds.feedburner.com/TechCrunch/",
        "https://www.wired.com/feed/rss"
    ]

    # Create test project and package IDs
    test_project_id = str(ObjectId())
    test_package_id = str(ObjectId())

    try:
        # Initialize the database connection
        db = ProtonDB()

        # Initialize the scraping package with embeddings enabled
        package = ScrapingPackage(
            package_id=test_package_id,
            project_ids=[test_project_id],
            name="Tech News Test Package",
            description="Test package for tech news from The Verge, TechCrunch, and Wired",
            rss_feeds=test_feeds,
            schedule_interval="1h",
            max_articles_per_run=10,  # Limit to 10 articles
            calculate_embeddings=True  # Enable embeddings
        )

        logger.info("=" * 50)
        logger.info("STARTING SCRAPING TEST")
        logger.info("=" * 50)

        # Run the package
        start_time = time.time()
        article_ids = package.run()
        duration = time.time() - start_time

        logger.info("=" * 50)
        logger.info(f"SCRAPING COMPLETED in {duration:.2f} seconds")
        logger.info(f"Processed {len(article_ids)} articles")
        logger.info("=" * 50)

        # Verify the results
        if article_ids:
            logger.info("Verifying article data...")

            # Check a few articles to verify NLP and embeddings
            for i, article_id in enumerate(article_ids[:3]):  # Check up to 3 articles
                # Query the article directly from MongoDB
                article = db.db.articles.find_one({"_id": ObjectId(article_id)})

                if article:
                    logger.info(f"\nARTICLE {i+1}:")
                    logger.info(f"Title: {article.get('title')}")
                    logger.info(f"Source: {article.get('source_name')}")

                    # Check keywords (NLP)
                    keywords = article.get('keywords', [])
                    logger.info(f"Keywords ({len(keywords)}): {', '.join(keywords[:10])}")

                    # Check summary (NLP)
                    summary = article.get('summary', '')
                    logger.info(f"Summary: {summary[:150]}...")

                    # Check vector embeddings
                    has_embeddings = 'vector_embedding' in article
                    embedding_model = article.get('embedding_model', 'unknown')

                    if has_embeddings:
                        embedding = article.get('vector_embedding', [])
                        logger.info(f"Embeddings: {embedding_model} model, {len(embedding)} dimensions")
                        logger.info(f"First 5 values: {embedding[:5]}")
                    else:
                        logger.warning("No vector embeddings found!")

                    logger.info("-" * 40)

            logger.info("\nTEST SUMMARY:")
            logger.info(f"Total articles: {len(article_ids)}")

            # Count articles with embeddings
            articles_with_embeddings = 0
            embedding_models_used = set()

            for article_id in article_ids:
                article = db.db.articles.find_one({"_id": ObjectId(article_id)})
                if article and 'vector_embedding' in article:
                    articles_with_embeddings += 1
                    embedding_models_used.add(article.get('embedding_model', 'unknown'))

            logger.info(f"Articles with embeddings: {articles_with_embeddings}/{len(article_ids)}")
            logger.info(f"Embedding models used: {', '.join(embedding_models_used)}")

            if articles_with_embeddings == len(article_ids):
                logger.info("‚úÖ SUCCESS: All articles have embeddings")
            else:
                logger.warning(f"‚ö†Ô∏è WARNING: {len(article_ids) - articles_with_embeddings} articles missing embeddings")

        else:
            logger.error("‚ùå ERROR: No articles were processed!")

    except Exception as e:
        logger.error(f"Error in test: {e}")
        logger.error(traceback.format_exc())
    finally:
        # Clean up
        if 'db' in locals():
            db.close()

if __name__ == "__main__":
    test_scraping_with_embeddings()
</file>

<file path="xx/test_streaming.py">
"""
Test Streaming App

A minimal app to test streaming with line breaks.
"""

import time
import queue
import threading
from flask import Flask, Response, render_template

app = Flask(__name__)
stream_queue = queue.Queue()

@app.route('/')
def index():
    return render_template('test_streaming.html')

@app.route('/stream')
def stream():
    def generate():
        # Send some test messages with explicit line breaks
        stream_queue.put("Line 1: This is a test\n")
        time.sleep(0.5)
        stream_queue.put("Line 2: With line breaks\n")
        time.sleep(0.5)
        stream_queue.put("Line 3: Testing\n")
        time.sleep(0.5)
        stream_queue.put("\n") # Empty line
        time.sleep(0.5)
        stream_queue.put("Line 4: After empty line\n")
        time.sleep(0.5)
        stream_queue.put("Line 5: Final line\n")
        time.sleep(0.5)
        stream_queue.put("[DONE]")
        
        # Stream from the queue
        while True:
            try:
                message = stream_queue.get(timeout=1.0)
                if message == "[DONE]":
                    yield "data: [DONE]\n\n"
                    break
                else:
                    # Explicitly encode newlines for SSE
                    message_sse = message.replace("\n", "\\n")
                    yield f"data: {message_sse}\n\n"
            except queue.Empty:
                yield "data: \n\n"  # Keep-alive
                continue
            except Exception as e:
                print(f"Error: {e}")
                yield f"data: Error: {str(e)}\n\n"
                yield "data: [DONE]\n\n"
                break
    
    return Response(generate(), mimetype='text/event-stream')

if __name__ == '__main__':
    app.run(debug=True, port=5004)
</file>

<file path="xx/update_embeddings.py">
"""
Update Article Embeddings Script for Proton CRM

This script updates the vector embeddings for existing articles in the database.
It can convert from local SentenceTransformer embeddings to OpenAI embeddings.
"""

import os
import sys
import logging
import argparse
from tqdm import tqdm
import datetime
from typing import List, Dict, Any

# Import MongoDB connection
from proton_db_setup import ProtonDB

# Import embedding generator
from vector_embeddings import EmbeddingGenerator

# Configure logging with both file and console handlers
def setup_logging():
    # Create logs directory if it doesn't exist
    os.makedirs('logs', exist_ok=True)
    
    # Create a timestamp for the log file
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = f'logs/embedding_updates_{timestamp}.log'
    
    # Configure logging
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    
    # Add handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()

def update_all_embeddings(embedding_type: str = "openai", batch_size: int = 10):
    """
    Update embeddings for all articles in the database
    
    Args:
        embedding_type: Type of embedding to use ('openai' or 'local')
        batch_size: Size of batches to process at once
    """
    db = ProtonDB()
    try:
        # Get all articles
        articles = list(db.db.articles.find({}))
        
        if not articles:
            logger.info("No articles found in database")
            return
        
        logger.info(f"Found {len(articles)} articles to update")
        logger.info(f"Using embedding type: {embedding_type}")
        
        # Initialize embedding generator
        embedder = EmbeddingGenerator(model_type=embedding_type)
        
        # Set batch size based on embedding type (OpenAI has rate limits)
        effective_batch_size = min(5, batch_size) if embedding_type == "openai" else batch_size
        
        # Process in batches
        updated_count = 0
        failed_count = 0
        
        for i in range(0, len(articles), effective_batch_size):
            batch = articles[i:i + effective_batch_size]
            batch_ids = [article["_id"] for article in batch]
            
            # Prepare texts for embedding
            batch_texts = []
            for article in batch:
                # Combine relevant fields for embedding
                text = f"{article.get('title', '')}. "
                text += f"{article.get('scraping_package_name', '')}. "
                text += article.get('content', '')
                batch_texts.append(text)
            
            # Generate embeddings
            try:
                logger.info(f"\nProcessing batch of {len(batch_texts)} articles")
                embeddings = embedder.batch_get_embeddings(batch_texts, batch_size=effective_batch_size)
                
                # Update each article
                for j, (article_id, embedding) in enumerate(zip(batch_ids, embeddings)):
                    try:
                        # Log before update
                        logger.info(f"\nUpdating article: {article_id}")
                        logger.info(f"Title: {batch[j].get('title', 'No title')}")
                        logger.info(f"Package: {batch[j].get('scraping_package_name', 'No package')}")
                        logger.info(f"Embedding size: {len(embedding)}")
                        logger.info(f"First 5 embedding values: {embedding[:5]}")
                        
                        # Perform update
                        result = db.db.articles.update_one(
                            {"_id": article_id},
                            {
                                "$set": {
                                    "vector_embedding": embedding,
                                    "embedding_model": embedding_type,
                                    "embedding_updated": datetime.datetime.utcnow()
                                }
                            }
                        )
                        
                        # Log after update
                        if result.modified_count > 0:
                            logger.info("‚úì Successfully updated embedding")
                        else:
                            logger.warning("‚ö† Document matched but not modified")
                            
                        updated_count += 1
                        
                    except Exception as e:
                        logger.error(f"‚ùå Error updating article {article_id}: {e}")
                        failed_count += 1
                
                logger.info(f"\nProgress: {i + len(batch)}/{len(articles)} articles")
                
            except Exception as e:
                logger.error(f"‚ùå Error generating embeddings for batch: {e}")
                failed_count += len(batch_texts)
        
        logger.info(f"\nEmbedding update complete:")
        logger.info(f"Total articles processed: {len(articles)}")
        logger.info(f"Successfully updated: {updated_count}")
        logger.info(f"Failed: {failed_count}")
        logger.info(f"Log file: {logger.handlers[0].baseFilename}")
        
    except Exception as e:
        logger.error(f"‚ùå Error updating embeddings: {e}")
    finally:
        db.close()

def update_package_embeddings(package_id: str, embedding_type: str = "openai", batch_size: int = 10):
    """
    Update embeddings for articles from a specific package
    
    Args:
        package_id: ID of the package to update articles for
        embedding_type: Type of embedding to use ('openai' or 'local')
        batch_size: Size of batches to process at once
    """
    from bson.objectid import ObjectId
    
    db = ProtonDB()
    try:
        # Get articles for this package
        articles = list(db.db.articles.find({"scraping_package_id": ObjectId(package_id)}))
        
        if not articles:
            logger.info(f"No articles found for package {package_id}")
            return
        
        logger.info(f"Found {len(articles)} articles to update for package {package_id}")
        logger.info(f"Using embedding type: {embedding_type}")
        
        # Initialize embedding generator
        embedder = EmbeddingGenerator(model_type=embedding_type)
        
        # Set batch size based on embedding type (OpenAI has rate limits)
        effective_batch_size = min(5, batch_size) if embedding_type == "openai" else batch_size
        
        # Process in batches
        updated_count = 0
        failed_count = 0
        
        for i in range(0, len(articles), effective_batch_size):
            batch = articles[i:i + effective_batch_size]
            batch_ids = [article["_id"] for article in batch]
            batch_texts = []
            
            for article in batch:
                text = f"{article.get('title', '')}. "
                text += f"{article.get('scraping_package_name', '')}. "
                text += article.get('content', '')
                batch_texts.append(text)
            
            try:
                logger.info(f"\nProcessing batch of {len(batch_texts)} articles")
                embeddings = embedder.batch_get_embeddings(batch_texts, batch_size=effective_batch_size)
                
                for j, (article_id, embedding) in enumerate(zip(batch_ids, embeddings)):
                    try:
                        # Log before update
                        logger.info(f"\nUpdating article: {article_id}")
                        logger.info(f"Title: {batch[j].get('title', 'No title')}")
                        logger.info(f"Package: {batch[j].get('scraping_package_name', 'No package')}")
                        logger.info(f"Embedding size: {len(embedding)}")
                        logger.info(f"First 5 embedding values: {embedding[:5]}")
                        
                        result = db.db.articles.update_one(
                            {"_id": article_id},
                            {
                                "$set": {
                                    "vector_embedding": embedding,
                                    "embedding_model": embedding_type,
                                    "embedding_updated": datetime.datetime.utcnow()
                                }
                            }
                        )
                        
                        if result.modified_count > 0:
                            logger.info("‚úì Successfully updated embedding")
                        else:
                            logger.warning("‚ö† Document matched but not modified")
                            
                        updated_count += 1
                        
                    except Exception as e:
                        logger.error(f"‚ùå Error updating article {article_id}: {e}")
                        failed_count += 1
                
            except Exception as e:
                logger.error(f"‚ùå Error generating embeddings for batch: {e}")
                failed_count += len(batch_texts)
        
        # Update the package to indicate embeddings were updated
        db.db.scraping_packages.update_one(
            {"_id": ObjectId(package_id)},
            {
                "$set": {
                    "embeddings_updated": datetime.datetime.utcnow(),
                    "embedding_model": embedding_type
                }
            }
        )
        
        logger.info(f"\nEmbedding update complete for package {package_id}:")
        logger.info(f"Total articles processed: {len(articles)}")
        logger.info(f"Successfully updated: {updated_count}")
        logger.info(f"Failed: {failed_count}")
        logger.info(f"Log file: {logger.handlers[0].baseFilename}")
        
    except Exception as e:
        logger.error(f"‚ùå Error updating embeddings for package {package_id}: {e}")
    finally:
        db.close()

def main():
    """Main function to parse arguments and run the appropriate command"""
    parser = argparse.ArgumentParser(description="Update vector embeddings for articles in the Proton CRM database")
    parser.add_argument("--all", action="store_true", help="Update embeddings for all articles")
    parser.add_argument("--package", type=str, help="Update embeddings for articles from a specific package")
    parser.add_argument("--embedding-type", type=str, choices=["openai", "local"], default="openai",
                        help="Type of embedding to use (default: openai)")
    parser.add_argument("--batch-size", type=int, default=10, help="Batch size for processing (default: 10)")
    
    args = parser.parse_args()
    
    if args.all:
        update_all_embeddings(embedding_type=args.embedding_type, batch_size=args.batch_size)
    elif args.package:
        update_package_embeddings(args.package, embedding_type=args.embedding_type, batch_size=args.batch_size)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
</file>

<file path="xx/view_articles.py">
"""
Simple script to view articles in the database
"""

from proton_db_setup import ProtonDB
from bson.objectid import ObjectId
import sys

def main():
    # Connect to the database
    db = ProtonDB()
    
    try:
        # Get all articles
        articles = list(db.db.articles.find())
        print(f"Found {len(articles)} articles in the database")
        
        # Display article details
        for idx, article in enumerate(articles, 1):
            title = article.get('title', 'No title')
            relevance = article.get('relevance_scores', {}).get('overall', 0)
            package_fit = article.get('relevance_scores', {}).get('package_fit', 0)
            recency = article.get('relevance_scores', {}).get('recency', 0)
            keywords = article.get('keywords', [])
            source = article.get('source_name', 'Unknown')
            date = article.get('published_date', 'Unknown date')
            date_str = date.strftime("%Y-%m-%d") if hasattr(date, 'strftime') else date
            package_name = article.get('scraping_package_name', 'No package')
            
            print(f"\n{idx}. {title}")
            print(f"   Source: {source} | Date: {date_str} | Package: {package_name}")
            print(f"   Relevance: {relevance:.2f} (Package fit: {package_fit:.2f}, Recency: {recency:.2f})")
            print(f"   Keywords: {', '.join(keywords[:10])}")

        # Check if package ID was provided as argument
        if len(sys.argv) > 1:
            package_id = sys.argv[1]
            try:
                # Filter articles by package ID
                package_articles = list(db.db.articles.find({"scraping_package_id": ObjectId(package_id)}))
                print(f"\n\nArticles from package {package_id}: {len(package_articles)}")
                
                # Get package name from first article
                if package_articles:
                    package_name = package_articles[0].get('scraping_package_name', 'Unknown package')
                    print(f"Package name: {package_name}\n")
                
                # Display article titles and relevance scores
                for idx, article in enumerate(package_articles, 1):
                    title = article.get('title', 'No title')
                    relevance = article.get('relevance_scores', {}).get('overall', 0)
                    package_fit = article.get('relevance_scores', {}).get('package_fit', 0)
                    
                    print(f"{idx}. {title} - Relevance: {relevance:.2f} (Package fit: {package_fit:.2f})")
            except Exception as e:
                print(f"Error filtering by package ID: {e}")
    
    except Exception as e:
        print(f"Error retrieving articles: {e}")
    finally:
        # Close the database connection
        db.close()
        print("\nDatabase connection closed")

if __name__ == "__main__":
    main()
</file>

<file path="xx/wsgi.py">
"""
WSGI entry point for Render deployment
"""
import os
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create a simple Flask app without NLTK dependencies
from flask import Flask

# Create a placeholder app - will be replaced with the real one
app = Flask(__name__)

try:
    # Try to import the real app, but only if running directly
    # This avoids import errors during build process
    if __name__ == "__main__":
        from article_viewer import app as real_app
        app = real_app
except ImportError as e:
    logger.error(f"Error importing app: {e}")
    
    @app.route('/')
    def home():
        return "Application is starting up. Please check logs for errors."

# This is for Render deployment
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 10000)))
</file>

<file path="Dockerfile">
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create uploads directory
RUN mkdir -p uploads

# Expose port
EXPOSE 5006

# Set environment variables
ENV PYTHONUNBUFFERED=1

# Run the application
CMD ["gunicorn", "--bind", "0.0.0.0:5006", "final_app:app"]
</file>

<file path="PROTON - Content curation and delivery system.md">
# **PROTON** {#proton}

## **AI-powered content curation and delivery system** {#ai-powered-content-curation-and-delivery-system}

Proton CRM Product Specification Document  
Date: 20.02.2025  
Project name: Proton CRM Product Specification Document  
Client: Proto

[**PROTON	1**](#proton)

[AI-powered content curation and delivery system	1](#ai-powered-content-curation-and-delivery-system)

[Introduction	3](#introduction)

[Purpose	3](#purpose)

[Scope	4](#scope)

[Target audience	4](#target-audience)

[Overview	4](#overview)

[Core Pages	4](#core-pages)

[Login	4](#login)

[Home	4](#home)

[Projects	4](#projects)

[Single project	4](#single-project)

[Project document archive	5](#project-document-archive)

[Project newsletter settings	5](#project-newsletter-settings)

[Persona & Recipient Management	5](#persona-&-recipient-management)

[Edit persona	5](#edit-persona)

[Scraping packages archive	5](#scraping-packages-archive)

[Scraping package configuration	5](#scraping-package-configuration)

[Global settings	5](#global-settings)

[Core Modules	6](#core-modules)

[Users	6](#users)

[Search Bar	6](#search-bar)

[Login page	6](#login-page)

[Authentication	6](#authentication)

[Credential Fields	6](#credential-fields)

[Actions & Options	6](#actions- &-options)

[Error Handling	7](#error-handling)

[Styling & UX	7](#styling- &-ux)

[Security Considerations	7](#security-considerations)

[Post-Login Flow	7](#post-login-flow)

[Forgot password process	7](#forgot-password-process)

[Home Page	7](#home-page)

[Header & Navigation	7](#header- &-navigation)

[Search bar	8](#search-bar-1)

[Purpose	8](#purpose-1)

[UI Behavior	8](#ui-behavior)

[Key Metrics & Stats	9](#key-metrics- &-stats)

[Recent Projects Section	10](#recent-projects-section)

[Recent Activity Feed	11](#recent-activity-feed)

[Projects page	12](#projects-page)

[Purpose	12](#purpose-2)

[UI Layout & Components	12](#ui-layout- &-components)

[Key Functionalities	13](#key-functionalities)

[Data Model & Fields	13](#data-model- &-fields)

[Error Handling & Validation	14](#error-handling- &-validation)

[Security & Permissions	15](#security- &-permissions)

[Future Enhancements	15](#future-enhancements)

[Single Project page	15](#single-project-page)

[Purpose	15](#purpose-3)

[UI Layout & Sections	16](#ui-layout- &-sections)

[Key Functionalities	17](#key-functionalities-1)

[Data Model	17](#data-model)

[Security & Permissions	19](#security- &-permissions-1)

[Validations & Error Handling	19](#validations- &-error-handling)

[UI Reference	19](#ui-reference)

[Future Enhancements	19](#future-enhancements-1)

## **UI Components and Interactions** {#ui-components-and-interactions}

### **Common Components** {#common-components}

#### **Modals and Dialogs** {#modals-and-dialogs}

##### **Create/Edit Project Modal**
- **Purpose**: Create new projects or edit existing ones
- **Fields**:
  - Project Name (required)
  - Description (required)
  - Status (Active/Archived)
- **Actions**:
  - Save
  - Cancel
- **Validation**:
  - Name must be unique
  - Required fields must be filled
  - Description max length: 500 characters

##### **Upload Document Modal**
- **Purpose**: Upload new documents to a project
- **Features**:
  - Drag and drop support
  - File type validation
  - Size limit (10MB)
  - Progress indicator
- **Actions**:
  - Upload
  - Cancel
- **Validation**:
  - File size check
  - File type check
  - Duplicate name check

##### **Add/Edit Recipient Modal**
- **Purpose**: Add new recipients or edit existing ones
- **Fields**:
  - Name (required)
  - Email (required)
  - Persona (required)
  - Status (Active/Inactive)
- **Actions**:
  - Save
  - Cancel
- **Validation**:
  - Email format
  - Required fields
  - Unique email per project

##### **Configure Scraping Package Modal**
- **Purpose**: Set up or modify scraping packages
- **Fields**:
  - Package Name (required)
  - Description (required)
  - Schedule (required)
  - Status (Active/Inactive)
- **Actions**:
  - Save
  - Test Run
  - Cancel
- **Validation**:
  - Required fields
  - Valid schedule format
  - Unique name

#### **Tables and Lists** {#tables-and-lists}

##### **Projects Table**
- **Columns**:
  - Name
  - Description
  - Status
  - Last Updated
  - Actions
- **Features**:
  - Sortable columns
  - Pagination
  - Search/filter
  - Bulk actions
- **Row Actions**:
  - Edit
  - Archive
  - View Details

##### **Documents List**
- **Display Mode**:
  - Grid view
  - List view
- **Item Information**:
  - File name
  - Type
  - Size
  - Upload date
  - Actions
- **Features**:
  - Sort by date
  - Filter by type
  - Search
- **Item Actions**:
  - Download
  - Delete
  - Preview

##### **Recipients Table**
- **Columns**:
  - Name
  - Email
  - Persona
  - Status
  - Actions
- **Features**:
  - Sortable columns
  - Filter by status
  - Search
  - Bulk actions
- **Row Actions**:
  - Edit
  - Remove
  - Change Status

#### **Forms and Inputs** {#forms-and-inputs}

##### **Newsletter Configuration Form**
- **Fields**:
  - Subject Line
  - Schedule
  - Recipients Selection
  - Content Template
- **Features**:
  - Rich text editor
  - Template selection
  - Preview
  - Schedule picker
- **Validation**:
  - Required fields
  - Valid date/time
  - At least one recipient

##### **Persona Configuration Form**
- **Fields**:
  - Name
  - Description
  - Tone Settings
  - Content Preferences
- **Features**:
  - Tone selector
  - Keyword input
  - Content type selection
- **Validation**:
  - Required fields
  - Valid tone selection
  - Valid preferences

#### **Navigation Components** {#navigation-components}

##### **Main Navigation**
- **Items**:
  - Home
  - Projects
  - Scraping Packages
  - Settings
- **Features**:
  - Active state indication
  - Collapsible on mobile
  - Quick actions menu

##### **Project Navigation**
- **Items**:
  - Overview
  - Documents
  - Recipients
  - Newsletters
  - Settings
- **Features**:
  - Breadcrumb navigation
  - Tab-based navigation
  - Quick actions

#### **Status Indicators** {#status-indicators}

##### **Project Status**
- **States**:
  - Active (green)
  - Archived (gray)
  - Pending (yellow)
- **Features**:
  - Color coding
  - Status text
  - Hover tooltip

##### **Newsletter Status**
- **States**:
  - Scheduled (blue)
  - Sent (green)
  - Failed (red)
  - Draft (gray)
- **Features**:
  - Color coding
  - Status text
  - Progress indicator

##### **Recipient Status**
- **States**:
  - Active (green)
  - Inactive (gray)
  - Unsubscribed (red)
- **Features**:
  - Color coding
  - Status text
  - Last activity date

### **Interactive Features** {#interactive-features}

#### **Drag and Drop** {#drag-and-drop}
- **Supported Areas**:
  - Document upload
  - Recipient reordering
  - Project card reordering
- **Visual Feedback**:
  - Drop zone highlighting
  - Drag preview
  - Success/error animations

#### **Real-time Updates** {#real-time-updates}
- **Features**:
  - Live status changes
  - Progress indicators
  - Toast notifications
- **Update Types**:
  - Document upload progress
  - Newsletter sending status
  - Scraping job status

#### **Search and Filter** {#search-and-filter}
- **Global Search**:
  - Projects
  - Documents
  - Recipients
- **Advanced Filters**:
  - Date range
  - Status
  - Type
  - Persona

#### **Bulk Actions** {#bulk-actions}
- **Supported Operations**:
  - Delete multiple items
  - Update status
  - Export data
- **UI Elements**:
  - Checkbox selection
  - Action toolbar
  - Confirmation dialog

### **Responsive Design** {#responsive-design}

#### **Breakpoints** {#breakpoints}
- **Mobile**: < 640px
- **Tablet**: 640px - 1024px
- **Desktop**: > 1024px

#### **Mobile Adaptations** {#mobile-adaptations}
- **Navigation**:
  - Hamburger menu
  - Bottom navigation
  - Swipe gestures
- **Tables**:
  - Card view
  - Horizontal scroll
  - Expandable rows
- **Forms**:
  - Full-width inputs
  - Stacked layout
  - Touch-friendly controls

#### **Tablet Optimizations** {#tablet-optimizations}
- **Layout**:
  - Split view
  - Side-by-side panels
  - Adaptive grid
- **Navigation**:
  - Collapsible sidebar
  - Tab navigation
  - Quick actions menu

### **Accessibility** {#accessibility}

#### **Keyboard Navigation** {#keyboard-navigation}
- **Focus Management**:
  - Logical tab order
  - Focus indicators
  - Skip links
- **Shortcuts**:
  - Global actions
  - Navigation
  - Common operations

#### **Screen Reader Support** {#screen-reader-support}
- **ARIA Labels**:
  - Interactive elements
  - Status messages
  - Form controls
- **Semantic HTML**:
  - Proper heading structure
  - Landmark regions
  - Live regions

#### **Color and Contrast** {#color-and-contrast}
- **Color Usage**:
  - Status indicators
  - Interactive elements
  - Text hierarchy
- **Contrast Ratios**:
  - Text: 4.5:1
  - Large text: 3:1
  - UI components: 3:1

### **Error Handling** {#error-handling}

#### **Form Validation** {#form-validation}
- **Real-time Validation**:
  - Field-level feedback
  - Error messages
  - Success states
- **Submission Validation**:
  - Form-level errors
  - Required fields
  - Format validation

#### **API Error Handling** {#api-error-handling}
- **Error States**:
  - Network errors
  - Validation errors
  - Server errors
- **User Feedback**:
  - Error messages
  - Retry options
  - Fallback content

#### **Offline Support** {#offline-support}
- **Features**:
  - Offline indicators
  - Data persistence
  - Sync status
- **User Experience**:
  - Clear messaging
  - Recovery options
  - Progress tracking

## **Introduction** {#introduction}

### **Purpose** {#purpose}

The purpose of this document is to provide a comprehensive technical specification for the Proton AI-powered content curation and delivery system. This system is designed to enhance the ongoing engagement of consulting clients by providing tailored content post-project completion, utilizing advanced AI capabilities integrated within a robust CRM interface.

### **Scope** {#scope}

Proton is an AI-powered content curation and delivery system with comprehensive CRM capabilities. Its purpose is to keep past consulting clients informed and engaged by automating the delivery of relevant, industry-specific content, and by managing client interactions and enabling content customization.

### **Target audience** {#target-audience}

This CRM is intended for Proto's internal teams who oversee content curation, client project engagements, and the creation of tailored personas for newsletters and other outreach. By centralizing these tasks, the system streamlines document uploads, scraping configurations, and persona-based content delivery, ensuring each client receives insights customized to their needs and roles

### **Overview** {#overview}

The document will detail the functionalities intended for the Proton CRM system, describe the interactions users will have with the platform, and outline the technical infrastructure required to support these operations .This document outlines the scope of work required for the CRM development team.

## **Core Pages** {#core-pages}

### **Login** {#login}

* The admin login page has a clean design with fields for email and password, It can contain security features like CAPTCHA.  
* UI Reference: Missing

### **Home** {#home}

* Dashboard view provides an overview of projects, recipients, packages, and newsletters, with detailed information about recent project activity.  
* UI Reference: "Home" screen in [wireframes](https://www.figma.com/design/O56Uaw3NQQvS2wLZK1n0Dy/Proton-Wireframes-0213?node-id=0-1&p=f&t=8jgp2AXxUuAbwcNU-0).

### **Projects** {#projects}

* A project archive that allows you to create new projects with summarized project information.  
* UI Reference: Missing.

### **Single project** {#single-project}

* Gives detailed information of a single project. Has the ability to edit a project, view project dashboards, view metrics (documents, scraping packages, recipients, next newsletter date).  
* UI Reference: "Projects" screen in [wireframes](https://www.figma.com/design/O56Uaw3NQQvS2wLZK1n0Dy/Proton-Wireframes-0213?node-id=0-1&p=f&t=8jgp2AXxUuAbwcNU-0).

#### **Project document archive** {#project-document-archive}

* Provides an overview of all uploaded documents and enables file management, including uploading, downloading, and deleting files.  
* UI Reference: Missing

#### **Project newsletter settings** {#project-newsletter-settings}

* Displays the newsletter settings for current projects and allows users to edit and save them.  
* UI Reference: Missing

#### **Persona & Recipient Management** {#persona- &-recipient-management}

* Displays an archive of personas and recipients with the ability to create, edit and delete them.  
* UI Reference: "Persona & Recipient" screen in [wireframes](https://www.figma.com/design/O56Uaw3NQQvS2wLZK1n0Dy/Proton-Wireframes-0213?node-id=0-1&p=f&t=8jgp2AXxUuAbwcNU-0).

#### **Edit persona** {#edit-persona}

* Allows for the modification of persona information, tone & prompt preferences, and the uploading of documents and packages.  
* UI Reference: "Edit persona" screen in [wireframes](https://www.figma.com/design/O56Uaw3NQQvS2wLZK1n0Dy/Proton-Wireframes-0213?node-id=0-1&p=f&t=8jgp2AXxUuAbwcNU-0).

### **Scraping packages archive** {#scraping-packages-archive}

* Presents a repository of Scraping packages that can be edited, activated and deactivated.  
* UI Reference: Missing.

### **Scraping package configuration** {#scraping-package-configuration}

* Enables users to set up and manage automated data ingestion sources for a specific target audience.  
* UI Reference: Missing.

### **Global settings** {#global-settings}

* Modify organisation name, time zone, AI module selection, SMTP configuration, and security and privacy settings in General settings.  
* UI Reference: "Global settings" screen in [wireframes](https://www.figma.com/design/O56Uaw3NQQvS2wLZK1n0Dy/Proton-Wireframes-0213?node-id=0-1&p=f&t=8jgp2AXxUuAbwcNU-0).

## **Core Modules** {#core-modules}

### **Users** {#users}

* The user is typically an internal staff member with assigned permissions (e.g., Admin, Editor) who manages project-related tasks such as uploading documents, configuring scraping packages, and scheduling newsletters. They aim to keep each project's data organized and ensure smooth delivery of relevant updates to various recipients.  
* UI Reference: "Home" screen in [wireframes](https://www.figma.com/design/O56Uaw3NQQvS2wLZK1n0Dy/Proton-Wireframes-0213?node-id=0-1&p=f&t=8jgp2AXxUuAbwcNU-0), located in the header. A profile page is missing.


### **Search Bar** {#search-bar}

* **Search bar:** A search box with auto-suggestions to find projects, documents, or recipients.

## **Login page** {#login-page}

### **Authentication** {#authentication}

* The CEM login page is the entry page where users are prompted to enter their login credentials (email or username and password). It includes a "Remember Me" checkbox and a link to reset passwords. Upon successful login, users are directed to the home page dashboard.

### **Credential Fields** {#credential-fields}

* **Username or Email Field**: A text field for the user's unique identifier.  
* **Password Field**: An obscured input for password entry.

### **Actions & Options** {#actions- &-options}

* **Login Button**: Submits the credentials to the authentication service.  
* **Remember Me** (optional): A checkbox so users can remain logged in for a period of 10 days.  
* **Forgot Password** Link: Initiates a password reset flow (e.g., email-based recovery).

### **Error Handling** {#error-handling}

* If the user enters invalid credentials, an error message (e.g., "Incorrect username or password") is displayed in the modal.  
* Security best practices dictate limiting the number of login attempts to 4 times and including a CAPTCHA if unusual activity is detected.

### **Styling & UX** {#styling- &-ux}

* A clean, minimal layout with clear labels and spacing.  
* Prominent call-to-action (Login Button) to reduce user confusion.  
* Branding elements (logo, color scheme) for a consistent user experience.

### **Security Considerations** {#security-considerations}

* **HTTPS/TLS** enforced to protect credentials in transit.

### **Post-Login Flow** {#post-login-flow}

* On successful login, users are redirected to the **CEM** home page dashboard, where they can see their projects, data summaries, and navigation menus.

### **Forgot password process** {#forgot-password-process}

* **User Trigger:** The user clicks the "Forgot Password" link within the login modal.  
* **Email Prompt:** A secondary prompt appears, asking the user to enter their registered email address.  
* **Secure Reset Link:** An email is sent containing a unique, time-limited link.  
* **Password Reset Form:** Clicking the link directs the user to a page where they can enter and confirm a new password.  
* **Confirmation:** After submission, the system updates their credentials, and the user can log in with the newly set password. The user will be provided with a login link.

## **Home Page** {#home-page}

### **Header & Navigation** {#header- &-navigation}

* **Branding/Logo:** Appears at the top-left, identifying the CEM platform.  
* **Primary Navigation Links:**  
  * **Home:** Returns to this main dashboard.  
  * **Projects:** View and manage all active or archived projects.  
  * **Scraping Packages:** Configure and manage data-gathering modules.  
  * **Global Settings:** Adjust organization-wide preferences (e.g., time zone, email server, default AI settings).  
* **User Profile:** The user profile picture is displayed at the top-right. The signout link appears when hovering over the user profile picture.  
* **Search bar:** An interactive input field that sends queries to the backend for matching projects, documents, or recipients, supporting real-time suggestions (e.g., auto-complete or typeahead) to help users quickly locate specific items

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| brandLogo | String (URL) | The image or icon URL used to display the platform's logo in the header. |
| brandName | String | A textual label (e.g., "CEM Dashboard"), typically adjacent to the logo. |
| navLinks | Array of Objects | A collection of navigation items displayed in the header; each object includes a name, icon, and target route. |
| userName | String | The logged-in user's display name. |
| userRole | String | The role of the currently authenticated user (Admin or Editor), used for conditional rendering of links. |
| logoutLink | String (URL/Route) | The endpoint or route used to log out. |
| searchBar | Object / String (UI) | Configuration and placeholder text for the search field, enabling quick lookup of projects, documents, or recipients. |

### **Search bar** {#search-bar-1}

### **Purpose** {#purpose-1}

* The search bar allows users to type queries and receive **on-the-fly** suggestions (e.g., matching project names or document titles).  
* No separate results page is opened; suggestions appear in a dropdown beneath the input field.

### **UI Behavior** {#ui-behavior}

* **Autocomplete Dropdown:**  
  * As the user types (e.g., each key press), the frontend triggers a debounce (to limit requests) and then sends a query to retrieve matching items.  
  * A list of suggestions appears below the search field, showing a limited set (e.g., top 5\) of relevant matches.  
  * Clicking on a suggestion (or pressing Enter on a highlighted one) navigates the user directly to that item's detail page (if applicable) or fills the input with the chosen suggestion.  
* **Styling & Accessibility:**  
  * Dropdown remains visible until the user clicks away or clears the input.  
  * Keyboard navigation (up/down arrows) cycles through suggestions, pressing Enter selects one.

### 

### **Key Metrics & Stats** {#key-metrics- &-stats}

* **Total Projects:** Displays the number of active projects in the system.  
* **Total Recipients:** Displays the total number of newsletter subscribers across all projects.  
* **Total Documents:** Displays the total number of documents across all projects.  
* **Total Packages:** Displays the total number of scraping packages are configured across all projects.  
* **Newsletters Sent:** Tracks the cumulative number of newsletters delivered.  
* **Newsletter opened:** Tracks Tracks the cumulative number of newsletters opened in mail clients (e.g., Gmail or Outlook).

For the summary dashboard this data will be needed:

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| totalProjects | Number | Total count of active (non-archived) projects in the system |
| totalProjectsIcon | String (URL) | The icon URL used to display the project's icon in the relevant item. |
| totalRecipients | Number | Total number of recipients across all projects |
| totalRecipientsIcom | String (URL) | The icon URL used to display the recipients icon in the relevant item. |
| totalDocuments | Number | Sum of all uploaded files stored for all projects |
| totalDocumentsIcon | String (URL) | The icon URL used to display the documents icon in the relevant item. |
| totalPackages | Number | Count of all configured scraping packages |
| totalPackagesIcon | String (URL) | The icon URL used to display the Packages icon in the relevant item. |
| newslettersSent | Number | Cumulative number of newsletters dispatched to date |
| newslettersSentIcon | String (URL) | The icon URL used to display the newsletters icon in the relevant item. |

### **Recent Projects Section** {#recent-projects-section}

* **Project Cards:** Displays a short list of the most recently updated or most relevant projects, each card showing a brief description, last update timestamp, number of documents, and the next scheduled newsletter.  
* **View All Projects Link:** A button or link that directs to the full Projects list for in-depth management.

The following data is required for the summary dashboard header:

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| sectionTitle | String | The heading displayed above the list of recent projects (e.g., "Recent Projects"). |
| viewAllProjectsButton | String (URL/Route) | The link or route that, when clicked, navigates to the full project list or Projects page (e.g., `/projects`). |

The following data is required for the Recent Projects dashboard card:

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| recentProjectsTitle | String | The heading displayed above the list of recent projects (e.g., "Recent Projects"). |
| viewAllProjectsButton | String (URL/Route) | The link or route that, when clicked, navigates to the full project list or Projects page (e.g., `/projects`). |
| projectId | String | A unique identifier (e.g., UUID) for the project. |
| projectName | String | The project's displayed name/title. |
| description | String | A brief summary of the project's purpose or focus. |
| lastUpdated | Date/DateTime | A timestamp that captures the most recent update to the project. |
| documentCount | Number | The number of documents associated with this project. |
| nextNewsletterDate | Date/DateTime | The scheduled date and/or time for the next newsletter send related to this project, if any. |

### **Recent Activity Feed** {#recent-activity-feed}

* **Activity Log:** Lists real-time updates on new activity of this type:  
  * Project additions, edit or delete  
  * Document uploads or delete.  
  * Recently sent newsletters.  
  * Persona additions, edit or delete.  
  * Scraping package configuration, activated, deactivated, edit or delete.  
* **Timestamp & Detail:** Each entry shows when the activity occurred and any relevant summary (e.g., "New document uploaded to 'Marketing Campaign 2025'").

The following data is required for the Recent Activity header:

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| recentActivityTitle | String | The heading shown above the list of recent activities (e.g., "Recent Activity"). |
| viewAllActivityButton | String (URL/Route) | A link or route that navigates to a more comprehensive activity log or audit trail page. |

The following data is required for the Recent Activity card:

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| activityType | String | The type of system event (e.g., "DocumentUpload," "NewsletterSent," "PersonaModified"). |
| activityTypeIcon | String (URL) | A path or class name for the icon displayed next to the activity (e.g., "icon-upload," "icon-envelope"). |
| projectName | String | The name of the project associated with the event. (If not project-related, this could be null or omitted.) |
| timestamp | Date/DateTime | A date/time indicating when the activity occurred. |
| details | String | A brief description or summary (e.g., "Newsletter sent to 1,234 recipients," "New document uploaded: Q1 Report"). |

## **Projects page** {#projects-page}

### **Purpose** {#purpose-2}

The **Projects Page** serves as an archive and management hub for all projects within the Proton CRM. From this page, authorized users can:

* **View** a list of all existing projects (both active and archived).  
* **Create** new projects.  
* **Edit** or **delete** existing projects.  
* **Activate** or **deactivate** a project.  
* **Access** each project's detailed view (the **Single Project** page).

### **UI Layout & Components** {#ui-layout- &-components}

* **Header & Navigation**  
  * Inherits the main header (logo, nav links, user profile) from the CRM layout.  
  * "Projects" is highlighted or selected in the primary navigation, indicating the current page.  
* **Projects Table / Grid**  
  * Presents each project in either a list or cards mode.  
  * Commonly shown columns/fields: Project Name, Description, Last Updated, Status (e.g., Active/Archived), and a quick action menu (Edit, Activate/deactivate, Archive or View).  
* **Create New Project Button**  
  * A button (e.g., "+ Create Project") that opens a module for creating a new project.  
  * Project creation contains the project's title and subtitle

**Pagination**

* **Display Conditions**  
  * If there are more than 50 projects in **List Mode**, pagination controls appear (e.g., "Page 1 of 5").  
  * If there are more than 25 projects in **Card Mode**, pagination controls appear under the grid.  
* **UI/UX Details**  
  * The pagination bar at the bottom of the project listings shows the current page and has arrows for navigation. The design is responsive, so on smaller screens page numbers might be replaced with a dropdown or Previous/Next buttons.

### **Key Functionalities** {#key-functionalities}

* **List All Projects**  
  * Displays basic project details at a glance.  
  * Allows sorting by date created, name, or last updated.  
* **Create Project**  
  * Opens a form or modal requesting the project's **name** and **description**.  
  * Edits are reflected immediately in the listing and in the single project view.  
* **Archive a Project**  
  * Offers an option to archive or remove a project entirely.  
  * Ensures confirmation dialogs to prevent accidental deletion.  
* **Navigate to Single Project**  
  * Clicking on a project in the list/card directs the user to the **Single Project** page, where they can view documents, manage scraping packages, configure newsletters, etc.  
* **Status Indicators**  
  * Each project can display a label or color-coded badge indicating whether it is active or archived.

### **Data Model & Fields** {#data-model- &-fields}

Below is a **data fields** table for rendering each project in the **Projects Page** list. It includes typical properties you might retrieve from the backend.

| Data Name | Data Type | Data Description | validation |
| :---- | :---- | :---- | :---- |
| ProjectId | String | A unique identifier (e.g., UUID) for the project. | \- **Generated** by the system (no user input). \- Must be unique across all projects. |
| dateCreated | Date/DateTime | When the project was initially created (auto-generated by system). | \- **Generated** by system on creation(no user input). |
| lastUpdated | Date/DateTime | Timestamp for the most recent update to the project's data or settings. | \- **Updated** automatically whenever the project record is modified. |
| status | String (Enum) | Indicates the project's state (e.g., "Active," "Archived") | \- **Generated** by system on creation(no user input). \- The default setting is Active. |
| projectName | String | The project's displayed name/title. | \- **Required** (cannot be empty). \- **Max length**: 100 characters (customizable). \- Must be unique among active projects, if that is a business requirement. |
| description | String | A brief summary of the project's purpose or focus. | \- **Required** (cannot be empty). \- **Max length**: 500 characters. |

### **Error Handling & Validation** {#error-handling- &-validation}

* **Unique Project Names**: The system should be able to detect duplicate names. If the suggested project name is already in use, the system should display an error message: "That project name is already taken. Please choose a different one."   
* **Mandatory Fields**: If the mandatory fields projectName and/or description are not completed, The system will generate an error message that reads "Please fill out this required field.".  
* **Permissions**: Certain roles (e.g., Admin, Editor) may have full access. Others might only create or modify projects

### **Security & Permissions** {#security- &-permissions}

* **Role-Based Access**:  
  * ***Admin*** can see and edit all projects.  
  * ***Editors*** can modify projects they own or are granted access to.  
* **Archived Projects**:  
  * Typically remain visible in the system but flagged as archived.  
  * May not appear in active project listings unless a "Show archived" filter is enabled.

### **Future Enhancements** {#future-enhancements}

* **Bulk Actions**: Archive or delete multiple projects simultaneously.  
* **Advanced Filters**: Filter by date range, owner, or tags.  
* **Analytics**: Include metrics such as open rate or newsletters sent for each project directly in the project list.

## **Single Project page** {#single-project-page}

### **Purpose** {#purpose-3}

The **Single Project Page** provides a detailed overview of one specific project, consolidating key metrics, configuration options, and quick access to related functionalities such as documents, scraping packages, recipients, and newsletter settings. It allows authorized users to **edit project details**, monitor relevant data, and perform project-specific actions.

---

### **UI Layout & Sections** {#ui-layout- &-sections}

* **Header & Project Overview**  
  * **Project Title & Description**: Displays the project name, a short summary, and an edit icon or button for modifying these details.  
  * **Key Metrics**: Quick stats:  
    * number of documents  
    * scheduled date for the next newsletter.  
    * total recipients  
    * Date of last update of these project parameter  
      * Project name  
      * Project description  
      * File uploaded or deleted  
      * Change to projects newsletter setup  
      * Change to projects recipients list  
      * Change to projects persona configuration  
* **Navigation Tabs**  
  * **Documents**: A link or tab to view and manage all uploaded documents for this project.  
  * **Scraping Packages**: A tab where users can see and configure scraping packages unique to this project.  
  * **Newsletter Settings**: A tab to edit newsletter frequency and dispatch timing.  
  * **Personas & Recipients**: A tab to manage personas, assign them to recipients, and add or   
* **Edit Project Modal or Drawer**  
  * Triggered by an "Edit" button within the **Project Overview** section.  
  * Allows updating **project name**, **description**, **status** (active/archived), and possibly advanced fields like tags or metadata.

### **Key Functionalities** {#key-functionalities-1}

* **View & Edit Project Details**  
  * Users can see the project name, description, and key stats at the top of the page.  
  * An "Edit" action lets authorized users update these fields.  
* **Quick Metrics**  
  * **Documents**: Count of how many files are currently uploaded.  
  * **Scraping Packages**: Number of active data-ingestion configurations.  
  * **Recipients**: Number of individuals subscribed under this project.  
  * **Next Newsletter Date**: Scheduled date/time of the next newsletter dispatch.  
* **Navigation to Sub-Pages**  
  * **Documents**: Lists all documents. Users can view, upload, or delete files here.  
  * **Scraping Packages**: Lists all scraping packages linked to this project with controls to toggle active/inactive or configure details.  
  * **Newsletter Settings**: Provides a form to change sending frequency, set AI prompts, and update the next send time.  
  * **Personas & Recipients**: Manages role-based personas and allows assigning them to recipients.  
* **Delete or Archive Project**  
  * An option to archive the entire project.  
  * Archiving a project triggers a confirmation dialogue.

### **Data Model** {#data-model}

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| **projectId** | String | Unique identifier for the project (e.g., UUID). |
| **projectName** | String | Human-readable title (e.g., "Marketing Campaign 2025"). |
| **description** | String | A short summary of the project's scope. |
| **status** | String (Enum) | Indicates if the project is "Active" or "Archived." |
| **dateCreated** | Date/DateTime | Timestamp of when the project was created. |
| **lastUpdated** | Date/DateTime | Most recent timestamp reflecting updates to this project. |
| **documentCount** | Number | Count of documents in this project. |
| **activePackages** | Number | Number of scraping packages currently active for this project. |
| **recipientCount** | Number | Number of recipients assigned to this project. |
| **nextNewsletterDate** | Date/DateTime | Scheduled send date/time for the next newsletter. |

### 

### 

### **Documents tab**

**Purpose:** The Documents section is a file management interface that allows users to upload, download, edit metadata, and delete documents related to the project.

#### **UI Layout & Structure**

The Documents Section is designed as an items repeater, meaning that each document is displayed as a repeating UI component in a list or grid view.

**Each document entry in the repeater includes the following fields:**

| Data Name | Data Type | Data Description |
| :---- | :---- | :---- |
| **documentId** | String | A unique identifier (e.g., UUID) for the project. |
| **documentName** | String | Human-readable title (e.g., "Marketing Campaign 2025 users analysis"). |
| **documentType** | String | File format (e.g., PDF, Word, Excel, CSV, Image) |
| **uploadDate** | Date/DateTime  | Timestamp when the file was uploaded |
| **Download button** | Image | Allows to download the file to the local storage |
| **Delete button** | image | Enables the removal of the file from both the project and the system. |

### 

#### **Functionalities**

**Each document in the items repeater supports the following actions:**

##### Download Documents

* Click the "Download" Button \- The system retrieves the document and downloads it to the user's device.

##### Delete Documents

* Click the "Delete" Button \- Triggers a confirmation dialog to prevent accidental deletions.

##### Upload New Documents

* Upload Button \- Opens a module that contains a file picker for users to upload one file.  
* Drag & Drop Support \- Users can drag and drop files directly into the upload area.  
* File Size Limit \- Restrict uploads to a maximum size of 10 mb


### **Newsletter settings tab**

The **Project Newsletter Tab** in Proton CRM allows users to manage the **newsletter scheduling, dispatch history** and **analytics tracking**.

#### **UI Elements**

The **Newsletter Tab** consists of the following key sections:

1. **Newsletter Scheduling & Frequency** \- Controls for **when and how often** newsletters are sent.  
2. **Scheduled Newsletters** \- Newsletters that have been scheduled along with their corresponding **scheduled times** are displayed in a list.  
3. **Newsletter Archive (Sent Newsletters List)** \- Displays the history of **past newsletters** with key engagement metrics.  
4. **Newsletter Analytics Overview** \- Provides **performance metrics** such as open and click rates.  
5. **Download Reports** \- Allows users to export newsletter analytics.

   ---

#### **Newsletter Scheduling & Frequency**

**Purpose**: Users can configure how often newsletters are sent and set delivery times.

##### UI Elements

**Delivery Frequency Selection** ‚Üí Users can choose from:

* **Daily**  
* **Weekly**  
* **Monthly**  
* **Custom** (user-defined intervals)  **Time Picker** \- Select the **specific time** the newsletter should be dispatched (e.g., 9:00 AM).  
* **Day of Week Selector** \- Available for **weekly and custom** schedules (e.g., Monday).  
*  **Save Settings Button** \- Saves the user's selections.

##### Data Table

| Field | Type | Description |
| :---- | :---- | :---- |
| **Frequency** | Enum (Daily/Weekly/Monthly/Custom) | How often the newsletter is sent |
| **Send Time** | Time | The time newsletters are dispatched |
| **Day of Week** | Enum (Monday-Sunday) | The specific day for **weekly** or **custom** schedules |
| **Next Scheduled Date** | Date/Time | The next planned newsletter dispatch |

#### **Scheduled newsletters**

The **Scheduled newsletters** will be positioned at the **middle of the Newsletter Tab**, above the **Sent Newsletters** list.

##### UI Elements

* **Next Newsletter Title** \- Displays the **subject line** of the upcoming newsletter.  
*  **Scheduled Send Date & Time** \- Shows when the newsletter will be sent.  
*  **Recipient Count** \- Number of recipients scheduled to receive it.  
* **Preview Newsletter** (Button) \- Opens a **preview modal** of the email content.  
* **Generate Content** (Button) \- **Generates content** with the AI module with the project's scraping module  for each persona.  
*  **Cancel** (Button) \- Allows users to **cancel** the scheduled newsletter.  
*  **Reschedule** (Button) \- Allows users to **change the send date** of the scheduled newsletter.

##### Data Table

#### 

| Field | Type | Description |
| :---- | :---- | ----- |
| **Newsletter ID**	 | UUID | Unique identifier for the newsletter |
| **Subject Line**	 | String | Title of the newsletter email |
| **Scheduled Send Date**	 | Date/Time | Planned send time |
| **Total Recipients**	 | Integer | Count of recipients receiving this newsletter |
| **Status** | Enum (Scheduled/Rescheduled/Canceled) | Status of the newsletter |
| **Actions** | Buttons | Preview  / Edit  / Cancel |

#### 

#### **Sent Newsletters** 

**Purpose**: Displays a **list of previously sent newsletters** with engagement statistics.

##### UI Elements

* **Search Bar** \- Allows users to search for newsletters by **subject line.**  
* **Sent Newsletter List** \- Displays these key details: **sent date, recipient count, open rate, and click rate**.

##### Data Table

| Field | Type | Description |
| :---- | :---- | ----- |
| **Newsletter ID** | UUID | Unique identifier for each newsletter |
| **Subject Line** | String | The email subject line |
| **Sent Date** | Date/Time | When the newsletter was sent |
| **Recipients Count** | Integer | Total number of recipients |
| **Open Rate (%)** | Number | Percentage of recipients who opened the newsletter |
| **Click Rate (%)** | Number | Percentage of recipients who clicked on a link |

#### **Newsletter Analytics Overview**

**Purpose**: Tracks **open rates, click rates, and subscriber trends** to measure campaign effectiveness.

#### **UI Elements**

* **Average Open Rate Display** \- Shows the average open percentage across all newsletters in the project.  
* **Average Click Rate Display** \- Displays the percentage of recipients who clicked a link.  
* **Total Subscribers Counter** \- Displays the total number of **subscribers**.

#### **Data Table**

#### 

| Field | Type | Description |
| :---- | :---- | :---- |
| **Average Open Rate (%)** | Number | The percentage of recipients who opened newsletters by mail client (e.g, Gmail or Outlook) |
| **Average Click Rate (%)** | Number | The percentage of recipients who clicked links |
| **Total Subscribers** | Number | The percentage of recipients who unsubscribed |

#### **Download Reports**

**Purpose**: Users can export newsletter performance reports for further analysis.

##### UI Elements

**Download Report Button** \- Generates a **CSV file** containing newsletter engagement metrics.

###### *Report Includes:*

* **Sent Newsletter List** (with open & click rates)  
* **Subscriber Growth Trends**  
* **Engagement Over Time** (heatmap of when users open emails)  
* **Device & Location Metrics** (if available)

### **Personas & Recipients tab**

The **Personas & Recipients Tab** allows users to manage **recipient lists and persona-based content customization** for a project. It provides tools to assign recipients to personas, adjust AI-driven tone and content settings, and track engagement metrics.

#### **UI Elements**

**Personas Section**

* **Add Persona Button** \- Opens a modal to create a new persona.  
* **Personas List** \- Displays **existing personas** with recipient counts and last modified timestamps.  
* **Edit Persona** (Button) **\-** Opens a persona editing modal.

**Recipients Section**

* **Add Recipients Button** \- Allows users to add new recipients manually or via CSV upload.  
* **Search & Filters** \- Search by **name, email, persona, or status**.  
*  **Recipients List** \- Displays all **assigned recipients**, including their persona classification.

#### **Data Table \- Personas Section**

| Field | Type | Description |
| :---- | :---- | :---- |
| **Persona ID** | UUID | Unique identifier for the persona |
| **Persona Name** | String | Name of the persona (e.g., "Executives") |
| **Recipients Count**	 | Integer | Total number of recipients assigned to this persona |
| **AI Tone Setting** | Array | Defines AI-generated content tone (e.g., Formal/Casual/Custom) |
| **Prompt Adjustments** | String | Custom AI instructions for content generation |
| **Last Modified** | Date/Time | Last update timestamp |
| **Action** | Button | Edit Persona |

#### **Data Table \- ◊ës Section**

| Field | Type | Description |
| :---- | :---- | :---- |
| **Recipient ID** | UUID | Unique identifier for the recipient |
| **Name Name** | String | Full name of the recipient |
| **Email** | String | Recipient's email address |
| **Persona Assigned** | String | The persona linked to the recipient |
| **Status** | Enum (Active/Inactive) | Indicates recipient subscription status |
| **Subscription Date** | Date/Time | When the recipient subscribed |
| **Action** | Button | Edit / / Remove |

### **Security & Permissions** {#security- &-permissions-1}

* **Role-Based Access**:  
  * **Admins**: Full view and edit privileges.  
  * **Editors**: Limited to updating or viewing certain fields if assigned to the project.

### **Validations & Error Handling** {#validations- &-error-handling}

* **Required Fields**: Must have a non-empty project name if editing.  
* **Status Updates**: If archiving a project, confirm that the user has admin privileges or the correct role.  
* **Not Found**: If the project ID is invalid or removed, return a `404 Not Found` response.

### **UI Reference** {#ui-reference}

This page typically appears after clicking a project card or list item from the **Projects Page** or from a **Recent Projects** section on the **Home Page**. The existing wireframes (under "Projects" or "Single Project" references) may detail how each subsection is presented (tabs or expanded sections).

### **Future Enhancements** {#future-enhancements-1}

* **Inline Document Previews**: Quick preview or search within documents from the Single Project interface.  
* **Project-Level Analytics**: Real-time stats on newsletter performance, open/click rates, or user engagement on a dedicated dashboard.  
* **Project Team Management**: Additional assignment of internal team roles to each project.

**Summary**  
The **Single Project Page** centralizes all project-specific information, enabling users to quickly manage the project's details, documents, scraping setups, newsletters, and recipients. Its intuitive layout, combined with direct navigation to deeper configuration tabs, helps streamline the overall management lifecycle for each client or internal initiative.

## **Scraping packages**

### **Purpose**

The **Scraping Packages Archive** page provides a centralized location where users can view all scraping packages in the system (across all projects or globally). Users can create new scraping packages and edit, activate, or deactivate existing ones. This helps maintain an organized repository of automated data ingestion configurations that feed content into Proton CRM.

### **UI Layout & Components**

* **Header & Navigation**

  * Inherits the same main header (logo, navigation, user profile) used throughout Proton CRM.  
  * A "Scraping Packages" link is highlighted or selected in the navigation to indicate the current page.  
* **Scraping Packages Table (or Grid)**

  * Displays each scraping package with basic details: Name, Description, Status (active/inactive), Last Run Time, and Next Scheduled Run (if applicable).  
  * May feature pagination when the total number of packages exceeds a configured threshold (e.g., 25 or 50).  
  * A search bar or filtering tool may appear at the top to find packages by name, status, or relevant keywords.  
* **Create New Package Button**

  * Allows users to create a new scraping package (opens a form or modal).  
  * The user can specify core fields such as name, source URLs, or scraping rules.  
* **Action Buttons (per package)**

  * **Edit** ‚Äì Opens a configuration form for the selected scraping package.  
  * **Activate/Deactivate** ‚Äì Toggles the status of the scraping package.  
  * **Delete** ‚Äì Permanently removes the scraping package from the system (with a confirmation dialog).

### **Key Functionalities**

* **List All Packages:** Show a quick overview of all existing packages, both active and inactive.  
* **Activate/Deactivate:** Toggle a package's ability to run. Deactivated packages do not fetch or ingest new data.  
* **Edit Scraping Package:** Modify any part of the package configuration (e.g., name, data source, rules).  
* **Create New Scraping Package:** Launch a form or modal to define a new package (e.g., define data source URLs, set scheduling).  
* **Delete Scraping Package:** Remove a package entirely (requires confirmation).

### **Data Model & Fields**

Below is an example of possible fields for each scraping package in the archive. (Note that the original document does not detail these fields explicitly; this table follows the style used elsewhere in the specification.)

| Field Name | Type | Description | Validation |
| :---- | :---- | :---- | :---- |
| **packageId** | String (UUID) | A unique identifier for the scraping package. | Auto-generated by the system. |
| **packageName** | String | A user-facing name or title for the scraping package (e.g., "Tech News Scraper"). | Required; must be unique among packages |
| **description** | String | A brief explanation of the package's purpose (e.g., "Scrapes top tech news sites weekly."). | Required |
| **status** | Enum | Active/Inactive, specifying whether the package is currently scraping data or paused. | Default: Active |
| **schedule** | Enum or String | Defines how often the scraping package runs (e.g., daily, weekly, monthly, or cron-like expression). | Required |
| **lastRun** | Date/DateTime | Timestamp of the most recent scraping job completion. | Auto-updated by system |
| **nextRun** | Date/DateTime | Timestamp of the next scheduled scraping job. | Auto-updated by system |
| **createdAt** | Date/DateTime | Timestamp when the package was created. | Auto-generated by system |
| **updatedAt** | Date/DateTime | Timestamp for the most recent modification. | Auto-updated by system |

### **Error Handling & Validation**

* **Unique Package Names:** If a user attempts to create or rename a package with a name that already exists, display an error (e.g., "This package name is already taken.").  
* **Required Fields:** If mandatory fields (such as `packageName` or `description`) are left blank, prompt the user to fill them in before saving.  
* **Confirmation on Delete:** Users must confirm deletion (e.g., "Are you sure you want to delete this package?").

### **Future Enhancements**

* **Bulk Actions:** Ability to activate/deactivate or delete multiple packages at once.  
* **Enhanced Filtering & Tagging:** Filter packages by category or attach tags for easier organization.  
* **Run History Logs:** Show the full log of scraping attempts, including success/failure states, data volumes ingested, etc.

## **Scraping Packages Archive**

### **Purpose**

The **Scraping Packages Archive** page provides a centralized location where users can view all scraping packages in the system (across all projects or globally). Users can create new scraping packages and edit, activate, or deactivate existing ones. This helps maintain an organized repository of automated data ingestion configurations that feed content into Proton CRM.

### **UI Layout & Components**

* **Header & Navigation**

  * Inherits the same main header (logo, navigation, user profile) used throughout Proton CRM.  
  * A "Scraping Packages" link is highlighted or selected in the navigation to indicate the current page.  
* **Scraping Packages Table (or Grid)**

  * Displays each scraping package with basic details: Name, Description, Status (active/inactive), Last Run Time, and Next Scheduled Run (if applicable).  
  * May feature pagination when the total number of packages exceeds a configured threshold (e.g., 25 or 50).  
  * A search bar or filtering tool may appear at the top to find packages by name, status, or relevant keywords.  
* **Create New Package Button**

  * Allows users to create a new scraping package (opens a form or modal).  
  * The user can specify core fields such as name, source URLs, or scraping rules.  
* **Action Buttons (per package)**

  * **Edit** ‚Äì Opens a configuration form for the selected scraping package.  
  * **Activate/Deactivate** ‚Äì Toggles the status of the scraping package.  
  * **Delete** ‚Äì Permanently removes the scraping package from the system (with a confirmation dialog).

### **Key Functionalities**

* **List All Packages:** Show a quick overview of all existing packages, both active and inactive.  
* **Activate/Deactivate:** Toggle a package's ability to run. Deactivated packages do not fetch or ingest new data.  
* **Edit Scraping Package:** Modify any part of the package configuration (e.g., name, data source, rules).  
* **Create New Scraping Package:** Launch a form or modal to define a new package (e.g., define data source URLs, set scheduling).  
* **Delete Scraping Package:** Remove a package entirely (requires confirmation).

### **Data Model & Fields**

Below is an example of possible fields for each scraping package in the archive. (Note that the original document does not detail these fields explicitly; this table follows the style used elsewhere in the specification.)

| Field Name | Type | Description | Validation |
| :---- | :---- | :---- | :---- |
| **packageId** | String (UUID) | A unique identifier for the scraping package. | Auto-generated by the system. |
| **packageName** | String | A user-facing name or title for the scraping package (e.g., "Tech News Scraper"). | Required; must be unique among packages |
| **description** | String | A brief explanation of the package's purpose (e.g., "Scrapes top tech news sites weekly."). | Required |
| **status** | Enum | Active/Inactive, specifying whether the package is currently scraping data or paused. | Default: Active |
| **schedule** | Enum or String | Defines how often the scraping package runs (e.g., daily, weekly, monthly, or cron-like expression). | Required |
| **lastRun** | Date/DateTime | Timestamp of the most recent scraping job completion. | Auto-updated by system |
| **nextRun** | Date/DateTime | Timestamp of the next scheduled scraping job. | Auto-updated by system |
| **createdAt** | Date/DateTime | Timestamp when the package was created. | Auto-generated by system |
| **updatedAt** | Date/DateTime | Timestamp for the most recent modification. | Auto-updated by system |

### 

### **Error Handling & Validation**

* **Unique Package Names:** If a user attempts to create or rename a package with a name that already exists, display an error (e.g., "This package name is already taken.").  
* **Required Fields:** If mandatory fields (such as `packageName` or `description`) are left blank, prompt the user to fill them in before saving.  
* **Confirmation on Delete:** Users must confirm deletion (e.g., "Are you sure you want to delete this package?").

### **Security & Permissions**

* **Role-Based Access:**  
  * **Admins:** Full access to view, create, edit, and delete any package.  
  * **Editors:** May be restricted to read-only or partial edit rights, depending on the organization's policy.  
* **Data Privacy:** If scraping packages involve credentials or private APIs, secure storage and masked display of such credentials is recommended (not shown in plain text).

### **Future Enhancements**

* **Bulk Actions:** Ability to activate/deactivate or delete multiple packages at once.  
* **Enhanced Filtering & Tagging:** Filter packages by category or attach tags for easier organization.  
* **Run History Logs:** Show the full log of scraping attempts, including success/failure states, data volumes ingested, etc.

## 

## **Scraping Package Configuration**

### **Purpose**

The **Scraping Package Configuration** page (or form) is where users specify exactly how a package scrapes data, what sources it uses, which schedule it follows, and any special rules for filtering or parsing the content. While the Archive lists packages, this page handles the detailed setup and maintenance of each package.

### **UI Layout & Components**

* **Header / Page Title**

  * Typically shows the package's name (e.g., "Configure: Tech News Scraper").  
* **Configuration Form Sections**

  * **Basic Info:** Name, description, active/inactive toggle.  
  * **Data Sources URL:** One or more source URLs or API endpoints the scraper targets.  
  * **Document Upload:** Section to upload and manage supplemental files that may aid the scraping logic or store domain data.  
  * **Schedule Settings:** Frequency of the scraping job (daily, weekly, monthly, custom cron).  
  * **Advanced Rules:** Keywords to include/exclude, language filtering, or region-based filters.

**Document Upload & Management**

* **Upload Button:** Allows adding one or more files (e.g., PDF, TXT, CSV) directly to the scraping package configuration.  
* **Document List (Items Repeater):** Displays uploaded files with file name, upload date, and possible actions (Download, Delete).  
* **File Size Limit:** Each document might be restricted to a maximum size (e.g., 10 MB).  
* **Allowed File Types:** (As needed) e.g., PDF, DOCX, CSV, TXT.  
    
* **Actions & Buttons**

  * **Save / Update Configuration** ‚Äì Saves all form changes.  
  * **Test Run** ‚Äì (Optional) Manually triggers a single scraping run to validate the config.  
  * **Cancel** ‚Äì Discards changes and returns to the previous screen (e.g., the archive).

### **Key Functionalities**

* **Set or Edit Name & Description:** Clearly identify the scraping package's purpose.  
* **Set Sources, Endpoint or Document:** Enter or edit one or multiple URLs, possibly with additional credentials if needed (e.g., API keys).  
* **Set Document:** Enter or edit one or multiple documents.  
* **Define Schedule:** Use a dropdown or text-based scheduler (e.g., cron syntax) to determine run intervals.  
* **Add Filter/Parsing Rules:** Indicate which types of content should be extracted or ignored.  
* **Activate/Deactivate the Package:** Toggle whether the package should run automatically.  
* **Manual Test Run (if supported):** Let users immediately try the scraping process to confirm correct setup.

### **Data Model & Fields**

Although each organization might have different data points to store, below is a typical set of fields for a single scraping package's configuration.

| Field Name | Type | Description | Validation |
| :---- | :---- | :---- | :---- |
| **packageId** | String (UUID) | Unique identifier for the scraping package. | Auto-generated, read-only. |
| **packageName** | String | User-facing title or label. | Required, unique among packages. |
| **description** | String | Brief explanation of the package's purpose. | Required. |
| **status** | Enum | Indicates Active / Inactive. | Default: Active. |
| **sourceURLs** | Array of Strings | One or more URLs to be scraped. | Must be valid URLs; at least one required. |
| **uploadedDocuments** | Array of Objects | Files attached for reference or specialized scraping instructions. Each object has at least `documentId`, `documentName`, `uploadDate`. | File size/type limits as defined by admin. |
| **schedule** | String / Enum | Defines frequency (daily, weekly, monthly, custom). Could also be cron-like expression. | Required. |
| **filters** | Object or String | Additional logic or filtering (e.g., only parse articles in English or containing certain keywords). | Optional. |
| **credentials** | Object (secure) | If applicable, API keys or basic auth credentials. Must be stored securely (e.g., encrypted at rest). | Protected, not displayed plainly. |
| **lastRun** | Date/DateTime | When the package last executed. | Auto-updated by system. |
| **nextRun** | Date/DateTime | Next scheduled run (calculated based on schedule). | Auto-updated by system. |
| **createdAt** | Date/DateTime | Creation timestamp. | Auto-generated, read-only. |
| **updatedAt** | Date/DateTime | Timestamp of the most recent configuration change. | Auto-updated by system. |

### **Error Handling & Validation**

* **Required Fields:** `packageName`, `description`, and `sourceURLs` cannot be blank.  
* **URL Validation:** If any `sourceURLs` entry is invalid, display an error message (e.g., "Please provide a valid URL").  
* **Schedule Validation:** If the user enters a custom schedule, ensure it follows the cron format or is recognized by the scheduling system.  
* **Unique Name Check:** If a user attempts to rename the package to a name that already exists, display an error.  
* **Credentials Security:** If credentials are incorrect or missing, provide a distinct warning or error message (e.g., "Invalid API key").

### **Security & Permissions**

* **Audit Logs:** Track changes to schedules, source URLs, or credentials.

### **Future Enhancements**

* **Multiple Source Schedules:** Support different frequencies for different URLs within the same package.  
* **Custom Transformers/Parsers:** Let users define transformations or parsing steps (e.g., using a simple script or JSONPath).  
* **Real-Time Monitoring & Error Reports:** Provide logs or email alerts if a scraping run fails.

## **Conclusion**

This specification document outlines the core functionality, data models, and user experience flows for the Proton CRM system. By consolidating project, persona, and content management within a single interface‚Äîand now extending these capabilities with robust scraping, document handling, and newsletter features‚ÄîProton aims to streamline the entire client engagement cycle. As the platform evolves, future enhancements and real-world feedback will guide iterative refinements, ensuring Proton remains a flexible, efficient, and scalable solution for content curation and delivery.
</file>

<file path="Proton Backend Specs.md">
# Proton Backend System Specification

## Document Information
- **Project Name**: Proton Backend
- **Document Status**: Draft
- **Last Updated**: March 20, 2025
- **Document Owner**: Engineering Team

## Executive Summary

This document outlines the technical specifications and implementation plan for the Proton backend system. Proton is an AI-powered content curation and delivery system designed to help consulting clients stay informed about relevant market developments after project completion. The backend consists of three core components:

1. **Content Ingestion & Curation (Scraping & Processing)**
2. **Data Storage & Management (Database)**
3. **AI-Powered Content Generation (RAG)**

This specification provides a comprehensive build plan for each component, detailing architecture, technologies, implementation phases, and integration strategies.

## 1. Content Ingestion & Curation (Scraping & Processing)

### Purpose
To collect, process, and prepare content from various sources for storage and AI analysis, ensuring a robust pipeline of relevant and timely information.

### Technical Architecture

#### Components Overview

1. **Scraper Service**
   - Microservice responsible for content extraction from defined sources
   - Configurable per source (web pages, APIs, RSS feeds)
   - Rate-limiting and retry mechanisms
   - IP rotation for avoiding anti-scraping measures

2. **Content Processor Service**
   - Extracts structured data from raw scraped content
   - Performs NLP tasks (entity recognition, keyword extraction, etc.)
   - Generates metadata and classifications

3. **Embedding Generator**
   - Creates vector embeddings for semantic search
   - Processes both text and metadata

4. **Content Ranking Engine**
   - Scores content based on relevance criteria
   - Applies filtering rules

5. **Job Scheduler**
   - Manages the timing and frequency of scraping jobs
   - Handles dependencies between jobs

#### Technology Stack

- **Primary Language**: Python 3.11+
- **Scraping Frameworks**: 
  - Scrapy for structured scraping
  - Selenium for JavaScript-rendered content
  - Beautiful Soup for HTML parsing
  - Newspaper3k for news-specific extraction
- **NLP/Embeddings**: 
  - Hugging Face Transformers
  - Sentence-Transformers for embeddings
  - spaCy for entity extraction and text processing
- **Job Orchestration**: 
  - Airflow for workflow scheduling
  - Celery for task queuing
- **Infrastructure**: 
  - Containerized with Docker
  - Kubernetes for orchestration
  - AWS Lambda for serverless processing tasks

#### Data Flow

1. **Source Configuration**:
   - Each source (URL, API, RSS feed) is registered with metadata
   - Scraping schedule and extraction rules defined

2. **Content Acquisition**:
   - Scheduled jobs fetch content from sources
   - Raw content stored temporarily
   - Provenance and timestamp metadata attached

3. **Processing Pipeline**:
   - Raw content ‚Üí HTML cleaning ‚Üí Text extraction
   - Content analysis (NLP) ‚Üí Entity extraction
   - Classification and tagging

4. **Embedding Generation**:
   - Content text vectorized for semantic search
   - Different embedding models for different content types

5. **Relevance Scoring**:
   - Multiple algorithms applied based on content type
   - Factored scoring system with configurable weights

### Implementation Plan

#### Phase 1: Basic Scraping Infrastructure (Weeks 1-3)

1. **Week 1: Source Configuration Framework**
   - Develop data schema for source configuration
   - Build basic configuration management API
   - Create source validation tools

2. **Week 2: Core Scraper Service**
   - Implement basic scrapers for common patterns
   - Develop HTML content extraction
   - Build rate limiting and retry logic

3. **Week 3: Basic Content Processing**
   - Implement text extraction and cleaning
   - Develop basic metadata generation
   - Create structured output format

#### Phase 2: Advanced Processing & Analysis (Weeks 4-6)

1. **Week 4: NLP Pipeline Integration**
   - Implement entity extraction and NER
   - Develop keyword identification
   - Create sentiment analysis module

2. **Week 5: Embedding Generation**
   - Integrate embedding models
   - Build vector storage interface
   - Develop batch processing for embeddings

3. **Week 6: Content Ranking System**
   - Implement scoring algorithms
   - Develop filtering rules engine
   - Create relevance feedback mechanisms

#### Phase 3: Optimization & Scaling (Weeks 7-8)

1. **Week 7: Scheduler & Orchestration**
   - Implement Airflow DAGs for workflow
   - Develop monitoring and alerting
   - Create failure recovery mechanisms

2. **Week 8: Performance Tuning**
   - Optimize resource usage
   - Implement caching strategies
   - Develop horizontal scaling capabilities

### API Endpoints

```
POST /api/v1/sources
GET /api/v1/sources/{id}
PUT /api/v1/sources/{id}
DELETE /api/v1/sources/{id}

POST /api/v1/sources/{id}/scrape
GET /api/v1/scrape-jobs/{job_id}

GET /api/v1/content
GET /api/v1/content/{id}
GET /api/v1/content/search

POST /api/v1/content/{id}/embeddings
GET /api/v1/content/{id}/embeddings

GET /api/v1/content/ranked?project={project_id}&limit={n}
```

### Monitoring & Logging

- **Metrics Collection**:
  - Scrape success/failure rates
  - Processing time per document
  - Content freshness metrics
  - Embedding generation statistics

- **Logging**:
  - Structured JSON logs
  - Error classification
  - Source-specific logging
  - Performance metrics

### Testing Strategy

- **Unit Tests**:
  - Individual scraper components
  - Processing functions
  - Ranking algorithms

- **Integration Tests**:
  - End-to-end pipeline tests
  - Source configuration to content storage

- **Performance Tests**:
  - Scraping rate limits
  - Processing throughput
  - Database load testing

## 2. Data Storage & Management (Database)

### Purpose
To store and organize all data in a way that supports efficient retrieval, ranking, and AI generation, ensuring scalability, performance, and data integrity.

### Technical Architecture

#### Database Schema

1. **Projects Table**
   - Primary information about client projects
   - Maps to frontend project management UI

2. **Sources Table**
   - Information about content sources
   - Configuration for scraping
   - Relationship to projects

3. **Content Table**
   - Core content storage
   - Metadata and extracted text
   - Relationships to sources and projects

4. **Embeddings Table/Collection**
   - Vector embeddings for content
   - Optimized for similarity search

5. **Personas Table**
   - Information about different user roles/personas
   - Preferences and customization

6. **Recipients Table**
   - End users receiving newsletters
   - Preferences and subscription settings

7. **Newsletters Table**
   - Generated newsletter content
   - Delivery metadata
   - Performance metrics

8. **Documents Table**
   - Uploaded document storage
   - Metadata and extracted content
   - Vector embeddings

#### Technology Stack

- **Primary Database**: 
  - MongoDB for primary document storage
  - Collections for projects, content, sources, and settings

- **Vector Database**:
  - Pinecone for vector search
  - Alternatively: MongoDB Atlas Vector Search

- **Caching Layer**:
  - Redis for high-performance caching
  - Content metadata and frequent queries

- **File Storage**:
  - AWS S3 for document storage
  - Optimized for large file retrieval

- **Search Index**:
  - Elasticsearch for full-text search capabilities
  - Custom analyzers for content-specific search

#### Data Access Patterns

1. **Content Retrieval Flows**:
   - By relevance score (ranked retrieval)
   - By semantic similarity (vector search)
   - By keyword or entity (text search)
   - By recency or source

2. **Write Patterns**:
   - Batch inserts from scrapers
   - Streaming updates for real-time sources
   - Periodic embedding updates

3. **Caching Strategy**:
   - Newsletter templates
   - Frequent queries
   - Recently used content
   - Project configuration

### Implementation Plan

#### Phase 1: Core Schema & Initial Implementation (Weeks 1-3)

1. **Week 1: Schema Design**
   - Finalize database schema
   - Design indexing strategy
   - Create migration scripts

2. **Week 2: Primary Database Setup**
   - MongoDB cluster deployment
   - Implement data models
   - Set up basic CRUD operations

3. **Week 3: File Storage Integration**
   - S3 bucket configuration
   - Document upload/download APIs
   - Content extraction from documents

#### Phase 2: Vector Search & Advanced Features (Weeks 4-6)

1. **Week 4: Vector Database Integration**
   - Deploy Pinecone instance
   - Implement vector storage API
   - Test similarity search performance

2. **Week 5: Search Capabilities**
   - Elasticsearch deployment
   - Index configuration
   - Query API development

3. **Week 6: Caching Implementation**
   - Redis cluster setup
   - Cache population logic
   - Cache invalidation strategies

#### Phase 3: Optimization & Scaling (Weeks 7-8)

1. **Week 7: Performance Tuning**
   - Index optimization
   - Query performance analysis
   - Data access pattern optimization

2. **Week 8: Data Lifecycle Management**
   - Implement data retention policies
   - Create archiving procedures
   - Develop backup and recovery processes

### API Endpoints

```
# Project APIs
POST /api/v1/projects
GET /api/v1/projects/{id}
PUT /api/v1/projects/{id}
DELETE /api/v1/projects/{id}

# Content APIs
GET /api/v1/content?project={project_id}&limit={n}&offset={o}
GET /api/v1/content/{id}
PUT /api/v1/content/{id}
DELETE /api/v1/content/{id}

# Vector Search APIs
POST /api/v1/vector-search
    {
      "query_vector": [...],
      "top_k": 10,
      "filter": {...}
    }

# Document Management APIs
POST /api/v1/documents
GET /api/v1/documents/{id}
DELETE /api/v1/documents/{id}
GET /api/v1/documents/{id}/content

# User & Persona APIs
POST /api/v1/personas
GET /api/v1/personas/{id}
PUT /api/v1/personas/{id}

# Newsletter Data APIs
GET /api/v1/newsletters
GET /api/v1/newsletters/{id}
POST /api/v1/newsletters
```

### Database Maintenance

- **Indexing Strategy**:
  - Text indices for content search
  - Compound indices for frequent queries
  - TTL indices for automatic data expiration

- **Backup Procedures**:
  - Daily snapshots
  - Point-in-time recovery capability
  - Cross-region replication

- **Monitoring**:
  - Query performance tracking
  - Storage utilization metrics
  - Index effectiveness analysis

## 3. AI-Powered Content Generation (RAG)

### Purpose
To generate personalized newsletters by combining relevant content from various sources, guided by user prompts and AI models, using a Retrieval-Augmented Generation (RAG) approach.

### Technical Architecture

#### Components Overview

1. **Content Retriever**
   - Responsible for finding relevant content based on queries
   - Utilizes vector search and keyword matching
   - Filters based on project, persona, and preferences

2. **RAG Orchestrator**
   - Manages the overall generation process
   - Handles prompt construction and context building
   - Monitors generation quality

3. **LLM Integration Service**
   - Interfaces with underlying language models
   - Manages API calls and rate limiting
   - Handles fallbacks and error recovery

4. **Content Formatter**
   - Structures generated content into newsletter format
   - Applies templates and styling
   - Ensures proper citation and attribution

5. **Analytics Engine**
   - Tracks newsletter performance
   - Captures feedback for model improvement
   - Generates insights for future optimization

#### Technology Stack

- **Primary Language**: Python 3.11+
- **LLM Integration**:
  - Anthropic Claude 3.5 Sonnet as primary LLM
  - Potentially Claude 3 Opus for higher-quality needs
  - LangChain for orchestration
- **Vector Search**:
  - Integration with Pinecone or MongoDB Atlas
  - Custom similarity scoring
- **Content Formatting**:
  - Jinja2 templates for newsletter structure
  - Markdown ‚Üí HTML conversion
- **API Layer**:
  - FastAPI for high-performance endpoints
  - Async processing for concurrent generations
- **Infrastructure**:
  - Containerized with Docker
  - Kubernetes for orchestration
  - GPU acceleration for local LLM (if applicable)

#### RAG Pipeline Flow

1. **Query Construction**:
   - Parse generation request
   - Extract project parameters and constraints
   - Build prompt with instructions and context

2. **Context Retrieval**:
   - Vector search for semantically relevant content
   - Keyword search for specific topics
   - Document search for project-specific content
   - Recent content prioritization

3. **Content Curation**:
   - Re-rank retrieved content
   - Filter for quality and relevance
   - Deduplicate similar content
   - Balance content types and sources

4. **Generation Process**:
   - Construct system prompt with instructions
   - Build user prompt with retrieved content
   - Call LLM with appropriate parameters
   - Process and validate generation

5. **Post-Processing**:
   - Apply formatting to generated text
   - Add citations and references
   - Insert images and links
   - Apply template styling

### Implementation Plan

#### Phase 1: RAG Foundation (Weeks 1-3)

1. **Week 1: Retrieval Mechanisms**
   - Implement vector search integration
   - Develop keyword-based retrieval
   - Create content filtering

2. **Week 2: LLM Integration**
   - Set up Anthropic API connectivity
   - Implement prompt engineering system
   - Create error handling and retries

3. **Week 3: Basic RAG Pipeline**
   - Develop end-to-end RAG flow
   - Implement content chunking
   - Create basic prompt templates

#### Phase 2: Advanced Generation & Formatting (Weeks 4-6)

1. **Week 4: Enhanced RAG Techniques**
   - Implement multi-stage generation
   - Develop self-critique and refinement
   - Create content diversity mechanisms

2. **Week 5: Newsletter Formatting**
   - Build template system
   - Implement citation generation
   - Develop HTML email formatting

3. **Week 6: Persona-Specific Generation**
   - Implement tone and style adaptation
   - Create persona-specific retrieval
   - Develop content preferences system

#### Phase 3: Analytics & Optimization (Weeks 7-8)

1. **Week 7: Performance Analytics**
   - Implement tracking for generated content
   - Create feedback collection mechanisms
   - Develop performance dashboards

2. **Week 8: Generation Optimization**
   - Tune generation parameters
   - Optimize context retrieval
   - Implement A/B testing framework

### API Endpoints

```
# RAG Generation APIs
POST /api/v1/generate/newsletter
    {
      "project_id": "...",
      "persona_id": "...",
      "focus_topics": [...],
      "excluded_topics": [...],
      "time_range": {...},
      "format_preferences": {...}
    }

GET /api/v1/generation/{id}/status
GET /api/v1/generation/{id}/result

# RAG Configuration APIs
POST /api/v1/rag/templates
GET /api/v1/rag/templates/{id}
PUT /api/v1/rag/templates/{id}

# Analytics APIs
GET /api/v1/analytics/newsletter/{id}
GET /api/v1/analytics/performance?timeframe={period}
```

### Prompt Engineering

- **System Prompts**:
  - Clear instructions for newsletter generation
  - Tone and style guidance
  - Citation and attribution requirements

- **User Prompts**:
  - Structured context from retrieved content
  - Specific generation instructions
  - Project and persona context

- **Example Template**:
```
{system_prompt}
You are generating a newsletter for {project_name} with focus on {focus_topics}.
Use a {tone} writing style appropriate for {persona_name}.
Always cite your sources for specific facts or quotes.
Structure the newsletter with: 1) an introduction, 2) {num_sections} key topic sections, 3) a brief conclusion.

{user_prompt}
Here is the context from relevant articles and documents:
{retrieved_content}

Based on this information, generate a newsletter focused on {focus_topics} that would be valuable for {target_audience}.
{additional_instructions}
```

### Performance & Monitoring

- **Generation Metrics**:
  - End-to-end latency
  - Retrieval coverage
  - LLM token usage
  - Error rates

- **Quality Metrics**:
  - Content relevance scores
  - Source diversity
  - Information accuracy audits
  - User feedback analysis

## Integration & Dependencies

### Component Interactions

1. **Content Ingestion ‚Üí Data Storage**:
   - Scraped content flows into database
   - Embeddings stored in vector database
   - Content metadata indexed for search

2. **Data Storage ‚Üí Content Generation**:
   - Content retrieved for RAG process
   - Project configuration guides retrieval
   - Persona preferences influence selection

3. **Content Generation ‚Üí Performance Analytics**:
   - Generated newsletters tracked
   - User interactions recorded
   - Feedback loops to improve future generations

### Shared Libraries & Utilities

1. **Authentication & Authorization**:
   - Consistent access control across all components
   - Role-based permissions aligned with frontend

2. **Logging & Monitoring**:
   - Standardized logging format
   - Centralized log collection
   - Common alerting thresholds

3. **Error Handling**:
   - Consistent error codes
   - Standardized error responses
   - Cross-component retry policies

### External Dependencies

1. **Anthropic Claude API**:
   - Used for newsletter generation
   - Rate limits and costs to monitor
   - Fallback strategy for API outages

2. **AWS Services**:
   - S3 for document storage
   - SQS for job queuing
   - CloudWatch for monitoring

3. **Third-Party Services**:
   - Email delivery service for newsletters
   - Analytics platform integration
   - Potentially OpenAI API as LLM fallback

## Deployment & Operations

### Environment Strategy

1. **Development Environment**:
   - Local Docker Compose setup
   - Mocked external services
   - Test databases

2. **Staging Environment**:
   - Kubernetes cluster with limited resources
   - Integration with test APIs
   - Full pipeline validation

3. **Production Environment**:
   - Highly available Kubernetes cluster
   - Autoscaling configuration
   - Production database clusters
   - Full monitoring and alerting

### CI/CD Pipeline

1. **Build Process**:
   - GitHub Actions for automated builds
   - Container image creation
   - Automated testing

2. **Deployment Process**:
   - ArgoCD for Kubernetes deployments
   - Blue/green deployment strategy
   - Automated rollbacks

3. **Monitoring & Alerting**:
   - Prometheus for metrics collection
   - Grafana for dashboards
   - PagerDuty integration for alerts

### Scaling Strategy

1. **Horizontal Scaling**:
   - Kubernetes pod autoscaling
   - Database read replicas
   - Cache distribution

2. **Load Balancing**:
   - API gateway for request distribution
   - Content generation load balancing
   - Database query distribution

3. **Resource Optimization**:
   - Efficient content caching
   - Scheduled vs. on-demand processing
   - Asynchronous task processing

## Timeline & Milestones

### Project Schedule

1. **Month 1: Foundation**
   - Week 1-2: Database schema implementation
   - Week 2-3: Basic scraper service
   - Week 3-4: Initial RAG pipeline

2. **Month 2: Core Functionality**
   - Week 5-6: Advanced content processing
   - Week 6-7: Vector search integration
   - Week 7-8: Newsletter generation

3. **Month 3: Integration & Optimization**
   - Week 9-10: Frontend integration
   - Week 10-11: Performance optimization
   - Week 11-12: End-to-end testing and validation

### Key Milestones

1. **Milestone 1: Content Pipeline (End of Week 4)**
   - Basic scraping operational
   - Content storage implemented
   - Initial processing pipeline functional

2. **Milestone 2: RAG Generation (End of Week 8)**
   - Content retrieval operational
   - Basic newsletter generation working
   - Template system implemented

3. **Milestone 3: Production Readiness (End of Week 12)**
   - Full pipeline integrated
   - Performance metrics met
   - Documentation completed
   - Deployment automation finished

## Risk Assessment & Mitigation

### Technical Risks

1. **LLM API Reliability**:
   - **Risk**: Anthropic API outages or rate limit issues
   - **Mitigation**: Implement fallback models and queuing system

2. **Scraping Challenges**:
   - **Risk**: Anti-scraping measures on target sites
   - **Mitigation**: Rotate IPs, implement backoff, use multiple techniques

3. **Data Volume Management**:
   - **Risk**: Excessive storage needs for content and embeddings
   - **Mitigation**: Implement data retention policies, optimize storage

### Project Risks

1. **Integration Complexity**:
   - **Risk**: Difficulty integrating all components
   - **Mitigation**: Clear interfaces, thorough testing, phased integration

2. **Performance Bottlenecks**:
   - **Risk**: Slow newsletter generation
   - **Mitigation**: Optimize retrieval, cache common queries, pre-generate content

3. **Content Quality Issues**:
   - **Risk**: Poor quality or irrelevant content in newsletters
   - **Mitigation**: Implement content filters, quality scoring, human review options

## Conclusion

This backend specification outlines the comprehensive plan for implementing the Proton system's server-side components. The three core components‚ÄîContent Ingestion & Curation, Data Storage & Management, and AI-Powered Content Generation‚Äîwork together to create a robust platform for delivering personalized, AI-generated newsletters.

The phased implementation approach allows for incremental development and testing, with clear milestones and dependencies. By following this specification, the development team can build a scalable, maintainable, and high-performance backend system that meets the requirements of the Proton project.

## Appendices

### A. API Reference

Detailed API documentation for all endpoints, including:
- Request/response formats
- Authentication requirements
- Error codes and handling
- Rate limits

### B. Database Schema Details

Complete database schema with:
- Collection/table definitions
- Field types and constraints
- Index specifications
- Relationship diagrams

### C. Environment Configuration

Configuration templates for:
- Development environment
- Staging environment
- Production environment
- CI/CD pipeline
</file>

<file path="Proton Product Req.md">
Proton Product Requirements

| DATE | `02.17.25` |  |  |
| :---- | :---- | :---- | :---- |
| PREPARED BY | **Gal Shaya** Associate Director, Tech   | **Peter Pawlick**Principal, Head of Experience | **Liam Forland** Strategist, Experience                 |
|  |  |  |  |
|  |  |  |  |

# 

\-+

## Document Information

* **Product Name**: Proton  
* **Document Status**: Draft  
* **Last Updated**: January 2025  
* **Document Owners**: XP Team 

## Executive Summary

Proton is an AI-powered content curation and delivery system designed to help consulting clients stay informed about relevant market developments after project completion. By extending Proton with a robust CRM-style frontend, internal users can manage project-specific content, customize newsletter delivery, and define role-based experiences seamlessly. This structure supports organization- or project-based management (e.g., Advent Health, Mastercard, Samsung), enabling granular control over documents, scraping packages, newsletter settings, and personalized recipient assignments.

## Problem Statement

### Current Situation

Post-project engagement with consulting clients typically ends abruptly, missing opportunities for continued value delivery and relationship building.

### Problem Definition

Clients need ongoing support to:

* Stay updated on relevant market developments  
* Understand the implications of industry changes  
* Maintain momentum from consulting engagement  
* Access insights relevant to their specific context

  ## Product Overview

  ### **Core Product Capabilities**

1. **Content Sourcing & Management**  
   * **Document Upload:**  
     Users can upload static data (e.g., PDFs, DOCX files) to the client Proton database, adding contextual depth for enhanced content curation.  
   * **Scraping Package Management:**  
     Users can choose and configure specific scraping packages to tailor content ingestion from diverse sources.  
   * **Content Package Management System:**  
     Organizes ingested content for efficient processing and subsequent analysis.  
2. **AI-Powered Analysis**  
   * **RAG (Retrieval Augmented Generation) System:**  
     Integrates AI to enhance content generation.  
   * **Context-Aware Curation:**  
     Analyzes both static documents and dynamically sourced content.  
   * **Semantic Search Capabilities:**  
     Enables efficient and targeted content retrieval.  
3. **Content Delivery**  
   * **Automated Email Newsletter System:**  
     Delivers personalized newsletters directly to clients.  
   * **Web-Based Content Management Interface:**  
     Offers real-time previews and editing capabilities.  
   * **Customizable Delivery Schedules & Templates:**  
     Empowers users to control timing, tone, and formatting of newsletters.  
4. **Enhanced CRM & User Management (New Frontend Capabilities)**  
   * **Project-Based Hierarchy:**  
     Internal users manage multiple projects (e.g., Advent Health, Mastercard, Samsung). Each project encapsulates its own documents, scraping packages, and newsletter settings.  
   * **Document Management:**  
     A dedicated interface for uploading, previewing, and managing documents within each project.  
   * **Scraping Package Configuration:**  
     A dashboard module to enable/disable and configure various scraping packages.  
   * **Newsletter Scheduling & Settings:**  
     A user-friendly control panel to set and adjust global newsletter settings (frequency, AI prompt configurations, content focus).  
   * **Role-Based Persona Management:**  
     * Every project comes with a default ‚ÄúGeneral Persona‚Äù for baseline newsletter delivery.  
     * Users can add and customize additional personas‚Äîduplicating the default‚Äîadjusting tone, content filters, or other parameters.  
     * Recipients can then be assigned a persona via a dropdown selection to ensure tailored communication.

   ---

   ## Target Users

* **External Users:**  
  * Client stakeholders from previous consulting engagements  
  * New business contacts requiring industry insights  
* **Internal Users:**  
  * Proton team members managing client relationships  
* **Extended CRM Users:**  
  * Administrators and project managers using role-based persona definitions  
  * End users who require personalized content adjustments based on their role within the organization

  ---

  ## Technical Requirements

  ### **System Architecture**

**Frontend (Angular-based CRM Management Interface):**

* **Dashboard & Navigation:**  
  * **Project Dashboard:** Displays a list of projects (e.g., Advent Health, Mastercard, Samsung) with summary metrics (newsletter performance, document uploads, active scraping packages).  
  * **Navigation Tabs:** Within each project, tabs include: Dashboard, Documents, Content Packages, Newsletter Settings, Newsletter Personas & Recipients, and Analytics.  
* **Key Functionalities:**  
  * **Document Upload Module:**  
    Allows users to upload static files via drag-and-drop or file selection, with options for file preview and metadata entry.  
  * **Scraping Package Selection:**  
    Interactive controls (toggles, checklists, parameter forms) to enable/disable and configure scraping packages.  
  * **Newsletter Scheduling & Settings:**  
    A calendar/scheduler interface for selecting frequencies (daily, weekly, monthly, custom) and setting preferred dispatch times. Adjustments to global AI prompts and content focus are previewed in real time.  
  * **Persona & Recipient Management:**  
    Interface for viewing the default ‚ÄúGeneral Persona,‚Äù adding new personas (via duplication with customization options), and assigning each recipient a preferred persona using dropdown menus.  
  * **Responsive Design:**  
    Optimized for both desktop and mobile devices.

**Backend:**

* **API:**  
  Python-based API to handle requests from the frontend.  
* **Database:**  
  MongoDB with vector embeddings for content and document management.  
* **Integrations:**  
  Secure file uploads, real-time notifications, email delivery system, document parsing, and AI analysis engines.

  ### **Integration Requirements:**

* Secure storage and transmission of documents.  
* Real-time status updates via Angular.  
* API endpoints to support configuration, scheduling, and personalized content adjustments.  
  ---

  ## Frontend User Journeys

  ### **1\. Project Navigation & Overview**

* **Step 1:**  
  The internal user logs in and lands on the central dashboard, which displays a list of projects (e.g., Advent Health, Mastercard, Samsung).  
* **Step 2:**  
  The user selects a project card (e.g., "Advent Health") to view detailed project information and management options.  
* **Step 3:**  
  A sidebar within the project view provides navigation to key modules: Documents, Scraping Packages, Newsletter Settings, and Newsletter Personas & Recipients.

  ### **2\. Document Upload Journey**

* **Step 1:**  
  Within a selected project, the user navigates to the "Documents" tab.  
* **Step 2:**  
  The user clicks ‚ÄúUpload New Document,‚Äù selecting a file from their device.  
* **Step 3:**  
  The user enters associated metadata (title, description, tags) to contextualize the document.  
* **Step 4:**  
  The document is uploaded to the project‚Äôs Proton database and indexed for context.  
* **Step 5:**  
  Confirmation is provided, and the document appears in the document library with options for previewing and editing.

  ### **3\. Scraping Package Selection Journey**

* **Step 1:**  
  The user accesses the "Content Packages" section within the selected project.  
* **Step 2:**  
  A list of available scraping packages is displayed with brief descriptions.  
* **Step 3:**  
  The user selects desired packages (via checkboxes or toggle switches) and configures any necessary parameters.  
* **Step 4:**  
  Changes are saved, and the system updates the project's content ingestion settings accordingly.

  ### **4\. Newsletter Settings Journey**

* **Step 1:**  
  The user navigates to the "Newsletter Settings" tab within the project.  
* **Step 2:**  
  Global newsletter settings are displayed, including frequency, AI prompt configurations, and content focus.  
* **Step 3:**  
  The user adjusts the settings using a calendar/scheduler interface and real-time preview features.  
* **Step 4:**  
  Upon saving, the updated settings are applied to future newsletter generation for the project.

  ### **5\. Newsletter Persona & Recipient Management Journey**

* **Step 1:**  
  The user selects the "Newsletter Personas & Recipients" tab within the project.  
* **Step 2:**  
  The default ‚ÄúGeneral Persona‚Äù is displayed automatically as the baseline template.  
* **Step 3:**  
  The user may click ‚ÄúAdd Persona‚Äù to duplicate the default, then customize parameters such as tone, content filters, and formatting.  
* **Step 4:**  
  In the recipients list, a dropdown menu next to each recipient allows the user to select which newsletter persona (default or custom) they will receive.  
* **Step 5:**  
  The system saves the personalized settings and applies these adjustments to subsequent newsletter distributions.  
  ---

  ## Success Metrics

  ### **Key Performance Indicators**

* **User Engagement:**  
  * Newsletter open rates  
  * Click-through rates  
  * Client feedback ratings  
* **Content Quality:**  
  * Relevance scores  
  * Client-reported insight value  
  * Content freshness  
* **Operational Efficiency:**  
  * Reduction in newsletter production time  
  * Improved content processing accuracy  
  * Lower configuration error rates

  ---

  ## Development Milestones

  ### **Phase 1: Core Infrastructure**

* Set up MongoDB with vector embeddings  
* Implement basic API structure  
* Develop the content scraping system  
* Create the foundational frontend framework

  ### **Phase 2: AI Integration**

* Implement the RAG system  
* Develop newsletter generation logic  
* Create content curation algorithms  
* Set up vector search capabilities

  ### **Phase 3: Delivery System**

* Build the email delivery system  
* Develop the web-based content management interface  
* Implement user controls and settings  
* Create monitoring and analytics modules

  ### **Phase 4: Enhanced CRM Frontend & User Journeys**

* **Project-Based Navigation:**  
  Develop the central project dashboard and detailed project views.  
* **Document Management Module:**  
  Build UI components for file uploads, metadata entry, and preview functionalities.  
* **Scraping Package Configuration Interface:**  
  Create interactive dashboards for package selection and configuration.  
* **Newsletter Scheduling & Settings:**  
  Integrate calendar-based scheduling with real-time previews.  
* **Persona & Recipient Management Module:**  
  Develop features for managing the default ‚ÄúGeneral Persona,‚Äù adding customized personas, and assigning recipients via dropdown.  
* **User Experience Enhancements:**  
  Implement responsive design, error handling, and real-time notifications to ensure a seamless user journey.  
  ---

  ## Constraints & Limitations

* **Data Privacy & Security:**  
  All operations must ensure secure transmission and storage of client data.  
* **Minimal Manual Intervention:**  
  Automation should reduce the need for human oversight in content curation, document processing, and newsletter scheduling.  
* **Compliance:**  
  Content must be properly attributed and adhere to copyright regulations.  
* **Performance:**  
  Enhancements must not adversely impact system responsiveness or existing operational processes.  
  ---


  
---
</file>

<file path="proton_db_setup.py">
from pymongo import MongoClient
from pymongo.collection import Collection
from pymongo.database import Database
import pymongo
import datetime
from typing import Dict, List, Optional, Any, Union
import logging
import os
from dotenv import load_dotenv
import dns.resolver
import hashlib
from bson.objectid import ObjectId

# Configure DNS resolver explicitly
dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
dns.resolver.default_resolver.nameservers = ['8.8.8.8', '8.8.4.4']  # Use Google's public DNS

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProtonDB:
    """
    Class to handle MongoDB connection and operations for Proton CRM
    """
    
    def __init__(self, connection_string: Optional[str] = None):
        """
        Initialize the MongoDB connection
        
        Args:
            connection_string: MongoDB connection string. If None, will use MONGODB_URI env variable.
        """
        # Get connection string from environment variable if not provided
        if connection_string is None:
            connection_string = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
        
        # Connect to MongoDB
        try:
            # Add connection options to handle DNS issues
            self.client = MongoClient(
                connection_string,
                connectTimeoutMS=30000,
                socketTimeoutMS=360000,
                serverSelectionTimeoutMS=30000
            )
            
            # Test connection
            self.client.admin.command('ping')
            
            self.db = self.client["proton"]
            logger.info("Connected to MongoDB successfully")
            
            # Create and configure collections with schema validation
            self._setup_articles_collection()
            
        except Exception as e:
            logger.error(f"Failed to connect to MongoDB: {e}")
            raise
    
    def _setup_articles_collection(self) -> None:
        """
        Setup the articles collection with schema validation
        """
        # Define the article schema for validation
        article_schema = {
            "$jsonSchema": {
                "bsonType": "object",
                "required": ["title", "content", "source_url", "scraping_package_id", "date_scraped", "project_ids"],
                "properties": {
                    "title": {
                        "bsonType": "string",
                        "description": "Title of the article"
                    },
                    "content": {
                        "bsonType": "string",
                        "description": "Main content of the article"
                    },
                    "summary": {
                        "bsonType": "string",
                        "description": "AI-generated summary of the article content"
                    },
                    "source_url": {
                        "bsonType": "string",
                        "description": "URL where the article was scraped from"
                    },
                    "source_name": {
                        "bsonType": "string",
                        "description": "Name of the source website/feed"
                    },
                    "author": {
                        "bsonType": "string",
                        "description": "Author of the article"
                    },
                    "published_date": {
                        "bsonType": ["date", "null"],
                        "description": "Original publication date of the article"
                    },
                    "date_scraped": {
                        "bsonType": "date",
                        "description": "Date and time when the article was scraped"
                    },
                    "date_updated": {
                        "bsonType": "date",
                        "description": "Date and time when the article was last updated in the database"
                    },
                    "scraping_package_id": {
                        "bsonType": "objectId",
                        "description": "Reference to the scraping package that retrieved this article"
                    },
                    "project_ids": {
                        "bsonType": "array",
                        "description": "List of project IDs this article is associated with",
                        "items": {
                            "bsonType": "objectId"
                        }
                    },
                    "keywords": {
                        "bsonType": "array",
                        "description": "List of keywords extracted from the article",
                        "items": {
                            "bsonType": "string"
                        }
                    },
                    "entities": {
                        "bsonType": "array",
                        "description": "Named entities extracted from the content",
                        "items": {
                            "bsonType": "object",
                            "required": ["text", "type"],
                            "properties": {
                                "text": {
                                    "bsonType": "string",
                                    "description": "Entity text"
                                },
                                "type": {
                                    "bsonType": "string",
                                    "description": "Entity type (e.g., PERSON, ORGANIZATION, etc.)"
                                }
                            }
                        }
                    },
                    "relevance_scores": {
                        "bsonType": "object",
                        "description": "Scores indicating relevance to different criteria",
                        "properties": {
                            "overall": {
                                "bsonType": "double",
                                "description": "Overall relevance score (0-1)"
                            },
                            "recency": {
                                "bsonType": "double",
                                "description": "Recency score (0-1)"
                            },
                            "persona_specific": {
                                "bsonType": "object",
                                "description": "Relevance scores specific to different personas",
                                "additionalProperties": True
                            }
                        }
                    },
                    "vector_embedding": {
                        "bsonType": "array",
                        "description": "Vector embedding of the article content for semantic search",
                        "items": {
                            "bsonType": "double"
                        }
                    },
                    "metadata": {
                        "bsonType": "object",
                        "description": "Additional metadata about the article",
                        "additionalProperties": True
                    },
                    "content_type": {
                        "bsonType": "string",
                        "description": "Type of content (e.g., news, blog, press release)",
                        "enum": ["news", "blog", "press_release", "research", "social_media", "other"]
                    },
                    "language": {
                        "bsonType": "string",
                        "description": "Language of the article content"
                    },
                    "tags": {
                        "bsonType": "array",
                        "description": "Custom tags assigned to the article",
                        "items": {
                            "bsonType": "string"
                        }
                    },
                    "used_in_newsletters": {
                        "bsonType": "array",
                        "description": "List of newsletter IDs this article has been used in",
                        "items": {
                            "bsonType": "objectId"
                        }
                    },
                    "sentiment": {
                        "bsonType": "object",
                        "description": "Sentiment analysis of the article content",
                        "properties": {
                            "score": {
                                "bsonType": "double",
                                "description": "Sentiment score from -1 (negative) to 1 (positive)"
                            },
                            "magnitude": {
                                "bsonType": "double",
                                "description": "Magnitude of the sentiment (intensity)"
                            }
                        }
                    },
                    "status": {
                        "bsonType": "string",
                        "description": "Status of the article in the system",
                        "enum": ["active", "archived", "flagged"]
                    },
                    "image_url": {
                        "bsonType": "string",
                        "description": "URL of the main image associated with the article"
                    }
                }
            }
        }
        
        # Create or update the articles collection with schema validation
        try:
            # Check if we're using MongoDB Atlas (look for "+srv" in connection string)
            is_atlas = "+srv" in os.getenv("MONGODB_URI", "")
            
            # Check if collection exists
            collection_names = self.db.list_collection_names()
            
            if "articles" not in collection_names:
                # Create new collection - if on Atlas, create without validation to avoid permission issues
                if is_atlas:
                    self.db.create_collection("articles")
                    logger.info("Created articles collection without schema validation (Atlas detected)")
                else:
                    # For self-hosted MongoDB, use validation
                    self.db.create_collection("articles", validator=article_schema)
                    logger.info("Created articles collection with schema validation")
            else:
                # Collection exists, only try to update schema if not on Atlas
                if not is_atlas:
                    try:
                        self.db.command("collMod", "articles", validator=article_schema)
                        logger.info("Updated articles collection schema validation")
                    except Exception as e:
                        logger.warning(f"Could not update schema validation: {e}")
                        logger.warning("Continuing without schema validation")
            
            # Create indexes for efficient querying
            articles_collection = self.db["articles"]
            
            # Create indexes regardless of Atlas or self-hosted
            try:
                articles_collection.create_index("source_url", unique=True)
                articles_collection.create_index("date_scraped")
                articles_collection.create_index("project_ids")
                articles_collection.create_index("scraping_package_id")
                articles_collection.create_index("keywords")
                articles_collection.create_index("content_type")
                articles_collection.create_index("status")
                articles_collection.create_index([("title", pymongo.TEXT), ("content", pymongo.TEXT)], name="text_search_index")
                
                logger.info("Created indexes for articles collection")
            except Exception as e:
                logger.warning(f"Could not create all indexes: {e}")
                logger.warning("Some queries may be slower than expected")
            
            # Create vector search index if enabled and supported by the server
            vector_search_enabled = os.getenv("MONGODB_VECTOR_SEARCH_ENABLED", "False").lower() in ("true", "1", "yes")
            if vector_search_enabled and is_atlas:
                try:
                    articles_collection.create_index([("vector_embedding", "2dsphere")], name="vector_search_index")
                    logger.info("Created vector search index")
                except Exception as e:
                    logger.warning(f"Could not create vector search index: {e}")
                    logger.warning("Vector search may not be available without MongoDB Atlas Vector Search")
            
        except Exception as e:
            logger.error(f"Failed to setup articles collection: {e}")
            logger.warning("Continuing without schema validation or all indexes")
    
    def insert_article(self, article_data):
        """
        Insert a new article into the database

        Args:
            article_data (dict): Article data

        Returns:
            str: ID of the inserted article
        """
        try:
            # Check if an article with this URL already exists
            existing = self.db.articles.find_one({"source_url": article_data["source_url"]})
            if existing:
                logger.info(f"Article with URL {article_data['source_url']} already exists in the database (ID: {existing['_id']})")
                return str(existing["_id"])

            # Set default fields if not provided
            if "date_scraped" not in article_data:
                article_data["date_scraped"] = datetime.datetime.utcnow()
            
            if "content_type" not in article_data:
                article_data["content_type"] = "news"
                
            if "status" not in article_data:
                article_data["status"] = "active"
                
            if "relevance_scores" not in article_data:
                # Default relevance score
                article_data["relevance_scores"] = {"overall": 0.5}
            
            # Calculate a content hash for deduplication checks
            content_to_hash = f"{article_data.get('title', '')}{article_data.get('content', '')}"
            article_data["content_hash"] = hashlib.md5(content_to_hash.encode()).hexdigest()
            
            # Store relationship to scraping package if available
            if "scraping_package_id" in article_data and not isinstance(article_data["scraping_package_id"], ObjectId):
                try:
                    article_data["scraping_package_id"] = ObjectId(article_data["scraping_package_id"])
                except Exception as e:
                    logger.warning(f"Invalid scraping_package_id format: {e}")
                    # Keep it as is if conversion fails
            
            # Ensure we capture package name if available
            if "scraping_package_name" not in article_data and "scraping_package_id" in article_data:
                # Try to look up package name from database
                try:
                    package = self.db.scraping_packages.find_one({"_id": article_data["scraping_package_id"]})
                    if package and "name" in package:
                        article_data["scraping_package_name"] = package["name"]
                except Exception as e:
                    logger.warning(f"Failed to look up package name: {e}")
            
            # Insert the article
            result = self.db.articles.insert_one(article_data)
            logger.info(f"Inserted article '{article_data.get('title', 'Untitled')}' with ID {result.inserted_id}")
            
            # Also update package statistics if package ID is provided
            if "scraping_package_id" in article_data:
                try:
                    self.db.scraping_packages.update_one(
                        {"_id": article_data["scraping_package_id"]},
                        {
                            "$inc": {"article_count": 1, "articles_last_run": 1},
                            "$set": {"last_run": datetime.datetime.utcnow()}
                        }
                    )
                except Exception as e:
                    logger.warning(f"Failed to update package statistics: {e}")
            
            return str(result.inserted_id)
        except Exception as e:
            logger.error(f"Failed to insert article: {e}")
            return None

    def get_article_by_id(self, article_id: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve an article by its ID
        
        Args:
            article_id: ID of the article to retrieve
            
        Returns:
            Optional[Dict[str, Any]]: Article data or None if not found
        """
        try:
            article = self.db.articles.find_one({"_id": ObjectId(article_id)})
            return article
        except Exception as e:
            logger.error(f"Failed to retrieve article: {e}")
            return None

    def get_articles_for_project(self, project_id: str, limit: int = 100, skip: int = 0) -> List[Dict[str, Any]]:
        """
        Retrieve articles associated with a specific project
        
        Args:
            project_id: ID of the project
            limit: Maximum number of articles to retrieve
            skip: Number of articles to skip (for pagination)
            
        Returns:
            List[Dict[str, Any]]: List of article data
        """
        try:
            articles = list(self.db.articles.find(
                {"project_ids": ObjectId(project_id), "status": "active"}
            ).sort("date_scraped", -1).skip(skip).limit(limit))
            return articles
        except Exception as e:
            logger.error(f"Failed to retrieve articles for project: {e}")
            return []

    def get_articles_by_scraping_package(self, package_id: str, limit: int = 100, skip: int = 0) -> List[Dict[str, Any]]:
        """
        Retrieve articles collected by a specific scraping package
        
        Args:
            package_id: ID of the scraping package
            limit: Maximum number of articles to retrieve
            skip: Number of articles to skip (for pagination)
            
        Returns:
            List[Dict[str, Any]]: List of article data
        """
        try:
            articles = list(self.db.articles.find(
                {"scraping_package_id": ObjectId(package_id), "status": "active"}
            ).sort("date_scraped", -1).skip(skip).limit(limit))
            return articles
        except Exception as e:
            logger.error(f"Failed to retrieve articles for scraping package: {e}")
            return []

    def update_article(self, article_id: str, updates: Dict[str, Any]) -> bool:
        """
        Update an existing article
        
        Args:
            article_id: ID of the article to update
            updates: Dictionary containing fields to update
            
        Returns:
            bool: True if update was successful, False otherwise
        """
        try:
            # Add update timestamp
            updates["date_updated"] = datetime.datetime.utcnow()
            
            result = self.db.articles.update_one(
                {"_id": ObjectId(article_id)},
                {"$set": updates}
            )
            
            return result.modified_count > 0
        except Exception as e:
            logger.error(f"Failed to update article: {e}")
            return False

    def search_articles(self, query: str, project_id: Optional[str] = None, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Search articles using text search
        
        Args:
            query: Search query string
            project_id: Optional project ID to restrict search
            limit: Maximum number of results
            
        Returns:
            List[Dict[str, Any]]: List of matching articles
        """
        try:
            search_filter = {"$text": {"$search": query}, "status": "active"}
            
            # Add project filter if specified
            if project_id:
                search_filter["project_ids"] = ObjectId(project_id)
            
            articles = list(self.db.articles.find(
                search_filter,
                {"score": {"$meta": "textScore"}}
            ).sort([("score", {"$meta": "textScore"})]).limit(limit))
            
            return articles
        except Exception as e:
            logger.error(f"Failed to search articles: {e}")
            return []

    def vector_search(self, embedding: List[float], project_id: Optional[str] = None, limit: int = 20) -> List[Dict[str, Any]]:
        """
        Search articles using vector similarity
        Note: Requires MongoDB Atlas Vector Search
        
        Args:
            embedding: Vector embedding to search for similar articles
            project_id: Optional project ID to restrict search
            limit: Maximum number of results
            
        Returns:
            List[Dict[str, Any]]: List of matching articles
        """
        try:
            # Define the search pipeline
            pipeline = [
                {
                    "$search": {
                        "index": "vector_search_index",
                        "knnBeta": {
                            "vector": embedding,
                            "path": "vector_embedding",
                            "k": limit
                        }
                    }
                },
                {"$match": {"status": "active"}}
            ]
            
            # Add project filter if specified
            if project_id:
                pipeline.append({"$match": {"project_ids": ObjectId(project_id)}})
            
            pipeline.append({"$limit": limit})
            
            # Execute the aggregation pipeline
            articles = list(self.db.articles.aggregate(pipeline))
            return articles
            
        except Exception as e:
            logger.error(f"Failed to perform vector search: {e}")
            logger.error("Note: Vector search requires MongoDB Atlas with Vector Search enabled")
            return []

    def mark_article_used_in_newsletter(self, article_id: str, newsletter_id: str) -> bool:
        """
        Mark an article as used in a newsletter
        
        Args:
            article_id: ID of the article
            newsletter_id: ID of the newsletter
            
        Returns:
            bool: True if update was successful, False otherwise
        """
        try:
            result = self.db.articles.update_one(
                {"_id": ObjectId(article_id)},
                {
                    "$addToSet": {"used_in_newsletters": ObjectId(newsletter_id)},
                    "$set": {"date_updated": datetime.datetime.utcnow()}
                }
            )
            return result.modified_count > 0
        except Exception as e:
            logger.error(f"Failed to mark article as used in newsletter: {e}")
            return False

    def close(self) -> None:
        """
        Close the MongoDB connection
        """
        if hasattr(self, 'client'):
            self.client.close()
            logger.info("MongoDB connection closed")


if __name__ == "__main__":
    # Example usage
    try:
        db = ProtonDB()
        logger.info("ProtonDB initialized successfully")
        
        # Example: Insert a test article
        test_article = {
            "title": "Test Article",
            "content": "This is a test article for the Proton CRM system.",
            "source_url": "https://example.com/test-article",
            "source_name": "Example News",
            "scraping_package_id": ObjectId(),  # Replace with actual scraping package ID
            "project_ids": [ObjectId()],  # Replace with actual project ID
            "date_scraped": datetime.datetime.utcnow(),
            "content_type": "news",
            "keywords": ["test", "proton", "example"]
        }
        
        article_id = db.insert_article(test_article)
        logger.info(f"Test article inserted with ID: {article_id}")
        
        # Close the connection
        db.close()
        
    except Exception as e:
        logger.error(f"Error in example usage: {e}")
</file>

<file path="requirements.txt">
flask==3.0.0
pymongo==4.5.0
dnspython==2.4.2
nltk==3.8.1
markupsafe==2.1.3
python-dateutil==2.8.2
python-dotenv>=0.21.1
requests>=2.28.2
beautifulsoup4>=4.11.2
numpy>=1.24.2
feedparser>=6.0.10
schedule==1.2.0
sentence-transformers>=2.2.2
scikit-learn==1.2.2
tqdm>=4.64.1
openai>=1.3.0
argparse>=1.4.0
gunicorn==21.2.0
</file>

<file path="run_scraper.py">
#!/usr/bin/env python
"""
Standalone script to run the scraper for all active packages.
Used for scheduled tasks like cron jobs, GitHub Actions, etc.
"""

import os
import sys
import logging
import datetime
from typing import List

# Import the database connection and scraper
from proton_db_setup import ProtonDB
from scraping_package_scheduler import ScrapePackageScheduler

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def run_all_active_scrapers():
    """Run all active scraping packages"""
    try:
        # Connect to database
        db = ProtonDB()
        logger.info("Connected to database")
        
        # Find all active packages
        packages = list(db.db.scraping_packages.find({"status": "active"}))
        logger.info(f"Found {len(packages)} active packages")
        
        # Run each package
        for package in packages:
            try:
                logger.info(f"Running package: {package['name']}")
                scheduler = ScrapePackageScheduler(db)
                scheduler.run_package(package['_id'])
                logger.info(f"Completed package: {package['name']}")
            except Exception as e:
                logger.error(f"Error running package {package['name']}: {str(e)}")
        
        logger.info("All packages completed")
        
    except Exception as e:
        logger.error(f"Error in scraper: {str(e)}")
        return False
    finally:
        # Close database connection
        if 'db' in locals():
            db.close()
    
    return True

if __name__ == "__main__":
    logger.info("Starting scheduled scraper run")
    success = run_all_active_scrapers()
    logger.info(f"Scraper run {'completed successfully' if success else 'failed'}")
    sys.exit(0 if success else 1)
</file>

<file path="run_scraping_system.py">
"""
Proton CRM Scraping System Runner

This script provides a command-line interface for running the scraping system:
- List all scraping packages
- Create a new test scraping package
- Run all active scraping packages once
- Run in scheduler mode
"""

import os
import sys
import time
import datetime
import logging
import argparse
from typing import Dict, Any, List, Optional
from bson.objectid import ObjectId

# Import ProtonDB for database operations
from proton_db_setup import ProtonDB

# Import scraping package modules
from scraping_package_model import ScrapingPackageModel
from scraping_package_scheduler import ScrapingPackage, ScrapingScheduler

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def list_packages():
    """List all scraping packages in the database"""
    db = ProtonDB()
    try:
        packages = list(db.db.scraping_packages.find())
        
        if not packages:
            print("No scraping packages found in the database")
            return
            
        print(f"Found {len(packages)} scraping package(s):")
        for idx, package in enumerate(packages, 1):
            print(f"\n{idx}. {package.get('name', 'Unnamed package')} (ID: {package['_id']})")
            print(f"   Description: {package.get('description', 'No description')}")
            print(f"   Feed count: {len(package.get('rss_feeds', []))}")
            print(f"   Schedule: {package.get('schedule_interval', 'Unknown')}")
            print(f"   Status: {package.get('status', 'Unknown')}")
            print(f"   Article count: {package.get('article_count', 0)}")
            
            # Show last run info if available
            if package.get('last_run'):
                # Format the datetime
                last_run = package['last_run']
                last_run_str = last_run.strftime("%Y-%m-%d %H:%M:%S") if hasattr(last_run, 'strftime') else str(last_run)
                print(f"   Last run: {last_run_str} ({package.get('articles_last_run', 0)} articles)")
    except Exception as e:
        logger.error(f"Error listing packages: {e}")
    finally:
        db.close()

def create_test_package():
    """Create a test scraping package"""
    package_model = ScrapingPackageModel()
    
    try:
        # Define a test package
        package_data = {
            "name": "Tech News Package",
            "description": "Combines tech news from The Verge, TechCrunch, and Wired",
            "project_ids": [ObjectId()],  # Generate a random project ID
            "rss_feeds": [
                "https://www.theverge.com/rss/index.xml",
                "https://feeds.feedburner.com/TechCrunch/",
                "https://www.wired.com/feed/rss"
            ],
            "schedule_interval": "1h",
            "max_articles_per_run": 5,
            "status": "active",
            "calculate_embeddings": True
        }
        
        # Create the package
        package_id = package_model.create_package(package_data)
        
        if package_id:
            print(f"Created test scraping package with ID: {package_id}")
            return package_id
        else:
            print("Failed to create test scraping package")
            return None
    except Exception as e:
        logger.error(f"Error creating test package: {e}")
        return None
    finally:
        package_model.close()

def load_packages() -> List[ScrapingPackage]:
    """
    Load all active scraping packages from the database
    
    Returns:
        List[ScrapingPackage]: List of active scraping package objects
    """
    db = ProtonDB()
    packages = []
    
    try:
        # Query all active packages
        package_docs = list(db.db.scraping_packages.find({"status": "active"}))
        
        if not package_docs:
            logger.info("No active scraping packages found in the database")
            return []
            
        logger.info(f"Loading {len(package_docs)} active scraping packages")
        
        # Create ScrapingPackage objects
        for package_doc in package_docs:
            try:
                package = ScrapingPackage(
                    package_id=str(package_doc["_id"]),
                    project_ids=[str(pid) for pid in package_doc.get("project_ids", [])],
                    name=package_doc.get("name", "Unnamed package"),
                    description=package_doc.get("description", ""),
                    rss_feeds=package_doc.get("rss_feeds", []),
                    schedule_interval=package_doc.get("schedule_interval", "1h"),
                    max_articles_per_run=package_doc.get("max_articles_per_run", 10),
                    calculate_embeddings=package_doc.get("calculate_embeddings", True)
                )
                packages.append(package)
            except Exception as e:
                logger.error(f"Error loading package {package_doc.get('_id')}: {e}")
        
        return packages
    except Exception as e:
        logger.error(f"Error loading packages: {e}")
        return []
    finally:
        db.close()

def update_package_stats(package_id: str, articles_processed: int) -> None:
    """
    Update package statistics after a run
    
    Args:
        package_id: ID of the package
        articles_processed: Number of articles processed in this run
    """
    db = ProtonDB()
    try:
        db.db.scraping_packages.update_one(
            {"_id": ObjectId(package_id)},
            {
                "$inc": {"article_count": articles_processed, "articles_last_run": articles_processed},
                "$set": {"last_run": datetime.datetime.utcnow()}
            }
        )
    except Exception as e:
        logger.error(f"Error updating package stats: {e}")
    finally:
        db.close()

def run_packages_once() -> None:
    """Run all active scraping packages once"""
    packages = load_packages()
    
    if not packages:
        print("No active packages to run")
        return
        
    print(f"Running {len(packages)} active scraping package(s)")
    
    for package in packages:
        try:
            print(f"Running package: {package.name}")
            article_ids = package.run()
            print(f"Processed {len(article_ids)} articles from {package.name}")
            
            # Update package statistics
            update_package_stats(package.package_id, len(article_ids))
        except Exception as e:
            logger.error(f"Error running package {package.name}: {e}")

def run_scheduler() -> None:
    """Run the scraping scheduler (continuous mode)"""
    packages = load_packages()
    
    if not packages:
        print("No active packages to run in scheduler mode")
        return
        
    print(f"Starting scheduler with {len(packages)} active package(s)")
    
    # Initialize scheduler
    scheduler = ScrapingScheduler()
    
    # Add packages to scheduler
    for package in packages:
        scheduler.add_package(package)
    
    # Start the scheduler
    scheduler.start()
    
    # Run all packages immediately
    for package in packages:
        scheduler.run_package_now(package.package_id)
    
    # Keep running until interrupted
    try:
        print("Scheduler running. Press Ctrl+C to stop...")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("Stopping scheduler...")
        scheduler.stop()
        print("Scheduler stopped.")

def refresh_relevance_scores(package_id=None):
    """
    Refresh relevance scores for all articles or for a specific package
    
    Args:
        package_id: Optional package ID to refresh scores for only articles from this package
    """
    db = ProtonDB()
    try:
        # Build the query
        query = {}
        if package_id:
            query["scraping_package_id"] = ObjectId(package_id)
            package_info = db.db.scraping_packages.find_one({"_id": ObjectId(package_id)})
            if not package_info:
                print(f"Package with ID {package_id} not found")
                return
            print(f"Refreshing relevance scores for articles from package: {package_info.get('name', 'Unknown')}")
        else:
            print("Refreshing relevance scores for all articles")
        
        # Get all articles matching the query
        articles = list(db.db.articles.find(query))
        print(f"Found {len(articles)} articles to update")
        
        # Process each article
        updated_count = 0
        for article in articles:
            try:
                # Skip articles without necessary fields
                if not article.get("content") or not article.get("scraping_package_id"):
                    continue
                
                # Get package information
                package_info = db.db.scraping_packages.find_one({"_id": article["scraping_package_id"]})
                if not package_info:
                    continue
                
                # Calculate new scores
                new_scores = calculate_article_relevance(
                    article, 
                    package_name=package_info.get("name", ""),
                    package_description=package_info.get("description", "")
                )
                
                # Update the article
                db.db.articles.update_one(
                    {"_id": article["_id"]},
                    {"$set": {"relevance_scores": new_scores}}
                )
                updated_count += 1
                
                if updated_count % 10 == 0:
                    print(f"Updated {updated_count}/{len(articles)} articles")
                
            except Exception as e:
                logger.error(f"Error updating article {article.get('_id')}: {e}")
        
        print(f"Successfully updated relevance scores for {updated_count} articles")
        
    except Exception as e:
        logger.error(f"Error refreshing relevance scores: {e}")
    finally:
        db.close()

def calculate_article_relevance(article, package_name, package_description):
    """
    Calculate relevance scores for an article based on package information
    
    Args:
        article: Article document
        package_name: Name of the scraping package
        package_description: Description of the scraping package
        
    Returns:
        Dict: Updated relevance scores
    """
    import datetime
    import numpy as np
    
    scores = {
        "overall": 0.75,  # Default score
        "recency": 0.0,   # Will be calculated based on publish date
        "package_fit": 0.8  # Default package fit score
    }
    
    # Calculate recency score (1.0 for today, decreasing with age)
    if "published_date" in article and article["published_date"]:
        now = datetime.datetime.now()
        age_days = (now - article["published_date"]).days
        
        # Exponential decay: score = exp(-age_days/14)
        # This gives ~0.95 for today, ~0.5 for 2 weeks old, ~0.25 for 1 month old
        recency = np.exp(-age_days/14) if age_days >= 0 else 0.95
        scores["recency"] = min(1.0, max(0.0, recency))
    else:
        # If no date, assume it's not very recent
        scores["recency"] = 0.5
    
    # Calculate package fit score (if we have content to analyze)
    content_to_check = article.get("original_content", article.get("content", ""))
    if content_to_check:
        # Look for package name and description keywords in the content
        package_keywords = set()
        
        # Add package name words
        for word in package_name.lower().split():
            if len(word) > 3:  # Only consider longer words
                package_keywords.add(word)
        
        # Add description words
        if package_description:
            for word in package_description.lower().split():
                if len(word) > 3:  # Only consider longer words
                    package_keywords.add(word)
        
        # Count matches in content
        content_lower = content_to_check.lower()
        match_count = sum(1 for word in package_keywords if word in content_lower)
        
        # Calculate package fit score
        if package_keywords:
            # Score is ratio of matched keywords to total keywords, with a minimum of 0.5
            package_fit = max(0.5, min(1.0, match_count / len(package_keywords)))
            scores["package_fit"] = package_fit
    
    # Adjust overall score based on recency and package fit
    scores["overall"] = 0.3 * scores["recency"] + 0.7 * scores["package_fit"]
    
    # Add persona-specific scores (placeholder for now)
    scores["persona_specific"] = {
        "general": scores["overall"]  # Default persona
    }
    
    return scores

def main():
    # Define command-line arguments
    parser = argparse.ArgumentParser(description="Proton CRM Scraping System Runner")
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    
    # List command
    list_parser = subparsers.add_parser("list", help="List all scraping packages")
    
    # Create command
    create_parser = subparsers.add_parser("create", help="Create a test scraping package")
    
    # Run command
    run_parser = subparsers.add_parser("run", help="Run scraping packages")
    run_parser.add_argument("--scheduler", action="store_true", help="Run in scheduler mode")
    
    # Refresh command
    refresh_parser = subparsers.add_parser("refresh", help="Refresh article relevance scores")
    refresh_parser.add_argument("--package", type=str, help="Optional package ID to refresh only articles from this package")
    
    # Parse arguments
    args = parser.parse_args()
    
    # Handle commands
    if args.command == "list":
        list_packages()
    elif args.command == "create":
        create_test_package()
    elif args.command == "run":
        if args.scheduler:
            run_scheduler()
        else:
            run_packages_once()
    elif args.command == "refresh":
        refresh_relevance_scores(args.package)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
</file>

<file path="sample_newsletter.json">
{
  "inputs": {
    "model_name": "gpt-4o",
    "date_range": "30days",
    "package_ids": ["64f7b1d3e2c28f9b9c8b4567", "64f7b1d3e2c28f9b9c8b4568"],
    "search_query": "healthcare innovation",
    "client_context": "A healthcare client interested in productization of healthcare innovations and direct services to individuals.",
    "project_context": "Newsletter focused on healthcare innovations, trends, and strategic alignment with client mission. Target audience is healthcare executives interested in consumer engagement, data-driven personalized care, and efficient, empathetic solutions. Each source must include properly formatted, clickable links to the original articles.",
    "prompt": "Create a professional newsletter for healthcare executives that highlights recent innovations and trends in healthcare technology, with a focus on patient engagement and personalized care. Include 3-5 key articles with brief summaries, and organize the content into clear sections with headers. Provide actionable insights and strategic implications for each trend discussed."
  },
  "outputs": {
    "content": "# Healthcare Innovation Insights\n\n**Weekly Newsletter for Healthcare Executives | April 2025**\n\n## Executive Summary\n\nThis week's newsletter focuses on breakthrough technologies in personalized care, AI-driven diagnostics, and patient engagement platforms that are reshaping healthcare delivery models. Recent innovations demonstrate significant potential for improving patient outcomes while optimizing operational efficiency.\n\n## Featured Innovations\n\n### AI-Powered Diagnostic Tools Reaching Clinical Maturity\n\nArtificial intelligence continues to transform diagnostic capabilities across multiple specialties. Recent studies show that AI algorithms now match or exceed specialist performance in several key areas:\n\n- Radiology AI systems demonstrating 97% accuracy in early cancer detection\n- Dermatology diagnostic tools reducing unnecessary biopsies by 31%\n- Cardiology predictive models identifying high-risk patients with 89% precision\n\n**Strategic Implications:** Healthcare organizations should evaluate departmental AI readiness and develop implementation roadmaps that address workflow integration, clinician training, and ethical guidelines. Early adopters are reporting 23% reductions in diagnostic time and 18% improvements in treatment plan optimization.\n\n### Patient Engagement Platforms Show Measurable Outcome Improvements\n\nDigital engagement solutions are demonstrating concrete ROI beyond simple satisfaction metrics. New research indicates properly implemented engagement platforms deliver:\n\n- 27% reduction in hospital readmissions for chronic condition management\n- 41% improvement in medication adherence\n- 35% increase in preventative care compliance\n\n**Strategic Implications:** Executives should prioritize platforms offering personalization capabilities, seamless EHR integration, and robust analytics. Success factors include thoughtful onboarding processes and care team workflow integration rather than technology sophistication alone.\n\n### Precision Medicine Approaches Becoming Operational Reality\n\nThe promise of precision medicine is transitioning from research to operational implementation. Leading organizations are developing scalable approaches to:\n\n- Pharmacogenomic testing integration into standard care protocols\n- Clinical decision support systems incorporating genetic factors\n- Population health stratification using biomarker data\n\n**Strategic Implications:** Organizations should assess their data infrastructure readiness and develop phased implementation plans focusing initially on high-impact conditions where precision approaches demonstrate clear outcome improvements and cost benefits.\n\n## Industry Trend Analysis\n\nThe convergence of AI diagnostics, engagement platforms, and precision medicine approaches is creating opportunities for truly personalized care delivery models. Organizations successfully integrating these technologies report significant competitive advantages in patient acquisition and retention, particularly among digitally-engaged demographics.\n\nRegulatory frameworks are evolving to accommodate these innovations, with recent FDA guidance streamlining approval pathways for AI-enabled medical devices and CMS reimbursement models increasingly supporting digital health interventions.\n\n## Conclusion\n\nHealthcare executives should prioritize creating organizational capabilities that can effectively evaluate, implement, and measure the impact of these emerging technologies. Success will require cross-functional collaboration between clinical, IT, and operational leadership to ensure innovations deliver meaningful improvements in care quality, patient experience, and operational efficiency.\n\n---\n\n*This newsletter is curated specifically for healthcare executives focused on innovation and strategic transformation. For more detailed analysis or implementation guidance, please contact our healthcare innovation team.*",
    "context_used": "CLIENT CONTEXT:\nA healthcare client interested in productization of healthcare innovations and direct services to individuals.\n\nPROJECT CONTEXT:\nNewsletter focused on healthcare innovations, trends, and strategic alignment with client mission. Target audience is healthcare executives interested in consumer engagement, data-driven personalized care, and efficient, empathetic solutions. Each source must include properly formatted, clickable links to the original articles.\n\nRELEVANT ARTICLES:\n[1] AI in Healthcare: Transforming Diagnosis and Treatment\nSource: Healthcare Technology Review\nURL: https://healthtechreview.com/ai-healthcare-transformation\nDate: 2025-03-15\nSummary: Comprehensive analysis of AI applications in clinical settings, with case studies showing 23% reduction in diagnostic time and 18% improvement in treatment optimization.\n\n[2] Patient Engagement Platforms: Beyond Satisfaction Metrics\nSource: Journal of Digital Health\nURL: https://jdigitalhealth.org/patient-engagement-roi\nDate: 2025-03-28\nSummary: Research study demonstrating 27% reduction in readmissions and 41% improvement in medication adherence through digital engagement platforms.\n\n[3] Precision Medicine Implementation Guide\nSource: Healthcare Innovation Today\nURL: https://healthcareinnovation.today/precision-medicine-guide\nDate: 2025-04-02\nSummary: Practical framework for operationalizing precision medicine approaches in healthcare organizations, with focus on pharmacogenomics and biomarker integration.\n\n[4] FDA Releases Updated Guidance for AI Medical Devices\nSource: MedTech Regulatory Review\nURL: https://medtechreview.com/fda-ai-guidance-2025\nDate: 2025-03-22\nSummary: Analysis of new regulatory pathways for AI-enabled medical devices and implications for development timelines and validation requirements.",
    "articles_count": 4
  }
}
</file>

<file path="vector_embeddings.py">
"""
Vector Embedding Utilities for Proton CRM

This module provides functions for generating vector embeddings using different models:
1. OpenAI API (requires API key)
2. SentenceTransformers (local model, no API key required)
"""

import os
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables from .env file
load_dotenv()

class EmbeddingGenerator:
    """Class for generating vector embeddings using different models"""
    
    def __init__(self, model_type: str = "local"):
        """
        Initialize the embedding generator
        
        Args:
            model_type: Type of model to use ('openai' or 'local')
        """
        self.model_type = model_type.lower()
        self.model = None
        
        if self.model_type == "openai":
            # Check for OpenAI API key
            self.api_key = os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                logger.warning("OpenAI API key not found in environment variables. Using local model instead.")
                self.model_type = "local"
                self._load_local_model()
            else:
                logger.info("Using OpenAI for embeddings")
                try:
                    import openai
                    self.client = openai.OpenAI(api_key=self.api_key)
                except ImportError:
                    logger.warning("OpenAI package not installed. Using local model instead.")
                    self.model_type = "local"
                    self._load_local_model()
        else:
            # Use local model
            self._load_local_model()
    
    def _load_local_model(self):
        """Load the local sentence transformer model"""
        try:
            from sentence_transformers import SentenceTransformer
            # Use MiniLM model (good balance of speed and quality)
            self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
            logger.info("Using SentenceTransformer for embeddings (local model)")
        except ImportError:
            logger.error("SentenceTransformer package not installed. Please install it with: pip install sentence-transformers")
            raise
    
    def get_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for the provided text
        
        Args:
            text: Text to generate embedding for
            
        Returns:
            List[float]: Vector embedding
        """
        if not text.strip():
            # Return zero vector if text is empty
            return [0.0] * (1536 if self.model_type == "openai" else 384)
        
        try:
            if self.model_type == "openai":
                return self._get_openai_embedding(text)
            else:
                return self._get_local_embedding(text)
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            # Return zero vector on error
            return [0.0] * (1536 if self.model_type == "openai" else 384)
    
    def _get_openai_embedding(self, text: str) -> List[float]:
        """Get embedding using OpenAI API"""
        try:
            # Truncate text if too long (OpenAI has token limits)
            truncated_text = text[:8000]  # Approximate token limit safety
            
            response = self.client.embeddings.create(
                input=truncated_text,
                model="text-embedding-3-small"  # Newer, better model
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            # Fall back to local model if OpenAI fails
            logger.info("Falling back to local model")
            if not self.model:
                self._load_local_model()
            return self._get_local_embedding(text)
    
    def _get_local_embedding(self, text: str) -> List[float]:
        """Get embedding using local SentenceTransformer model"""
        # Truncate text if too long (for memory reasons)
        truncated_text = text[:10000]
        embedding = self.model.encode(truncated_text)
        return embedding.tolist()

    def batch_get_embeddings(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        Generate embeddings for a batch of texts
        
        Args:
            texts: List of texts to generate embeddings for
            batch_size: Size of batches to process at once
            
        Returns:
            List[List[float]]: List of vector embeddings
        """
        results = []
        
        if self.model_type == "openai":
            # Process in batches to avoid rate limits
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                batch_results = []
                
                for text in batch:
                    batch_results.append(self.get_embedding(text))
                
                results.extend(batch_results)
                if i + batch_size < len(texts):
                    logger.info(f"Processed {i + batch_size}/{len(texts)} embeddings with OpenAI")
        else:
            # SentenceTransformer can handle batches natively
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                truncated_batch = [text[:10000] for text in batch]
                
                # Handle empty strings
                for j, text in enumerate(truncated_batch):
                    if not text.strip():
                        truncated_batch[j] = "empty"
                
                batch_embeddings = self.model.encode(truncated_batch)
                results.extend(batch_embeddings.tolist())
                
                if i + batch_size < len(texts):
                    logger.info(f"Processed {i + batch_size}/{len(texts)} embeddings with SentenceTransformer")
        
        return results


# Simple example usage
if __name__ == "__main__":
    # Test both embedding types
    test_text = "This is a test of the embedding system for Proton CRM."
    
    # Try OpenAI first
    try:
        openai_embedder = EmbeddingGenerator(model_type="openai")
        openai_embedding = openai_embedder.get_embedding(test_text)
        print(f"OpenAI embedding dimension: {len(openai_embedding)}")
        print(f"First 5 values: {openai_embedding[:5]}")
    except Exception as e:
        print(f"OpenAI embedding test failed: {e}")
    
    # Try local model
    try:
        local_embedder = EmbeddingGenerator(model_type="local")
        local_embedding = local_embedder.get_embedding(test_text)
        print(f"Local embedding dimension: {len(local_embedding)}")
        print(f"First 5 values: {local_embedding[:5]}")
    except Exception as e:
        print(f"Local embedding test failed: {e}")
</file>

<file path="vercel.json">
{
  "version": 2,
  "builds": [
    {
      "src": "wsgi.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/(.*)",
      "dest": "/wsgi.py"
    }
  ],
  "env": {
    "MONGODB_URI": "your-mongodb-connection-string"
  }
}
</file>

<file path="deploy/newsletter_agent/sample_newsletter.json">
{
  "inputs": {
    "model_name": "gpt-4.1",
    "date_range": "30days",
    "package_ids": ["64f7b1d3e2c28f9b9c8b4567", "64f7b1d3e2c28f9b9c8b4568"],
    "search_query": "healthcare innovation",
    "article_limit": "30",
    "client_context": "A healthcare client interested in productization of healthcare innovations and direct services to individuals.",
    "project_context": "Newsletter focused on healthcare innovations, trends, and strategic alignment with client mission. Target audience is healthcare executives interested in consumer engagement, data-driven personalized care, and efficient, empathetic solutions. Each source must include properly formatted, clickable links to the original articles.",
    "prompt": "Create a professional newsletter for healthcare executives that highlights recent innovations and trends in healthcare technology, with a focus on patient engagement and personalized care. Include 3-5 key articles with brief summaries, and organize the content into clear sections with headers. Provide actionable insights and strategic implications for each trend discussed."
  },
  "outputs": {
    "content": "# Healthcare Innovation Insights\n\n**Weekly Newsletter for Healthcare Executives | April 2025**\n\n## Executive Summary\n\nThis week's newsletter focuses on breakthrough technologies in personalized care, AI-driven diagnostics, and patient engagement platforms that are reshaping healthcare delivery models. Recent innovations demonstrate significant potential for improving patient outcomes while optimizing operational efficiency.\n\n## Featured Innovations\n\n### AI-Powered Diagnostic Tools Reaching Clinical Maturity\n\nArtificial intelligence continues to transform diagnostic capabilities across multiple specialties. Recent studies show that AI algorithms now match or exceed specialist performance in several key areas:\n\n- Radiology AI systems demonstrating 97% accuracy in early cancer detection\n- Dermatology diagnostic tools reducing unnecessary biopsies by 31%\n- Cardiology predictive models identifying high-risk patients with 89% precision\n\n**Strategic Implications:** Healthcare organizations should evaluate departmental AI readiness and develop implementation roadmaps that address workflow integration, clinician training, and ethical guidelines. Early adopters are reporting 23% reductions in diagnostic time and 18% improvements in treatment plan optimization.\n\n### Patient Engagement Platforms Show Measurable Outcome Improvements\n\nDigital engagement solutions are demonstrating concrete ROI beyond simple satisfaction metrics. New research indicates properly implemented engagement platforms deliver:\n\n- 27% reduction in hospital readmissions for chronic condition management\n- 41% improvement in medication adherence\n- 35% increase in preventative care compliance\n\n**Strategic Implications:** Executives should prioritize platforms offering personalization capabilities, seamless EHR integration, and robust analytics. Success factors include thoughtful onboarding processes and care team workflow integration rather than technology sophistication alone.\n\n### Precision Medicine Approaches Becoming Operational Reality\n\nThe promise of precision medicine is transitioning from research to operational implementation. Leading organizations are developing scalable approaches to:\n\n- Pharmacogenomic testing integration into standard care protocols\n- Clinical decision support systems incorporating genetic factors\n- Population health stratification using biomarker data\n\n**Strategic Implications:** Organizations should assess their data infrastructure readiness and develop phased implementation plans focusing initially on high-impact conditions where precision approaches demonstrate clear outcome improvements and cost benefits.\n\n## Industry Trend Analysis\n\nThe convergence of AI diagnostics, engagement platforms, and precision medicine approaches is creating opportunities for truly personalized care delivery models. Organizations successfully integrating these technologies report significant competitive advantages in patient acquisition and retention, particularly among digitally-engaged demographics.\n\nRegulatory frameworks are evolving to accommodate these innovations, with recent FDA guidance streamlining approval pathways for AI-enabled medical devices and CMS reimbursement models increasingly supporting digital health interventions.\n\n## Conclusion\n\nHealthcare executives should prioritize creating organizational capabilities that can effectively evaluate, implement, and measure the impact of these emerging technologies. Success will require cross-functional collaboration between clinical, IT, and operational leadership to ensure innovations deliver meaningful improvements in care quality, patient experience, and operational efficiency.\n\n---\n\n*This newsletter is curated specifically for healthcare executives focused on innovation and strategic transformation. For more detailed analysis or implementation guidance, please contact our healthcare innovation team.*",
    "context_used": "CLIENT CONTEXT:\nA healthcare client interested in productization of healthcare innovations and direct services to individuals.\n\nPROJECT CONTEXT:\nNewsletter focused on healthcare innovations, trends, and strategic alignment with client mission. Target audience is healthcare executives interested in consumer engagement, data-driven personalized care, and efficient, empathetic solutions. Each source must include properly formatted, clickable links to the original articles.\n\nRELEVANT ARTICLES:\n[1] AI in Healthcare: Transforming Diagnosis and Treatment\nSource: Healthcare Technology Review\nURL: https://healthtechreview.com/ai-healthcare-transformation\nDate: 2025-03-15\nSummary: Comprehensive analysis of AI applications in clinical settings, with case studies showing 23% reduction in diagnostic time and 18% improvement in treatment optimization.\n\n[2] Patient Engagement Platforms: Beyond Satisfaction Metrics\nSource: Journal of Digital Health\nURL: https://jdigitalhealth.org/patient-engagement-roi\nDate: 2025-03-28\nSummary: Research study demonstrating 27% reduction in readmissions and 41% improvement in medication adherence through digital engagement platforms.\n\n[3] Precision Medicine Implementation Guide\nSource: Healthcare Innovation Today\nURL: https://healthcareinnovation.today/precision-medicine-guide\nDate: 2025-04-02\nSummary: Practical framework for operationalizing precision medicine approaches in healthcare organizations, with focus on pharmacogenomics and biomarker integration.\n\n[4] FDA Releases Updated Guidance for AI Medical Devices\nSource: MedTech Regulatory Review\nURL: https://medtechreview.com/fda-ai-guidance-2025\nDate: 2025-03-22\nSummary: Analysis of new regulatory pathways for AI-enabled medical devices and implications for development timelines and validation requirements.",
    "articles_count": 4
  }
}
</file>

<file path="deploy/scraper/add_feed.py">
#!/usr/bin/env python
"""
Add RSS feeds to Proton scraping packages

This script adds RSS feeds to existing packages in the database.
It can be run manually to add or update feeds.
"""

import os
import sys
import logging
import datetime
import traceback
from typing import List, Dict, Any, Optional
import pymongo
from bson.objectid import ObjectId
import dns.resolver
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

def add_feed_to_package(
    package_id: str,
    feed_url: str,
    feed_name: str,
    client: pymongo.MongoClient,
    db_name: str = "proton"
) -> bool:
    """
    Add a feed to a scraping package by adding it to the rss_feeds array
    
    Args:
        package_id: The ObjectId of the package (as string)
        feed_url: The URL of the RSS feed
        feed_name: A descriptive name for the feed
        client: MongoDB client
        db_name: Database name
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        # Get the database
        db = client[db_name]
        
        # Check if the package exists
        package = db.scraping_packages.find_one({"_id": ObjectId(package_id)})
        if not package:
            logger.error(f"Package with ID {package_id} not found")
            return False
        
        # Get existing rss_feeds array
        rss_feeds = package.get("rss_feeds", [])
        
        # Standardize the rss_feeds list
        standardized_feeds = []
        for feed in rss_feeds:
            # Handle string URLs or dictionary entries
            if isinstance(feed, str):
                standardized_feeds.append({
                    "name": feed.split("/")[-1],
                    "url": feed,
                    "created_date": datetime.datetime.utcnow()
                })
            elif isinstance(feed, dict):
                standardized_feeds.append(feed)
        
        # Check if this feed already exists in the package
        feed_exists = False
        for i, feed in enumerate(standardized_feeds):
            # Check if feed is a string or a dict with url
            feed_url_to_check = feed.get("url") if isinstance(feed, dict) else feed
            
            if feed_url_to_check == feed_url:
                # Update the existing feed
                standardized_feeds[i] = {
                    "name": feed_name,
                    "url": feed_url,
                    "updated_date": datetime.datetime.utcnow()
                }
                feed_exists = True
                break
        
        if feed_exists:
            # Update the package with the modified feeds array
            result = db.scraping_packages.update_one(
                {"_id": ObjectId(package_id)},
                {"$set": {"rss_feeds": standardized_feeds}}
            )
            logger.info(f"Updated existing feed: {feed_name} ({feed_url}) in package")
            return True
        else:
            # Create a new feed entry
            new_feed = {
                "name": feed_name,
                "url": feed_url,
                "created_date": datetime.datetime.utcnow()
            }
            
            # Add to the rss_feeds array
            result = db.scraping_packages.update_one(
                {"_id": ObjectId(package_id)},
                {"$push": {"rss_feeds": new_feed}}
            )
            
            logger.info(f"Added new feed: {feed_name} ({feed_url}) to package")
            
            # Also increment the package's feed count if that field exists
            if "feed_count" in package:
                db.scraping_packages.update_one(
                    {"_id": ObjectId(package_id)},
                    {"$inc": {"feed_count": 1}}
                )
            
            return True
    
    except Exception as e:
        logger.error(f"Error adding feed: {str(e)}")
        return False

def main():
    """Main function to add feeds to packages"""
    try:
        # Get MongoDB URI from environment variable
        mongodb_uri = os.environ.get('MONGODB_URI')
        if not mongodb_uri:
            logger.error("MONGODB_URI environment variable not set!")
            sys.exit(1)
        
        # Get database name from environment or use default
        db_name = os.environ.get('MONGODB_DB_NAME', 'proton')
        
        # Connect to MongoDB
        logger.info(f"Connecting to MongoDB with URI: {mongodb_uri[:20]}***")
        
        # Handle DNS resolution issues with MongoDB+SRV URIs
        if mongodb_uri.startswith('mongodb+srv://'):
            logger.info("Using MongoDB+SRV connection string, configuring DNS resolver")
            try:
                # Configure resolver with Google's DNS server
                dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
                dns.resolver.default_resolver.nameservers = ['8.8.8.8']  # Google DNS server
            except Exception as dns_error:
                logger.warning(f"Failed to configure DNS resolver: {dns_error}")
        
        client = pymongo.MongoClient(mongodb_uri)
        client.admin.command('ping')
        logger.info("Successfully connected to MongoDB")
        
        # Define packages and feeds
        packages_and_feeds = [
            # Tech News Package
            {
                "package_id": "67e59bd4f85c14b4d2896825",  # Tech News Package
                "feeds": [
                    {"url": "https://feeds.feedburner.com/TechCrunch", "name": "TechCrunch"},
                    {"url": "https://www.theverge.com/rss/index.xml", "name": "The Verge"},
                    {"url": "https://www.wired.com/feed/rss", "name": "Wired"},
                    {"url": "https://hnrss.org/newest", "name": "Hacker News"},
                    {"url": "https://www.cnet.com/rss/news/", "name": "CNET"},
                    {"url": "https://venturebeat.com/feed/", "name": "VentureBeat"}
                ]
            },
            # General Business & Strategy
            {
                "package_id": "67e5a3bf75087b73ac722ce0",  # General Business & Strategy
                "feeds": [
                    {"url": "https://hbr.org/feed", "name": "Harvard Business Review"},
                    {"url": "https://www.forbes.com/business/feed/", "name": "Forbes Business"},
                    {"url": "https://www.fastcompany.com/feed", "name": "Fast Company"}
                ]
            },
            # IoT and Connectivity
            {
                "package_id": "67e5b1aa252d081c9f5000ea",  # IoT and Connectivity
                "feeds": [
                    {"url": "https://www.iotforall.com/feed", "name": "IoT For All"},
                    {"url": "https://www.iotworldtoday.com/feed", "name": "IoT World Today"},
                    {"url": "https://staceyoniot.com/feed/", "name": "Stacey on IoT"}
                ]
            }
        ]
        
        # Add feeds to packages
        for package_data in packages_and_feeds:
            package_id = package_data["package_id"]
            feeds = package_data["feeds"]
            
            # Check if the package exists
            db = client[db_name]
            package = db.scraping_packages.find_one({"_id": ObjectId(package_id)})
            if not package:
                logger.warning(f"Package with ID {package_id} not found, skipping")
                continue
                
            logger.info(f"Adding feeds to package: {package.get('name', 'Unknown')} ({package_id})")
            
            # Add each feed
            for feed in feeds:
                success = add_feed_to_package(
                    package_id=package_id,
                    feed_url=feed["url"],
                    feed_name=feed["name"],
                    client=client,
                    db_name=db_name
                )
                
                if success:
                    logger.info(f"Successfully added feed: {feed['name']} to package: {package.get('name', 'Unknown')}")
                else:
                    logger.error(f"Failed to add feed: {feed['name']} to package: {package.get('name', 'Unknown')}")
        
        logger.info("Feed addition complete")
    
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        traceback.print_exc()
    finally:
        if 'client' in locals():
            client.close()
            logger.info("MongoDB connection closed")

if __name__ == "__main__":
    main()
</file>

<file path="deploy/web/article_viewer_simple.py">
#!/usr/bin/env python
"""
Simplified article viewer web application for Render.com deployment.
This version doesn't use any NLTK functionality to avoid deployment issues.
"""

import os
import datetime
import math
from typing import List, Dict, Any

from flask import Flask, render_template, request, jsonify, redirect, url_for, flash
from markupsafe import Markup
from bson.objectid import ObjectId
import pymongo

# Configure logging
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__, template_folder='templates')
app.secret_key = os.urandom(24)  # For flash messages

# Initialize global database connection
db = None

# Add context processor for current datetime
@app.context_processor
def inject_now():
    return {'now': datetime.datetime.now()}

class SimpleProtonDB:
    """Simplified version of ProtonDB connection"""
    
    def __init__(self):
        """Initialize database connection"""
        self.mongo_uri = os.environ.get('MONGODB_URI')
        if not self.mongo_uri:
            logger.error("MONGODB_URI environment variable not set!")
            raise ValueError("MONGODB_URI environment variable not set")
        
        logger.info(f"Attempting to connect to MongoDB with URI: {self.mongo_uri[:20]}***")
        try:
            # Handle DNS resolution issues with MongoDB+SRV URIs
            if self.mongo_uri.startswith('mongodb+srv://'):
                logger.info("Using MongoDB+SRV connection string, configuring DNS resolver")
                # Configure DNS resolver for pymongo
                import dns.resolver
                dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
                dns.resolver.default_resolver.nameservers = ['8.8.8.8', '8.8.4.4']  # Google's public DNS
            
            self.client = pymongo.MongoClient(self.mongo_uri)
            # Force a command to check the connection
            self.client.admin.command('ping')
            self.db = self.client.proton
            logger.info("Successfully connected to MongoDB")
        except Exception as e:
            logger.error(f"Failed to connect to MongoDB: {str(e)}")
            raise
    
    def close(self):
        """Close database connection"""
        if hasattr(self, 'client'):
            self.client.close()
            logger.info("Closed MongoDB connection")

def get_db():
    """Get database connection, creating it if it doesn't exist"""
    global db
    if db is None:
        db = SimpleProtonDB()
    return db

@app.route('/')
def index():
    """Redirect to articles page"""
    return redirect(url_for('list_articles'))

@app.route('/articles')
def list_articles():
    """List all articles with pagination"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 10, type=int)
    package_id = request.args.get('package')
    search_query = request.args.get('q')
    sort_by = request.args.get('sort', 'date')
    sort_order = request.args.get('order', 'desc')
    
    # Get database connection
    db_conn = get_db()
    
    # Build query
    query = {}
    if package_id:
        query['scraping_package_id'] = ObjectId(package_id)
    
    if search_query:
        query['$text'] = {'$search': search_query}
    
    # Get packages for filter dropdown
    packages = list(db_conn.db.scraping_packages.find({}, {'name': 1}))
    
    # Build sort query
    sort_query = []
    if sort_by == 'date':
        sort_query.append(('published_date', -1 if sort_order == 'desc' else 1))
    elif sort_by == 'relevance':
        sort_query.append(('relevance_scores.overall', -1 if sort_order == 'desc' else 1))
    elif sort_by == 'title':
        sort_query.append(('title', 1 if sort_order == 'asc' else -1))
    
    # Add secondary sort by date
    if sort_by != 'date':
        sort_query.append(('published_date', -1))
    
    # Count total articles
    total_articles = db_conn.db.articles.count_documents(query)
    total_pages = math.ceil(total_articles / per_page)
    
    # Adjust page number to be within bounds
    page = max(1, min(page, total_pages)) if total_pages > 0 else 1
    
    # Get articles
    skip = (page - 1) * per_page
    articles = list(db_conn.db.articles.find(query).sort(sort_query).skip(skip).limit(per_page))
    
    # Format dates for display
    for article in articles:
        if 'published_date' in article and article['published_date']:
            article['display_date'] = article['published_date'].strftime('%Y-%m-%d %H:%M')
        else:
            article['display_date'] = 'Unknown'
    
    # Render template
    return render_template(
        'articles.html',
        articles=articles,
        packages=packages,
        current_page=page,
        total_pages=total_pages,
        total_articles=total_articles,
        per_page=per_page,
        package_id=package_id,
        search_query=search_query,
        sort_by=sort_by,
        sort_order=sort_order,
        now=datetime.datetime.now()
    )

@app.route('/articles/<article_id>')
def view_article(article_id):
    """View a single article"""
    db_conn = get_db()
    article = db_conn.db.articles.find_one({'_id': ObjectId(article_id)})
    
    if not article:
        flash('Article not found')
        return redirect(url_for('list_articles'))
    
    # Get package info if available
    package = None
    if 'scraping_package_id' in article:
        package = db_conn.db.scraping_packages.find_one({'_id': article['scraping_package_id']})
    
    # Format date for display
    if 'published_date' in article and article['published_date']:
        article['display_date'] = article['published_date'].strftime('%Y-%m-%d %H:%M')
    else:
        article['display_date'] = 'Unknown'
    
    # Convert content to Markup to preserve HTML
    if 'content' in article:
        article['display_content'] = Markup(article['content'])
    
    return render_template('article.html', article=article, package=package, now=datetime.datetime.now())

@app.route('/packages')
def list_packages():
    """List all scraping packages"""
    db_conn = get_db()
    packages = list(db_conn.db.scraping_packages.find())
    
    # Format dates for display
    for package in packages:
        if 'last_run' in package and package['last_run']:
            package['last_run_display'] = package['last_run'].strftime('%Y-%m-%d %H:%M')
        else:
            package['last_run_display'] = 'Never'
    
    return render_template('packages.html', packages=packages, now=datetime.datetime.now())

@app.route('/packages/<package_id>')
def view_package(package_id):
    """View a single package and its articles"""
    db_conn = get_db()
    package = db_conn.db.scraping_packages.find_one({'_id': ObjectId(package_id)})
    
    if not package:
        flash('Package not found')
        return redirect(url_for('list_packages'))
    
    # Get recent articles for this package
    articles = list(db_conn.db.articles.find(
        {'scraping_package_id': ObjectId(package_id)}
    ).sort('published_date', -1).limit(20))
    
    # Format dates for display
    for article in articles:
        if 'published_date' in article and article['published_date']:
            article['display_date'] = article['published_date'].strftime('%Y-%m-%d %H:%M')
        else:
            article['display_date'] = 'Unknown'
    
    return render_template('package.html', package=package, articles=articles, now=datetime.datetime.now())

@app.template_filter('truncate_html')
def truncate_html(html, length=200):
    """Truncate HTML content to a certain length"""
    if not html:
        return ""
    
    # Simple truncation for this simplified version
    text = str(html)
    if len(text) <= length:
        return Markup(text)
    
    return Markup(f"{text[:length]}...")

# Create templates when app starts
def create_templates():
    """Create Flask templates for the application"""
    # Create templates directory if it doesn't exist
    if not os.path.exists('templates'):
        os.makedirs('templates')
    
    # Create templates
    with open('templates/base.html', 'w', encoding='utf-8') as f:
        f.write('''<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{% block title %}Proton CRM Article Viewer{% endblock %}</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        .article-card {
            height: 100%;
            display: flex;
            flex-direction: column;
        }
        .article-card .card-body {
            flex: 1;
        }
        .metadata {
            font-size: 0.85rem;
            color: #6c757d;
        }
        .entities span {
            display: inline-block;
            margin-right: 5px;
            margin-bottom: 5px;
            padding: 2px 8px;
            background-color: #e9ecef;
            border-radius: 4px;
            font-size: 0.85rem;
        }
        .keywords span {
            display: inline-block;
            margin-right: 5px;
            margin-bottom: 5px;
            padding: 2px 8px;
            background-color: #cfe2ff;
            border-radius: 4px;
            font-size: 0.85rem;
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark mb-4">
        <div class="container">
            <a class="navbar-brand" href="{{ url_for('index') }}">Proton CRM</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('list_articles') }}">Articles</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('list_packages') }}">Packages</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container">
        {% with messages = get_flashed_messages() %}
          {% if messages %}
            {% for message in messages %}
              <div class="alert alert-info mb-3">{{ message }}</div>
            {% endfor %}
          {% endif %}
        {% endwith %}

        {% block content %}{% endblock %}
    </div>

    <footer class="mt-5 py-3 text-center text-secondary">
        <div class="container">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>''')
    
    with open('templates/articles.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}Articles - Proton CRM{% endblock %}

{% block content %}
<div class="row mb-4">
    <div class="col-md-8">
        <h1>Articles</h1>
    </div>
    <div class="col-md-4">
        <form action="{{ url_for('list_articles') }}" method="get" class="d-flex">
            <input type="text" name="q" class="form-control me-2" placeholder="Search articles..." value="{{ search_query or '' }}">
            <button type="submit" class="btn btn-primary">Search</button>
        </form>
    </div>
</div>

<div class="row mb-4">
    <div class="col-md-3">
        <div class="card">
            <div class="card-header">
                Filters
            </div>
            <div class="card-body">
                <form action="{{ url_for('list_articles') }}" method="get">
                    {% if search_query %}
                    <input type="hidden" name="q" value="{{ search_query }}">
                    {% endif %}
                    
                    <div class="mb-3">
                        <label class="form-label">Package</label>
                        <select name="package" class="form-select">
                            <option value="">All Packages</option>
                            {% for package in packages %}
                            <option value="{{ package._id }}" {% if package_id == package._id|string %}selected{% endif %}>
                                {{ package.name }}
                            </option>
                            {% endfor %}
                        </select>
                    </div>
                    
                    <div class="mb-3">
                        <label class="form-label">Sort By</label>
                        <select name="sort" class="form-select">
                            <option value="date" {% if sort_by == 'date' %}selected{% endif %}>Date</option>
                            <option value="relevance" {% if sort_by == 'relevance' %}selected{% endif %}>Relevance</option>
                            <option value="title" {% if sort_by == 'title' %}selected{% endif %}>Title</option>
                        </select>
                    </div>
                    
                    <div class="mb-3">
                        <label class="form-label">Order</label>
                        <select name="order" class="form-select">
                            <option value="desc" {% if sort_order == 'desc' %}selected{% endif %}>Descending</option>
                            <option value="asc" {% if sort_order == 'asc' %}selected{% endif %}>Ascending</option>
                        </select>
                    </div>
                    
                    <button type="submit" class="btn btn-primary w-100">Apply Filters</button>
                </form>
            </div>
        </div>
    </div>
    <div class="col-md-9">
        {% if articles %}
            <div class="row row-cols-1 row-cols-md-2 g-4">
                {% for article in articles %}
                <div class="col">
                    <div class="card article-card h-100">
                        <div class="card-body">
                            <h5 class="card-title">
                                <a href="{{ url_for('view_article', article_id=article._id) }}" class="text-decoration-none">
                                    {{ article.title }}
                                </a>
                            </h5>
                            <p class="metadata">
                                <span class="me-2">{{ article.display_date }}</span>
                                {% if article.source %}
                                <span class="me-2">Source: {{ article.source }}</span>
                                {% endif %}
                            </p>
                            <p class="card-text">
                                {% if article.summary %}
                                    {{ article.summary|truncate_html }}
                                {% elif article.content %}
                                    {{ article.content|truncate_html }}
                                {% endif %}
                            </p>
                        </div>
                        <div class="card-footer">
                            <div class="keywords">
                                {% if article.keywords %}
                                    {% for keyword in article.keywords[:5] %}
                                    <span>{{ keyword }}</span>
                                    {% endfor %}
                                {% endif %}
                            </div>
                        </div>
                    </div>
                </div>
                {% endfor %}
            </div>
            
            <!-- Pagination -->
            {% if total_pages > 1 %}
            <nav class="mt-4">
                <ul class="pagination justify-content-center">
                    <li class="page-item {% if current_page == 1 %}disabled{% endif %}">
                        <a class="page-link" href="{{ url_for('list_articles', page=current_page-1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">Previous</a>
                    </li>
                    
                    {% set start_page = [current_page - 2, 1]|max %}
                    {% set end_page = [start_page + 4, total_pages]|min %}
                    {% set start_page = [end_page - 4, 1]|max %}
                    
                    {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {% if p == current_page %}active{% endif %}">
                        <a class="page-link" href="{{ url_for('list_articles', page=p, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">{{ p }}</a>
                    </li>
                    {% endfor %}
                    
                    <li class="page-item {% if current_page == total_pages %}disabled{% endif %}">
                        <a class="page-link" href="{{ url_for('list_articles', page=current_page+1, per_page=per_page, package=package_id, q=search_query, sort=sort_by, order=sort_order) }}">Next</a>
                    </li>
                </ul>
            </nav>
            {% endif %}
            
            <div class="mt-3 text-center text-secondary">
                Showing {{ articles|length }} of {{ total_articles }} articles
            </div>
        {% else %}
            <div class="alert alert-info">
                No articles found. Try changing your search or filter criteria.
            </div>
        {% endif %}
    </div>
</div>
{% endblock %}''')
    
    with open('templates/article.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}{{ article.title }} - Proton CRM{% endblock %}

{% block content %}
<nav aria-label="breadcrumb">
    <ol class="breadcrumb">
        <li class="breadcrumb-item"><a href="{{ url_for('list_articles') }}">Articles</a></li>
        <li class="breadcrumb-item active">{{ article.title }}</li>
    </ol>
</nav>

<article>
    <h1 class="mb-3">{{ article.title }}</h1>
    
    <div class="metadata mb-4">
        <div class="row">
            <div class="col-md-6">
                <p>
                    <strong>Date:</strong> {{ article.display_date }}<br>
                    {% if article.source %}
                    <strong>Source:</strong> {{ article.source }}<br>
                    {% endif %}
                    {% if article.author %}
                    <strong>Author:</strong> {{ article.author }}<br>
                    {% endif %}
                    {% if article.url %}
                    <strong>URL:</strong> <a href="{{ article.url }}" target="_blank">Original Article</a><br>
                    {% endif %}
                </p>
            </div>
            <div class="col-md-6">
                <p>
                    {% if package %}
                    <strong>Package:</strong> <a href="{{ url_for('view_package', package_id=package._id) }}">{{ package.name }}</a><br>
                    {% endif %}
                    {% if article.relevance_scores and article.relevance_scores.overall %}
                    <strong>Relevance Score:</strong> {{ (article.relevance_scores.overall * 100)|round(1) }}%<br>
                    {% endif %}
                </p>
            </div>
        </div>
    </div>
    
    {% if article.summary %}
    <div class="card mb-4">
        <div class="card-header">
            <h5 class="mb-0">Summary</h5>
        </div>
        <div class="card-body">
            <p>{{ article.summary }}</p>
        </div>
    </div>
    {% endif %}
    
    <div class="mb-4">
        <div class="row">
            {% if article.keywords %}
            <div class="col-md-6 mb-3">
                <h5>Keywords</h5>
                <div class="keywords">
                    {% for keyword in article.keywords %}
                    <span>{{ keyword }}</span>
                    {% endfor %}
                </div>
            </div>
            {% endif %}
            
            {% if article.entities %}
            <div class="col-md-6 mb-3">
                <h5>Entities</h5>
                <div class="entities">
                    {% for entity in article.entities %}
                    <span>{{ entity.text }} ({{ entity.type }})</span>
                    {% endfor %}
                </div>
            </div>
            {% endif %}
        </div>
    </div>
    
    <div class="card mb-4">
        <div class="card-header">
            <h5 class="mb-0">Content</h5>
        </div>
        <div class="card-body">
            {% if article.display_content %}
            {{ article.display_content }}
            {% else %}
            <p class="text-secondary">No content available</p>
            {% endif %}
        </div>
    </div>
</article>

<div class="mt-4">
    <a href="{{ url_for('list_articles') }}" class="btn btn-secondary">Back to Articles</a>
</div>
{% endblock %}''')
    
    with open('templates/packages.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}Scraping Packages - Proton CRM{% endblock %}

{% block content %}
<div class="row mb-4">
    <div class="col-md-8">
        <h1>Scraping Packages</h1>
    </div>
</div>

{% if packages %}
<div class="table-responsive">
    <table class="table table-striped table-hover">
        <thead>
            <tr>
                <th>Name</th>
                <th>Description</th>
                <th>Status</th>
                <th>Last Run</th>
                <th>Actions</th>
            </tr>
        </thead>
        <tbody>
            {% for package in packages %}
            <tr>
                <td>{{ package.name }}</td>
                <td>{{ package.description }}</td>
                <td>
                    <span class="badge {% if package.status == 'active' %}bg-success{% else %}bg-secondary{% endif %}">
                        {{ package.status }}
                    </span>
                </td>
                <td>{{ package.last_run_display }}</td>
                <td>
                    <a href="{{ url_for('view_package', package_id=package._id) }}" class="btn btn-sm btn-primary">View</a>
                </td>
            </tr>
            {% endfor %}
        </tbody>
    </table>
</div>
{% else %}
<div class="alert alert-info">
    No scraping packages found.
</div>
{% endif %}
{% endblock %}''')
    
    with open('templates/package.html', 'w', encoding='utf-8') as f:
        f.write('''{% extends "base.html" %}

{% block title %}{{ package.name }} - Proton CRM{% endblock %}

{% block content %}
<nav aria-label="breadcrumb">
    <ol class="breadcrumb">
        <li class="breadcrumb-item"><a href="{{ url_for('list_packages') }}">Packages</a></li>
        <li class="breadcrumb-item active">{{ package.name }}</li>
    </ol>
</nav>

<div class="card mb-4">
    <div class="card-header">
        <h1 class="h3 mb-0">{{ package.name }}</h1>
    </div>
    <div class="card-body">
        <div class="row">
            <div class="col-md-8">
                <p><strong>Description:</strong> {{ package.description }}</p>
                <p><strong>Status:</strong> <span class="badge {% if package.status == 'active' %}bg-success{% else %}bg-secondary{% endif %}">{{ package.status }}</span></p>
                <p><strong>Last Run:</strong> {{ package.last_run_display }}</p>
            </div>
            <div class="col-md-4">
                <div class="card">
                    <div class="card-header">Schedule</div>
                    <div class="card-body">
                        {% if package.schedule %}
                        <p><strong>Frequency:</strong> {{ package.schedule.frequency }}</p>
                        <p><strong>Time:</strong> {{ package.schedule.time }}</p>
                        {% else %}
                        <p>No schedule configured</p>
                        {% endif %}
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<h2 class="mb-3">Recent Articles</h2>

{% if articles %}
<div class="row row-cols-1 row-cols-md-3 g-4">
    {% for article in articles %}
    <div class="col">
        <div class="card h-100">
            <div class="card-body">
                <h5 class="card-title">
                    <a href="{{ url_for('view_article', article_id=article._id) }}" class="text-decoration-none">
                        {{ article.title }}
                    </a>
                </h5>
                <p class="metadata">{{ article.display_date }}</p>
                <p class="card-text">
                    {% if article.summary %}
                        {{ article.summary|truncate_html(100) }}
                    {% elif article.content %}
                        {{ article.content|truncate_html(100) }}
                    {% endif %}
                </p>
            </div>
            <div class="card-footer">
                <a href="{{ url_for('view_article', article_id=article._id) }}" class="btn btn-sm btn-primary">View Article</a>
            </div>
        </div>
    </div>
    {% endfor %}
</div>

<div class="mt-4">
    <a href="{{ url_for('list_articles', package=package._id) }}" class="btn btn-primary">View All Articles</a>
</div>
{% else %}
<div class="alert alert-info">
    No articles found for this package.
</div>
{% endif %}

<div class="mt-4">
    <a href="{{ url_for('list_packages') }}" class="btn btn-secondary">Back to Packages</a>
</div>
{% endblock %}''')
    
    logger.info("Created article viewer and templates")

# Create templates on module import
create_templates()

if __name__ == "__main__":
    app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
</file>

<file path="deploy/web/wsgi.py">
"""
WSGI entry point for Render deployment - Web version
"""
import os
import logging
import sys

# Add the current directory to the system path to make imports work
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Create a simple Flask app without NLTK dependencies
from flask import Flask

# Create a placeholder app that will be used if there are import errors
app = Flask(__name__)

@app.route('/')
def home():
    return """
    <html>
        <head>
            <title>Proton CRM Article Viewer</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
                h1 { color: #333; }
                .message { background: #f8f9fa; padding: 20px; border-radius: 5px; border-left: 5px solid #28a745; }
            </style>
        </head>
        <body>
            <h1>Proton CRM Article Viewer</h1>
            <div class="message">
                <p>The application is starting up. Please wait a moment and refresh the page.</p>
                <p>If this message persists, please check the application logs for errors.</p>
            </div>
        </body>
    </html>
    """

# Import the app from the simplified viewer
try:
    # Try the simplified version first with direct import (no module path)
    logger.info("Attempting to import article_viewer_simple...")
    from article_viewer_simple import app as real_app
    app = real_app
    logger.info("Successfully imported article_viewer_simple")
except Exception as e:
    logger.error(f"Error importing simplified app directly: {str(e)}")
    
    # Try with module path
    try:
        logger.info("Attempting to import with module path...")
        from deploy.web.article_viewer_simple import app as real_app
        app = real_app
        logger.info("Successfully imported deploy.web.article_viewer_simple")
    except Exception as e:
        logger.error(f"Error importing with module path: {str(e)}")
        
        # Fallback to regular import
        try:
            logger.info("Attempting fallback import of article_viewer...")
            from article_viewer import app as full_app
            app = full_app
            logger.info("Successfully imported article_viewer")
        except Exception as e:
            logger.error(f"Error importing full app: {str(e)}")
            logger.info("Using placeholder app instead")

# This is for Render deployment
if __name__ == "__main__":
    port = int(os.environ.get('PORT', 10000))
    logger.info(f"Starting application on port {port}")
    app.run(host='0.0.0.0', port=port)
</file>

<file path="templates/base.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{% block title %}Agent Newsletter Generator{% endblock %}</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .form-group { margin-bottom: 1rem; }
        #loading { display: none; }
        #result { display: none; }
        .card { margin-bottom: 20px; }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <h1>{% block header %}Agent Newsletter Generator{% endblock %}</h1>
        </header>

        {% with messages = get_flashed_messages() %}
          {% if messages %}
            {% for message in messages %}
              <div class="alert alert-info mb-3">{{ message }}</div>
            {% endfor %}
          {% endif %}
        {% endwith %}

        {% block content %}{% endblock %}

        <footer class="mt-5 py-3 text-center text-secondary">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    {% block scripts %}{% endblock %}
</body>
</html>
</file>

<file path="templates/final_index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agent Newsletter Generator</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .form-group { margin-bottom: 1rem; }
        #loading { display: none; }
        #streaming-window { display: none; }
        #result { display: none; }
        .card { margin-bottom: 20px; }

        /* Styling for the agent thoughts */
        #streaming-content {
            height: 400px;
            overflow-y: auto;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 0.25rem;
            padding: 1rem;
            font-family: monospace;
            line-height: 1.4;
            font-size: 14px;
            color: #333;
            white-space: pre; /* Use pre to preserve whitespace */
        }

        /* Styling for the generated newsletter */
        #generated-content {
            line-height: 1.6;
            font-size: 16px;
        }

        /* Add some spacing between paragraphs in the newsletter */
        #generated-content p {
            margin-bottom: 1rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <h1>Agent Newsletter Generator</h1>
        </header>

        <div class="row">
            <div class="col-md-12">
                <div class="card">
                    <div class="card-header">
                        <h2>Generate Newsletter with AI Agent</h2>
                    </div>
                    <div class="card-body">
                        <form id="newsletter-form" method="post" enctype="multipart/form-data">
                            <div class="row">
                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="model_name"><strong>Model:</strong></label>
                                        <select class="form-control" id="model_name" name="model_name">
                                            {% for model in openai_models %}
                                                <option value="{{ model }}">OpenAI: {{ model }}</option>
                                            {% endfor %}
                                        </select>
                                    </div>

                                    <div class="form-group">
                                        <label for="date_range"><strong>Date Range:</strong></label>
                                        <select class="form-control" id="date_range" name="date_range">
                                            <option value="all">All Time</option>
                                            <option value="7days">Last 7 Days</option>
                                            <option value="30days">Last 30 Days</option>
                                            <option value="90days">Last 90 Days</option>
                                        </select>
                                    </div>

                                    <div class="form-group">
                                        <label for="package_ids"><strong>Scraping Packages:</strong></label>
                                        <select class="form-control" id="package_ids" name="package_ids" multiple size="5">
                                            {% for package in packages %}
                                                <option value="{{ package._id }}">{{ package.name|default('Unnamed Package') }}</option>
                                            {% endfor %}
                                        </select>
                                        <small class="form-text text-muted">Hold Ctrl/Cmd to select multiple packages. Leave empty to include all.</small>
                                    </div>

                                    <div class="form-group">
                                        <label for="search_query"><strong>Search Query:</strong></label>
                                        <input type="text" class="form-control" id="search_query" name="search_query" placeholder="Enter search terms...">
                                        <small class="form-text text-muted">The agent will search for relevant articles using this query.</small>
                                    </div>
                                </div>

                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="client_context"><strong>Client Context:</strong></label>
                                        <textarea class="form-control" id="client_context" name="client_context" rows="3" placeholder="Information about the client..."></textarea>
                                    </div>

                                    <div class="form-group">
                                        <label for="project_context"><strong>Project Context:</strong></label>
                                        <textarea class="form-control" id="project_context" name="project_context" rows="3" placeholder="Information about the project..."></textarea>
                                    </div>

                                    <div class="form-group">
                                        <label for="context_files"><strong>Additional Context Files:</strong></label>
                                        <input type="file" class="form-control" id="context_files" name="context_files" multiple>
                                        <small class="form-text text-muted">Upload TXT, PDF, MD, or DOCX files with additional context.</small>
                                    </div>
                                </div>
                            </div>

                            <div class="form-group mt-3">
                                <label for="prompt"><strong>Newsletter Instructions:</strong></label>
                                <textarea class="form-control" id="prompt" name="prompt" rows="5" placeholder="Detailed instructions for the newsletter..."></textarea>
                            </div>

                            <div class="form-group mt-3 d-flex justify-content-between">
                                <div>
                                    <button type="submit" class="btn btn-primary" id="generate-btn">Generate Newsletter</button>
                                </div>
                                <div>
                                    <label for="json-import" class="btn btn-outline-secondary">Import from JSON</label>
                                    <input type="file" id="json-import" accept=".json" style="display: none;">
                                </div>
                            </div>
                        </form>

                        <div id="loading" class="mt-4">
                            <div class="d-flex justify-content-center">
                                <div class="spinner-border text-primary" role="status">
                                    <span class="visually-hidden">Loading...</span>
                                </div>
                            </div>
                            <p class="text-center mt-2">Connecting to streaming service...</p>
                        </div>

                        <!-- Streaming Window for Agent Thoughts -->
                        <div id="streaming-window" class="mt-4">
                            <div class="card">
                                <div class="card-header">
                                    <h3>Agent Activity</h3>
                                </div>
                                <div class="card-body">
                                    <div id="streaming-content"></div>
                                </div>
                            </div>
                        </div>

                        <!-- Result Window for Generated Newsletter -->
                        <div id="result" class="mt-4">
                            <div class="card">
                                <div class="card-header d-flex justify-content-between align-items-center">
                                    <h3>Generated Newsletter</h3>
                                    <div>
                                        <button class="btn btn-sm btn-outline-secondary me-2" id="download-btn">Download JSON</button>
                                        <button class="btn btn-sm btn-outline-secondary" id="copy-btn">Copy to Clipboard</button>
                                    </div>
                                </div>
                                <div class="card-body">
                                    <div id="generated-content"></div>
                                </div>
                            </div>

                            <!-- Context Used Section -->
                            <div class="card mt-3">
                                <div class="card-header">
                                    <h3>Context Used <small class="text-muted">(<span id="articles-count">0</span> articles)</small></h3>
                                </div>
                                <div class="card-body">
                                    <pre id="context-used" class="border p-3 bg-light" style="max-height: 300px; overflow-y: auto; white-space: pre-wrap; font-size: 14px;"></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <footer class="mt-5 py-3 text-center text-secondary">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const form = document.getElementById('newsletter-form');
        const generateBtn = document.getElementById('generate-btn');
        const loading = document.getElementById('loading');
        const streamingWindow = document.getElementById('streaming-window');
        const streamingContent = document.getElementById('streaming-content');
        const result = document.getElementById('result');
        const generatedContent = document.getElementById('generated-content');
        const copyBtn = document.getElementById('copy-btn');
        const downloadBtn = document.getElementById('download-btn');
        const jsonImport = document.getElementById('json-import');

        // Store form data and results for download
        let currentFormData = {};
        let currentResults = {};

        form.addEventListener('submit', function(e) {
            e.preventDefault();

            // Clear previous content
            streamingContent.textContent = '';
            generatedContent.innerHTML = '';

            // Show loading indicator
            loading.style.display = 'block';
            streamingWindow.style.display = 'none';
            result.style.display = 'none';
            generateBtn.disabled = true;

            // Submit form data via AJAX
            const formData = new FormData(form);

            // Store form data for download
            currentFormData = {
                model_name: formData.get('model_name'),
                date_range: formData.get('date_range'),
                search_query: formData.get('search_query'),
                client_context: formData.get('client_context'),
                project_context: formData.get('project_context'),
                prompt: formData.get('prompt')
            };

            // Get selected package IDs
            const packageSelect = document.getElementById('package_ids');
            currentFormData.package_ids = Array.from(packageSelect.selectedOptions).map(option => option.value);

            // First send the form data to start the generation process
            fetch('/generate', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    alert('Error: ' + data.error);
                    loading.style.display = 'none';
                    generateBtn.disabled = false;
                    return;
                }

                // Hide loading and show streaming window
                loading.style.display = 'none';
                streamingWindow.style.display = 'block';

                // Connect to the streaming endpoint for agent thoughts
                const eventSource = new EventSource('/stream-thoughts');

                // Handle incoming messages
                eventSource.onmessage = function(event) {
                    if (event.data === '[DONE]') {
                        // End of stream
                        eventSource.close();

                        // Get the final newsletter content
                        fetch('/get-newsletter')
                            .then(response => response.json())
                            .then(data => {
                                // Show the final result
                                result.style.display = 'block';
                                generatedContent.innerHTML = formatContent(data.content);

                                // Update context information
                                document.getElementById('context-used').textContent = data.context_used;
                                document.getElementById('articles-count').textContent = data.articles_count;

                                // Store results for download
                                currentResults = {
                                    content: data.content,
                                    context_used: data.context_used,
                                    articles_count: data.articles_count,
                                    timestamp: new Date().toISOString()
                                };

                                // Re-enable the generate button
                                generateBtn.disabled = false;

                                // Scroll to result
                                result.scrollIntoView({ behavior: 'smooth' });
                            })
                            .catch(error => {
                                console.error('Error fetching newsletter:', error);
                                generateBtn.disabled = false;
                            });
                    } else if (event.data.trim() !== '') {
                        // Decode the SSE-encoded newlines
                        const decodedData = event.data.replace(/\\n/g, '\n');

                        // Append to streaming content
                        streamingContent.textContent += decodedData;

                        // Scroll to bottom
                        streamingContent.scrollTop = streamingContent.scrollHeight;
                    }
                };

                eventSource.onerror = function(error) {
                    console.error('EventSource error:', error);
                    eventSource.close();
                    generateBtn.disabled = false;
                    alert('Error in streaming. Please try again.');
                };
            })
            .catch(error => {
                console.error('Fetch error:', error);
                loading.style.display = 'none';
                generateBtn.disabled = false;
                alert('Error: ' + error);
            });
        });

        // Copy to clipboard functionality
        copyBtn.addEventListener('click', function() {
            const content = document.getElementById('generated-content').innerText;
            navigator.clipboard.writeText(content).then(() => {
                const originalText = copyBtn.textContent;
                copyBtn.textContent = 'Copied!';
                setTimeout(() => {
                    copyBtn.textContent = originalText;
                }, 2000);
            });
        });

        // Download JSON functionality
        downloadBtn.addEventListener('click', function() {
            // Create a combined object with inputs and outputs
            const downloadData = {
                inputs: currentFormData,
                outputs: currentResults
            };

            // Convert to JSON string
            const jsonString = JSON.stringify(downloadData, null, 2);

            // Create a blob and download link
            const blob = new Blob([jsonString], { type: 'application/json' });
            const url = URL.createObjectURL(blob);

            // Create a temporary link and trigger download
            const a = document.createElement('a');
            a.href = url;
            a.download = `newsletter_${new Date().toISOString().replace(/[:.]/g, '-')}.json`;
            document.body.appendChild(a);
            a.click();

            // Clean up
            setTimeout(() => {
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
            }, 0);
        });

        // Handle JSON import
        jsonImport.addEventListener('change', function(e) {
            if (!this.files || !this.files[0]) return;

            const file = this.files[0];
            if (!file.name.endsWith('.json')) {
                alert('Please select a JSON file');
                return;
            }

            // Show loading indicator
            loading.style.display = 'block';
            streamingWindow.style.display = 'none';
            result.style.display = 'none';
            generateBtn.disabled = true;

            // Create FormData and append the file
            const formData = new FormData();
            formData.append('json_file', file);

            // Send the file to the server
            fetch('/import-json', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                // Hide loading indicator
                loading.style.display = 'none';
                generateBtn.disabled = false;

                if (data.error) {
                    alert('Error: ' + data.error);
                    return;
                }

                // Update the form with the imported data
                const inputs = data.inputs;
                if (inputs) {
                    // Update form fields
                    if (inputs.model_name) document.getElementById('model_name').value = inputs.model_name;
                    if (inputs.date_range) document.getElementById('date_range').value = inputs.date_range;
                    if (inputs.search_query) document.getElementById('search_query').value = inputs.search_query;
                    if (inputs.client_context) document.getElementById('client_context').value = inputs.client_context;
                    if (inputs.project_context) document.getElementById('project_context').value = inputs.project_context;
                    if (inputs.prompt) document.getElementById('prompt').value = inputs.prompt;

                    // Update package selection
                    if (inputs.package_ids && inputs.package_ids.length > 0) {
                        const packageSelect = document.getElementById('package_ids');
                        Array.from(packageSelect.options).forEach(option => {
                            option.selected = inputs.package_ids.includes(option.value);
                        });
                    }

                    // Store the imported form data
                    currentFormData = inputs;
                }

                // Update the results
                const outputs = data.outputs;
                if (outputs) {
                    // Show the result section
                    result.style.display = 'block';

                    // Update the content
                    generatedContent.innerHTML = formatContent(outputs.content);
                    document.getElementById('context-used').textContent = outputs.context_used;
                    document.getElementById('articles-count').textContent = outputs.articles_count;

                    // Store the imported results
                    currentResults = {
                        content: outputs.content,
                        context_used: outputs.context_used,
                        articles_count: outputs.articles_count,
                        timestamp: new Date().toISOString()
                    };

                    // Scroll to result
                    result.scrollIntoView({ behavior: 'smooth' });
                }

                // Reset the file input
                jsonImport.value = '';
            })
            .catch(error => {
                console.error('Error importing JSON:', error);
                loading.style.display = 'none';
                generateBtn.disabled = false;
                alert('Error importing JSON: ' + error);

                // Reset the file input
                jsonImport.value = '';
            });
        });

        // Simple Markdown-like formatting
        function formatContent(text) {
            if (!text) return '';

            // Replace newlines with <br>
            text = text.replace(/\n/g, '<br>');

            // Format headers
            text = text.replace(/^# (.+)$/gm, '<h1>$1</h1>');
            text = text.replace(/^## (.+)$/gm, '<h2>$1</h2>');
            text = text.replace(/^### (.+)$/gm, '<h3>$1</h3>');

            // Format bold and italic
            text = text.replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>');
            text = text.replace(/\*(.+?)\*/g, '<em>$1</em>');

            // Format lists
            text = text.replace(/^- (.+)$/gm, '<li>$1</li>');
            text = text.replace(/(<li>.+<\/li>\n)+/g, '<ul>$&</ul>');

            return text;
        }
    });
    </script>
</body>
</html>
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Environment
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Jupyter Notebook
.ipynb_checkpoints

# PyCharm
.idea/

# VS Code
.vscode/

# Flask
instance/
.webassets-cache

# MongoDB local data
data/

# NLTK data (if downloaded locally)
nltk_data/

# Logs
*.log
logs/

# Local templates generated by article_viewer.py
# templates/

# System files
.DS_Store
Thumbs.db

# Local development files
.python-version
.coverage
htmlcov/
</file>

<file path="README.md">
# Proton CRM - MongoDB Database Schema

This repository contains the database schema and utilities for the Proton AI-powered content curation and delivery system. The system uses MongoDB to store scraped articles and other data for the Proton CRM.

## System Features

- **MongoDB Integration:** Uses PyMongo driver for connecting to MongoDB Atlas.
- **Schema Validation:** Ensures data integrity with comprehensive schema validation.
- **RSS Feed Scraping:** Combines multiple RSS feeds for comprehensive content coverage.
- **Content Analysis:** Extracts keywords, summaries, and entities from scraped content.
- **Vector Embeddings:** Generates embeddings for semantic search capabilities.
- **Relevance Scoring:** Calculates content relevance based on multiple factors:
  - Package-content fit (how well the content matches the package theme)
  - Content recency (fresher content scores higher)
  - Semantic relevance to specific topics or personas
- **Scheduling System:** Allows for automated content updates at specified intervals.
- **Error Handling:** Robust error handling for NLTK resources and network issues.
- **Package Management:** Create and manage scraping packages with different configurations.

## Project Structure

- `proton_db_setup.py` - Core database connection and schema definition
- `scraping_package_model.py` - MongoDB model for scraping packages
- `scraping_package_scheduler.py` - Scheduler for automated content scraping
- `run_scraping_system.py` - Command-line interface for running the system
- `view_articles.py` - Utility for viewing articles in the database
- `scraper_example.py` - Example script for scraping individual articles
- `requirements.txt` - Dependencies for the project

## Command Line Usage

```
# Install dependencies
pip install -r requirements.txt

# List all scraping packages
python run_scraping_system.py list

# Create a test scraping package
python run_scraping_system.py create

# Run all active scraping packages once
python run_scraping_system.py run

# Run the scheduler in continuous mode
python run_scraping_system.py run --scheduler

# Refresh relevance scores for all articles
python run_scraping_system.py refresh

# Refresh relevance scores for a specific package
python run_scraping_system.py refresh --package <package_id>

# View all articles in the database
python view_articles.py

# View articles from a specific package
python view_articles.py <package_id>
```

## Relevance Scoring System

The Proton CRM system uses a sophisticated relevance scoring system to determine how well an article matches a scraping package's theme and user interests:

### Package-Content Fit
- Calculates how well the article content matches the package theme
- Analyzes keywords from package name and description
- Scores range from 0.5 to 1.0, with higher scores indicating better fit

### Recency Score
- Based on the article's publication date
- Uses exponential decay: today's content has a score of ~0.95
- Two-week-old content scores ~0.5
- One-month-old content scores ~0.25

### Overall Relevance
- Weighted combination: 70% package fit + 30% recency
- This balance ensures fresh content about relevant topics scores highest

### Implementation
- Scores are calculated automatically when articles are scraped
- Can be refreshed using the `refresh` command
- Used for sorting and filtering content in the CRM
- Stored with each article in the MongoDB database

## MongoDB Schema for Articles

The MongoDB schema for articles is designed to store content scraped from various sources, with rich metadata and support for advanced features like vector search and AI-powered relevance scoring.

### Key Fields in the Article Schema

- **title**: The article's title
- **content**: Full text content of the article
- **summary**: AI-generated summary of the article
- **source_url**: Original URL where the article was scraped from
- **source_name**: Name of the source website or feed
- **author**: Author of the article
- **published_date**: Original publication date
- **date_scraped**: When the article was scraped by the system
- **scraping_package_id**: Reference to the scraping package configuration
- **project_ids**: List of projects this article is associated with
- **keywords**: Automatically extracted keywords from the article
- **entities**: Named entities (people, organizations, etc.) extracted from the content
- **vector_embedding**: Vector representation of the article for semantic search
- **relevance_scores**: Various scores indicating relevance to different criteria
- **content_type**: Type of content (news, blog, etc.)
- **status**: Status of the article (active, archived, flagged)

## Scraping Package System

The Proton CRM Scraping Package System allows you to define, schedule, and execute content scraping operations from multiple RSS feeds.

### Scraping Package Components

- **Scraping Package Model**: Database schema and operations for scraping packages
- **Scraping Package Scheduler**: Scheduler for executing packages at specified intervals
- **Command-line Interface**: Manages and runs the scraping system

### Scraping Package Schema

Each scraping package contains:

- **name**: Descriptive name for the package
- **description**: Details about what the package does
- **rss_feeds**: List of RSS feed URLs to scrape
- **project_ids**: Projects associated with this package
- **schedule_interval**: How often to run the scraper (e.g., "30m", "1h", "1d")
- **max_articles_per_run**: Maximum number of articles to process per run
- **status**: Package status (active, paused, archived)
- **filters**: Filtering criteria for articles
  - Keywords to include or exclude
  - Minimum article length
  - Domains to include or exclude
- **processing_options**:
  - Whether to calculate vector embeddings
  - Whether to extract named entities
  - Whether to generate summaries
- **stats**: Statistics about package runs and processed articles

### RSS Feed Processing

Each package combines multiple RSS feeds into a unified stream:

1. **Feed Parsing**: Processes each RSS feed into a common format
2. **Content Extraction**: Fetches full article content from feed URLs
3. **Text Analysis**:
   - Extracts keywords from article content
   - Generates article summaries
   - Calculates vector embeddings for semantic search
4. **Relevance Scoring**: Computes relevance metrics based on content and metadata
5. **Storage**: Stores processed articles in the MongoDB database

### Command-line Usage

The `run_scraping_system.py` script provides a command-line interface for the scraping system:

```bash
# Create a test package
python run_scraping_system.py create-test

# List all packages
python run_scraping_system.py list

# Run all active packages once
python run_scraping_system.py run

# Run a specific package by ID
python run_scraping_system.py run --package-id <package_id>

# Run in scheduler mode (continuous operation)
python run_scraping_system.py run --scheduler
```

## Getting Started

### Prerequisites

- MongoDB 4.4+ (MongoDB Atlas recommended for vector search capabilities)
- Python 3.8+
- Required Python packages: See `requirements.txt`

### Installation

1. Clone this repository
2. Install required packages:
   ```
   pip install -r requirements.txt
   ```
3. Copy `.env.template` to `.env` and update with your MongoDB connection details
4. For NLTK resources, uncomment and run the download commands in `scraper_example.py` once

### Running the Scraping System

1. Create a test scraping package:
   ```
   python run_scraping_system.py create-test
   ```

2. List available packages:
   ```
   python run_scraping_system.py list
   ```

3. Run in scheduler mode:
   ```
   python run_scraping_system.py run --scheduler
   ```

## Example Scraper

The `scraper_example.py` file provides a basic implementation of an article scraper that:

1. Fetches content from URLs
2. Extracts article title, content, author, date, etc.
3. Analyzes the text to extract keywords and entities
4. Generates a simple summary
5. Stores the processed article in MongoDB

Example usage:

```python
from scraper_example import SimpleArticleScraper

scraper = SimpleArticleScraper(project_id, scraping_package_id)
article_ids = scraper.scrape_and_store_articles([
    "https://example.com/article1",
    "https://example.com/article2"
])
```

## MongoDB Indexes

The schema creates the following indexes for efficient querying:

- `source_url` (unique) - Prevents duplicate articles
- `date_scraped` - For time-based queries
- `project_ids` - For project-specific queries
- `scraping_package_id` - For package-specific queries 
- `keywords` - For keyword-based filtering
- `content_type` - For content type filtering
- `status` - For status filtering
- Text index on `title` and `content` - For full-text search
- Vector index on `vector_embedding` (if using MongoDB Atlas) - For semantic search

## Working with Vector Search

To use vector search capabilities:

1. Use MongoDB Atlas, which provides vector search functionality
2. Set `MONGODB_VECTOR_SEARCH_ENABLED=true` in your `.env` file
3. Generate vector embeddings for your articles (using models like Sentence-Transformers)
4. Store the embeddings in the `vector_embedding` field
5. Use the `vector_search` method to find semantically similar articles

## Vector Embeddings

The Proton CRM system supports two types of vector embeddings for article content:

1. **Local Embeddings (Default)**: Uses SentenceTransformers to generate embeddings locally. This option is free and doesn't require an API key, but provides lower quality embeddings.

2. **OpenAI Embeddings**: Uses OpenAI's text-embedding-3-small model to generate high-quality embeddings. This requires an OpenAI API key and has associated costs, but provides superior semantic search capabilities.

### Setup for OpenAI Embeddings

1. Copy `.env.template` to `.env` if you haven't already
2. Add your OpenAI API key to the `.env` file: `OPENAI_API_KEY=sk-your-key-here`
3. Set `EMBEDDING_MODEL=openai` in the `.env` file

### Updating Existing Embeddings

To update existing articles with OpenAI embeddings:

```bash
# Update all articles in the database
python update_embeddings.py --all --embedding-type openai

# Update articles from a specific package
python update_embeddings.py --package <package_id> --embedding-type openai
```

## Further Development

For production use, consider:

1. Adding more robust error handling and retry logic
2. Implementing more sophisticated scraping logic specific to your target sites
3. Adding a worker queue for distributed scraping
4. Developing a more robust text analysis pipeline
5. Implementing proper user authentication and authorization

## Troubleshooting

### DNS Resolution Issues

If you encounter DNS resolution issues when connecting to MongoDB Atlas, such as:

```
ERROR - Failed to connect to MongoDB: nameserver 8.8.4.4, is not a dns.nameserver.Nameserver instance...
```

The solution is to configure the DNS resolver explicitly in your code:

1. Install the dnspython package:
   ```
   pip install dnspython
   ```

2. Configure the DNS resolver with Google's public DNS servers:
   ```python
   import dns.resolver
   
   # Configure DNS resolver explicitly
   dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
   dns.resolver.default_resolver.nameservers = ['8.8.8.8', '8.8.4.4']
   ```

This solution is already implemented in the `proton_db_setup.py` file.

### MongoDB Atlas Permissions

When running with MongoDB Atlas, you might encounter permission errors when trying to create schema validation or certain indexes. The code has been updated to handle these cases gracefully, creating collections without schema validation when running on Atlas, and applying them only in self-hosted MongoDB environments where the user has full admin privileges.

### NLTK Resource Errors

If you encounter NLTK resource errors like:

```
Resource punkt_tab not found. Please use the NLTK Downloader to obtain the resource
```

Make sure to download all required NLTK resources by uncommenting and running these lines in `scraper_example.py`:

```python
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('punkt_tab')
```

The code has also been updated with fallback mechanisms that will still work (with reduced functionality) if some NLTK resources are unavailable.

### Sentence Transformers Errors

For vector embeddings, we use the Sentence Transformers library. If you encounter errors related to this:

1. Make sure you have installed the required packages:
   ```
   pip install sentence-transformers
   ```

2. If you receive CUDA errors but don't have a GPU, set this environment variable:
   ```
   export CUDA_VISIBLE_DEVICES=""
   ```

3. If you want to disable embeddings calculation for performance reasons, you can set:
   ```python
   calculate_embeddings=False
   ```
   when creating a scraping package. 

## Article Viewer

The project includes a simple web interface for browsing articles stored in the database:

### Features
- View all articles with pagination
- Filter articles by package
- Search articles by text
- Sort by date, relevance, or title
- View detailed article information including content, keywords, and entities
- Browse scraping packages and their associated articles

### Running the Web Interface
```bash
# Install required dependencies
pip install -r requirements.txt

# Run the web server
python article_viewer.py
```

The web interface will be available at http://localhost:5000 

## Deployment Instructions

The application can be deployed using Render.com with the provided configuration files. There are two main components:

1. **Web Viewer**: The Flask web application that displays articles
2. **Scraper**: A scheduled job that collects articles from RSS feeds

### Deployment Steps

1. Fork or clone this repository 
2. Create a new Render account or log in to your existing account
3. Connect your GitHub repository to Render
4. Set up the required environment variables:
   - `MONGODB_URI`: Your MongoDB connection string
   - `MONGODB_DB_NAME`: The name of your MongoDB database (default: "proton")
5. Deploy using the Blueprint feature of Render, which will use the `render.yaml` file

### Environment Variables

- `MONGODB_URI`: MongoDB connection string (required)
- `MONGODB_DB_NAME`: MongoDB database name (default: "proton")

## Troubleshooting

### MongoDB Atlas Connection Issues

When connecting to MongoDB Atlas, you might encounter DNS resolution issues, especially with SRV connection strings in certain environments.

#### DNS Resolution

If you encounter DNS resolution errors, you might need to configure your DNS resolver explicitly:

```python
import dns.resolver
dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
dns.resolver.default_resolver.nameservers = ['8.8.8.8']  # Google DNS server
```

#### MongoDB Atlas Permissions

Make sure your MongoDB Atlas user has the correct permissions and that your network IP is whitelisted in the MongoDB Atlas Network Access settings.

### NLTK Resource Errors

If you encounter errors related to missing NLTK resources, you need to download them:

```python
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
```

For automated deployment, you can include these commands in your setup script.
</file>

<file path="deploy/newsletter_agent/requirements.txt">
flask==2.3.3
werkzeug==2.3.7
pymongo==4.5.0
dnspython==2.4.2
python-dotenv==1.0.0
openai==1.3.5
anthropic==0.22.0
requests==2.31.0
httpx==0.24.1
PyPDF2==3.0.1
python-docx==0.8.11
gunicorn==21.2.0
markupsafe==2.1.3
</file>

<file path="deploy/scraper/vector_embeddings.py">
"""
Vector Embedding Utilities for Proton CRM

This module provides functions for generating vector embeddings using different models:
1. OpenAI API (requires API key)
2. SentenceTransformers (local model, no API key required)
"""

import os
import logging
from typing import List
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables from .env file
load_dotenv()

class EmbeddingGenerator:
    """Class for generating vector embeddings using different models"""

    def __init__(self, model_type: str = "local"):
        """
        Initialize the embedding generator

        Args:
            model_type: Type of model to use ('openai' or 'local')
        """
        self.model_type = model_type.lower()
        self.model = None
        self.client = None
        self.api_key = None
        self.embedding_size = 384  # Default size for local embeddings

        logger.info(f"Initializing embedding generator with model type: {self.model_type}")

        if self.model_type == "openai":
            # Try to set up OpenAI embedding
            self._setup_openai()
        else:
            # Try to set up local model
            self._load_local_model()

    def _setup_openai(self):
        """Set up OpenAI API client"""
        try:
            # Check for OpenAI API key
            self.api_key = os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                logger.warning("OpenAI API key not found in environment variables. Using local model instead.")
                self.model_type = "local"
                self._load_local_model()
                return

            # Try to import openai
            try:
                import openai
                self.client = openai.OpenAI(api_key=self.api_key)
                self.embedding_size = 1536  # Size for text-embedding-3-small and text-embedding-ada-002
                logger.info("Using OpenAI for embeddings")

                # Verify API connection with a simple test if not in CI
                if 'CI' not in os.environ and 'RENDER' not in os.environ:
                    try:
                        # Test the API connection
                        _ = self.client.embeddings.create(
                            input="Connection test",
                            model="text-embedding-3-small"
                        )
                        logger.info("Successfully connected to OpenAI API")
                    except Exception as e:
                        logger.warning(f"OpenAI API connection test failed: {e}")
                        logger.warning("Will continue, but embeddings may fail at runtime")

            except ImportError:
                logger.warning("OpenAI package not installed. Using local model instead.")
                self.model_type = "local"
                self._load_local_model()

        except Exception as e:
            logger.warning(f"Error setting up OpenAI: {e}")
            logger.warning("Falling back to local model")
            self.model_type = "local"
            self._load_local_model()

    def _load_local_model(self):
        """Load the local sentence transformer model"""
        try:
            from sentence_transformers import SentenceTransformer
            # Use MiniLM model (good balance of speed and quality)
            self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
            self.embedding_size = 384  # Size for paraphrase-MiniLM-L6-v2
            logger.info("Using SentenceTransformer for embeddings (local model)")
        except ImportError:
            logger.error("SentenceTransformer package not installed. Please install it with: pip install sentence-transformers")
            logger.warning("Will continue without vector embedding functionality")
            self.model_type = "none"
        except Exception as e:
            logger.error(f"Error loading SentenceTransformer model: {e}")
            logger.warning("Will continue without vector embedding functionality")
            self.model_type = "none"

    def get_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for the provided text

        Args:
            text: Text to generate embedding for

        Returns:
            List[float]: Vector embedding
        """
        if self.model_type == "none":
            logger.warning("No embedding model available")
            return [0.0] * 384  # Return zero vector of default size

        if not text.strip():
            # Return zero vector if text is empty
            return [0.0] * self.embedding_size

        try:
            if self.model_type == "openai":
                return self._get_openai_embedding(text)
            else:
                return self._get_local_embedding(text)
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            # Return zero vector on error
            return [0.0] * self.embedding_size

    def _get_openai_embedding(self, text: str) -> List[float]:
        """Get embedding using OpenAI API"""
        if not self.client:
            logger.warning("OpenAI client not initialized. Falling back to local model")
            if not self.model:
                self._load_local_model()
            return self._get_local_embedding(text)

        try:
            # Truncate text if too long (OpenAI has token limits)
            truncated_text = text[:8000]  # Approximate token limit safety

            response = self.client.embeddings.create(
                input=truncated_text,
                model="text-embedding-3-small"  # Newer, better model with 1536 dimensions
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            # Fall back to local model if OpenAI fails
            logger.info("Falling back to local model")
            if not self.model:
                self._load_local_model()
            return self._get_local_embedding(text)

    def _get_local_embedding(self, text: str) -> List[float]:
        """Get embedding using local SentenceTransformer model"""
        if not self.model:
            logger.error("Local model not initialized")
            return [0.0] * self.embedding_size

        try:
            # Truncate text if too long (for memory reasons)
            truncated_text = text[:10000]
            embedding = self.model.encode(truncated_text)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error generating local embedding: {e}")
            return [0.0] * self.embedding_size

    def batch_get_embeddings(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        Generate embeddings for a batch of texts

        Args:
            texts: List of texts to generate embeddings for
            batch_size: Size of batches to process at once

        Returns:
            List[List[float]]: List of vector embeddings
        """
        if self.model_type == "none":
            logger.warning("No embedding model available")
            return [[0.0] * self.embedding_size for _ in range(len(texts))]

        results = []

        if self.model_type == "openai" and self.client:
            # Process in batches to avoid rate limits
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                batch_results = []

                try:
                    truncated_batch = [text[:8000] for text in batch]

                    # Handle empty strings
                    for j, text in enumerate(truncated_batch):
                        if not text.strip():
                            truncated_batch[j] = "empty"

                    response = self.client.embeddings.create(
                        input=truncated_batch,
                        model="text-embedding-ada-002"  # Update to use the same model
                    )

                    # Extract embeddings in the correct order
                    for item in response.data:
                        batch_results.append(item.embedding)

                except Exception as e:
                    logger.error(f"Error in batch OpenAI embedding: {e}")
                    # Fall back to processing one by one
                    for text in batch:
                        batch_results.append(self.get_embedding(text))

                results.extend(batch_results)
                if i + batch_size < len(texts):
                    logger.info(f"Processed {i + batch_size}/{len(texts)} embeddings with OpenAI")
        elif self.model and self.model_type == "local":
            # SentenceTransformer can handle batches natively
            try:
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i+batch_size]
                    truncated_batch = [text[:10000] for text in batch]

                    # Handle empty strings
                    for j, text in enumerate(truncated_batch):
                        if not text.strip():
                            truncated_batch[j] = "empty"

                    batch_embeddings = self.model.encode(truncated_batch)
                    results.extend(batch_embeddings.tolist())

                    if i + batch_size < len(texts):
                        logger.info(f"Processed {i + batch_size}/{len(texts)} embeddings with SentenceTransformer")
            except Exception as e:
                logger.error(f"Error in batch local embedding: {e}")
                # Fall back to processing one by one
                for text in texts:
                    results.append(self.get_embedding(text))
        else:
            # No valid embedding model available
            logger.warning("No embedding model available for batch processing")
            results = [[0.0] * self.embedding_size for _ in range(len(texts))]

        return results


# Simple example usage
if __name__ == "__main__":
    # Test both embedding types
    test_text = "This is a test of the embedding system for Proton CRM."

    # Try OpenAI first
    try:
        openai_embedder = EmbeddingGenerator(model_type="openai")
        openai_embedding = openai_embedder.get_embedding(test_text)
        print(f"OpenAI embedding dimension: {len(openai_embedding)}")
        print(f"First 5 values: {openai_embedding[:5]}")
    except Exception as e:
        print(f"OpenAI embedding test failed: {e}")

    # Try local model
    try:
        local_embedder = EmbeddingGenerator(model_type="local")
        local_embedding = local_embedder.get_embedding(test_text)
        print(f"Local embedding dimension: {len(local_embedding)}")
        print(f"First 5 values: {local_embedding[:5]}")
    except Exception as e:
        print(f"Local embedding test failed: {e}")
</file>

<file path="deploy/web/render.yaml">
services:
  # Web service for the Flask application
  - type: web
    name: proton-db-viewer
    runtime: python
    rootDir: deploy/web
    buildCommand: |
      pip install --upgrade pip
      pip install flask==3.0.0 pymongo==4.5.0 dnspython==2.4.2 python-dotenv==1.0.0 gunicorn==21.2.0 markupsafe==2.1.3
    startCommand: gunicorn 'wsgi:app' --log-file -
    envVars:
      - key: MONGODB_URI
        sync: false
    plan: free
    autoDeploy: true
</file>

<file path="scraping_package_scheduler.py">
"""
Scraping Package Scheduler for Proton CRM

This module provides functionality to schedule and execute RSS feed scraping packages.
It combines multiple RSS feeds, extracts articles, and stores them in the MongoDB database.
"""

import os
import sys
import time
import datetime
import logging
import feedparser
import schedule
import threading
import queue
from typing import Dict, List, Optional, Any
from bson.objectid import ObjectId
import requests
from urllib.parse import urlparse
import numpy as np
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize

# Add parent directory to path to import ProtonDB
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from proton_db_setup import ProtonDB

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global queue for scheduled jobs
job_queue = queue.Queue()

class ScrapingPackage:
    """
    Represents a scraping package configuration that combines multiple RSS feeds
    """

    def __init__(self, package_id: str, project_ids: List[str], name: str,
                 description: str, rss_feeds: List[str], schedule_interval: str = "1h",
                 max_articles_per_run: int = 10, calculate_embeddings: bool = True):
        """
        Initialize a scraping package

        Args:
            package_id: MongoDB ObjectId of the scraping package
            project_ids: List of project IDs this package is associated with
            name: Name of the scraping package
            description: Description of the package
            rss_feeds: List of RSS feed URLs to scrape
            schedule_interval: How often to run the scraper (e.g., "30m", "1h", "1d")
            max_articles_per_run: Maximum number of articles to process per run
            calculate_embeddings: Whether to calculate vector embeddings for articles
        """
        self.package_id = package_id
        self.project_ids = project_ids
        self.name = name
        self.description = description
        self.rss_feeds = rss_feeds
        self.schedule_interval = schedule_interval
        self.max_articles_per_run = max_articles_per_run
        self.calculate_embeddings = calculate_embeddings

        # Load embedding model if needed
        if self.calculate_embeddings:
            try:
                from vector_embeddings import EmbeddingGenerator
                # Try to use OpenAI first, will fall back to local if API key not available
                self.embedding_generator = EmbeddingGenerator(model_type="openai")
                logger.info(f"Loaded embedding model for package {self.name} (type: {self.embedding_generator.model_type})")
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")
                self.calculate_embeddings = False

        # Initialize database connection
        self.db = ProtonDB()

        # Initialize NLTK for text processing
        try:
            # Download necessary NLTK resources
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            logger.info("Successfully downloaded NLTK resources")
        except Exception as e:
            logger.warning(f"Failed to download NLTK resources: {e}")
            logger.info("Will attempt to download resources again when needed")

    def parse_feeds(self) -> List[Dict[str, Any]]:
        """
        Parse all RSS feeds and combine entries

        Returns:
            List[Dict[str, Any]]: Combined list of feed entries
        """
        all_entries = []

        for feed_url in self.rss_feeds:
            try:
                logger.info(f"Parsing RSS feed: {feed_url}")
                feed = feedparser.parse(feed_url)

                # Skip feeds with errors
                if hasattr(feed, 'bozo_exception'):
                    logger.warning(f"Feed error for {feed_url}: {feed.bozo_exception}")
                    continue

                # Extract feed entries
                for entry in feed.entries:
                    try:
                        # Basic validation
                        if not hasattr(entry, 'link') or not entry.link:
                            continue

                        # Build entry data
                        entry_data = {
                            "title": getattr(entry, 'title', 'Untitled'),
                            "link": entry.link,
                            "summary": getattr(entry, 'summary', None),
                            "published": getattr(entry, 'published', None),
                            "source_feed": feed_url,
                            "source_name": feed.feed.get('title', urlparse(feed_url).netloc)
                        }

                        # Try to extract published date
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                # Convert struct_time to datetime
                                entry_data['published_date'] = datetime.datetime(*entry.published_parsed[:6])
                            except Exception as e:
                                logger.debug(f"Failed to parse date: {e}")

                        all_entries.append(entry_data)
                    except Exception as e:
                        logger.warning(f"Error processing feed entry: {e}")

                logger.info(f"Extracted {len(feed.entries)} entries from {feed_url}")

            except Exception as e:
                logger.error(f"Failed to parse feed {feed_url}: {e}")

        # Sort by published date (newest first) if available
        all_entries.sort(
            key=lambda x: x.get('published_date', datetime.datetime.min),
            reverse=True
        )

        # Limit to max_articles_per_run
        return all_entries[:self.max_articles_per_run]

    def fetch_full_content(self, entry: Dict[str, Any]) -> Dict[str, Any]:
        """
        Fetch the full content of an article from its URL

        Args:
            entry: Feed entry with basic metadata

        Returns:
            Dict[str, Any]: Entry with additional content and metadata
        """
        url = entry["link"]
        try:
            # Add a small delay to be considerate to the target server
            time.sleep(1)

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # Parse with BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')

            # Extract article content
            article_content = ""
            main_content = soup.find('article') or soup.find('main') or soup.find('div', class_='content')

            if main_content:
                # Get all paragraphs from the main content
                paragraphs = main_content.find_all('p')
                article_content = ' '.join([p.get_text().strip() for p in paragraphs])
            else:
                # Fallback: get all paragraphs from the body
                paragraphs = soup.find_all('p')
                article_content = ' '.join([p.get_text().strip() for p in paragraphs])

            # Extract author information if available
            author_elem = soup.find('meta', attrs={'name': 'author'}) or soup.find('a', rel='author')
            author = author_elem.get('content', '') if hasattr(author_elem, 'get') and author_elem.get('content') else (
                author_elem.get_text().strip() if author_elem else "Unknown"
            )

            # Extract image if available
            image_url = None
            image_elem = soup.find('meta', property='og:image')
            if image_elem and image_elem.get('content'):
                image_url = image_elem.get('content')
            else:
                # Try to find the first large image in the article
                images = main_content.find_all('img') if main_content else soup.find_all('img')
                for img in images:
                    if img.get('src') and (img.get('width') is None or int(img.get('width') or 0) >= 300):
                        image_url = img.get('src')
                        if not image_url.startswith('http'):
                            # Handle relative URLs
                            base_url = '/'.join(url.split('/')[:3])  # Get the domain part
                            image_url = f"{base_url}/{image_url.lstrip('/')}"
                        break

            # Add package info to content to improve relevance
            package_info = f"This article is part of the '{self.name}' package. {self.description}"
            enhanced_content = f"{package_info}\n\n{article_content}"

            # Extract keywords
            keywords = self._extract_keywords(enhanced_content)

            # Generate summary
            summary = self._generate_summary(article_content)

            # Update entry with full content and metadata
            entry.update({
                "content": enhanced_content,
                "original_content": article_content,  # Keep the original content too
                "summary": summary if summary else entry.get("summary", ""),
                "author": author,
                "keywords": keywords,
                "image_url": image_url,
                "package_name": self.name  # Add package name as metadata
            })

            # Calculate vector embedding if enabled
            if self.calculate_embeddings and hasattr(self, 'embedding_generator'):
                try:
                    # Combine title, package name, description, and content for embedding
                    text_to_embed = f"{entry['title']}. {self.name}. {self.description}. {article_content}"
                    embedding = self.embedding_generator.get_embedding(text_to_embed)
                    entry["vector_embedding"] = embedding
                    entry["embedding_model"] = self.embedding_generator.model_type
                    logger.info(f"Generated {self.embedding_generator.model_type} embedding for article: {entry['title']}")
                except Exception as e:
                    logger.warning(f"Failed to generate embedding: {e}")

            # Calculate relevance score (basic implementation)
            entry["relevance_scores"] = self._calculate_relevance_scores(entry)

            return entry

        except Exception as e:
            logger.error(f"Error fetching content from {url}: {e}")
            # Return the original entry with minimal data
            entry["content"] = f"This article is part of the '{self.name}' package. {self.description}\n\n" + entry.get("summary", "")
            entry["original_content"] = entry.get("summary", "")
            entry["package_name"] = self.name
            entry["relevance_scores"] = {"overall": 0.5}  # Default score
            return entry

    def _extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """
        Extract keywords from text

        Args:
            text: Text to extract keywords from
            max_keywords: Maximum number of keywords to extract

        Returns:
            List[str]: List of keywords
        """
        try:
            # Make sure NLTK resources are downloaded
            try:
                nltk.download('punkt', quiet=True)
                nltk.download('stopwords', quiet=True)
            except Exception as e:
                logger.warning(f"Failed to download NLTK resources: {e}")

            from nltk.tokenize import word_tokenize
            from nltk.corpus import stopwords

            # Add package name and keywords from description as additional keywords
            additional_keywords = []

            # Add package name as a keyword
            name_words = self.name.lower().split()
            additional_keywords.extend([word for word in name_words if len(word) > 3])

            # Add words from description as keywords
            if self.description:
                desc_words = self.description.lower().split()
                additional_keywords.extend([word for word in desc_words if len(word) > 3])

            # Tokenize the text
            try:
                tokens = word_tokenize(text.lower())
                logger.info(f"Successfully tokenized text with NLTK (found {len(tokens)} tokens)")
            except Exception as e:
                logger.warning(f"NLTK tokenization failed: {e}, falling back to simple split")
                tokens = text.lower().split()

            # Remove stopwords and non-alphabetic tokens
            try:
                stop_words = set(stopwords.words('english'))
                logger.info("Successfully loaded NLTK stopwords")
            except Exception as e:
                logger.warning(f"Failed to load NLTK stopwords: {e}, using basic stopwords")
                stop_words = set(['the', 'and', 'a', 'to', 'in', 'of', 'is', 'that', 'it', 'for', 'with', 'as', 'on',
                                 'this', 'be', 'are', 'was', 'were', 'has', 'have', 'had', 'do', 'does', 'did',
                                 'but', 'or', 'by', 'not', 'what', 'all', 'their', 'there', 'when', 'up', 'use', 'how',
                                 'out', 'if', 'so', 'no', 'such', 'they', 'then', 'than'])

            filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 3]
            logger.info(f"Filtered to {len(filtered_tokens)} meaningful tokens")

            # Count frequency of each word
            word_freq = {}
            for word in filtered_tokens:
                if word in word_freq:
                    word_freq[word] += 1
                else:
                    word_freq[word] = 1

            # Give higher weight to package name and description words
            for word in additional_keywords:
                if word in word_freq:
                    word_freq[word] += 3  # Boost the frequency
                else:
                    word_freq[word] = 3   # Add with higher initial frequency

            # Sort by frequency and get top keywords
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            keywords = [word for word, _ in sorted_words[:max_keywords]]

            logger.info(f"Extracted {len(keywords)} keywords: {', '.join(keywords[:5])}...")
            return keywords
        except Exception as e:
            logger.warning(f"Error extracting keywords: {e}")
            # Return at least the package name as a keyword
            fallback_keywords = [word for word in self.name.lower().split() if len(word) > 3]
            logger.info(f"Using fallback keywords: {fallback_keywords}")
            return fallback_keywords

    def _generate_summary(self, text: str, max_sentences: int = 3) -> str:
        """
        Generate a simple summary from text (first few sentences)

        Args:
            text: Text to summarize
            max_sentences: Maximum number of sentences in the summary

        Returns:
            str: Summary text
        """
        try:
            # Make sure NLTK resources are downloaded
            try:
                nltk.download('punkt', quiet=True)
            except Exception as e:
                logger.warning(f"Failed to download NLTK punkt: {e}")

            # Try NLTK sentence tokenization
            try:
                sentences = sent_tokenize(text)
                logger.info(f"Successfully tokenized text into {len(sentences)} sentences with NLTK")
            except Exception as e:
                logger.warning(f"NLTK sentence tokenization failed: {e}, falling back to simple split")
                # Simple fallback for sentence tokenization
                sentences = []
                for sentence in text.split('.'):
                    if len(sentence.strip()) > 10:  # Ignore very short fragments
                        sentences.append(sentence.strip() + '.')
                logger.info(f"Fallback tokenization found {len(sentences)} sentences")

            # Get the first few sentences for the summary
            if sentences:
                summary_sentences = sentences[:max_sentences]
                summary = ' '.join(summary_sentences)
                logger.info(f"Generated summary of {len(summary)} characters from {len(summary_sentences)} sentences")
                return summary
            else:
                # If no sentences were found, return a portion of the text
                logger.warning("No sentences found in text, using text prefix as summary")
                return text[:250] + "..."
        except Exception as e:
            logger.warning(f"Error generating summary: {e}")
            return text[:250] + "..."  # Fallback to first 250 chars

    def _calculate_relevance_scores(self, entry: Dict[str, Any]) -> Dict[str, float]:
        """
        Calculate relevance scores for an article

        Args:
            entry: Article data

        Returns:
            Dict[str, float]: Relevance scores
        """
        scores = {
            "overall": 0.75,  # Default score
            "recency": 0.0,   # Will be calculated based on publish date
            "package_fit": 0.8  # Default package fit score
        }

        # Calculate recency score (1.0 for today, decreasing with age)
        if "published_date" in entry and entry["published_date"]:
            now = datetime.datetime.now()
            age_days = (now - entry["published_date"]).days

            # Exponential decay: score = exp(-age_days/14)
            # This gives ~0.95 for today, ~0.5 for 2 weeks old, ~0.25 for 1 month old
            recency = np.exp(-age_days/14) if age_days >= 0 else 0.95
            scores["recency"] = min(1.0, max(0.0, recency))
        else:
            # If no date, assume it's not very recent
            scores["recency"] = 0.5

        # Calculate package fit score (if we have content to analyze)
        if "original_content" in entry and entry["original_content"]:
            # Look for package name and description keywords in the content
            package_keywords = set()

            # Add package name words
            for word in self.name.lower().split():
                if len(word) > 3:  # Only consider longer words
                    package_keywords.add(word)

            # Add description words
            if self.description:
                for word in self.description.lower().split():
                    if len(word) > 3:  # Only consider longer words
                        package_keywords.add(word)

            # Count matches in content
            content_lower = entry["original_content"].lower()
            match_count = sum(1 for word in package_keywords if word in content_lower)

            # Calculate package fit score
            if package_keywords:
                # Score is ratio of matched keywords to total keywords, with a minimum of 0.5
                package_fit = max(0.5, min(1.0, match_count / len(package_keywords)))
                scores["package_fit"] = package_fit

        # Adjust overall score based on recency and package fit
        scores["overall"] = 0.3 * scores["recency"] + 0.7 * scores["package_fit"]

        # Add persona-specific scores (placeholder for now)
        scores["persona_specific"] = {
            "general": scores["overall"]  # Default persona
        }

        return scores

    def store_article(self, entry: Dict[str, Any]) -> Optional[str]:
        """
        Store an article in the database

        Args:
            entry: Article data from RSS feed + full content

        Returns:
            Optional[str]: ID of the inserted article or None if failed
        """
        try:
            # Prepare article data for MongoDB
            article_data = {
                "title": entry["title"],
                "content": entry["content"],
                "original_content": entry.get("original_content", ""),  # Store original content
                "summary": entry["summary"],
                "source_url": entry["link"],
                "source_name": entry["source_name"],
                "author": entry.get("author", "Unknown"),
                "published_date": entry.get("published_date"),
                "date_scraped": datetime.datetime.now(datetime.timezone.utc),
                "scraping_package_id": ObjectId(self.package_id),
                "scraping_package_name": self.name,  # Store package name
                "project_ids": [ObjectId(project_id) for project_id in self.project_ids],
                "keywords": entry.get("keywords", []),
                "content_type": "news",  # Default to news
                "status": "active",
                "image_url": entry.get("image_url"),
                "relevance_scores": entry.get("relevance_scores", {"overall": 0.5})
            }

            # Add vector embedding if available
            if "vector_embedding" in entry:
                article_data["vector_embedding"] = entry["vector_embedding"]

            # Store in database
            article_id = self.db.insert_article(article_data)
            return article_id

        except Exception as e:
            logger.error(f"Failed to store article '{entry.get('title')}': {e}")
            return None

    def run(self) -> List[str]:
        """
        Execute the scraping package: fetch feeds, extract content, and store in database

        Returns:
            List[str]: List of article IDs that were processed
        """
        start_time = time.time()
        logger.info(f"Running scraping package '{self.name}' (ID: {self.package_id})")

        try:
            # Parse all RSS feeds and combine entries
            entries = self.parse_feeds()
            logger.info(f"Found {len(entries)} entries across {len(self.rss_feeds)} feeds")

            # Process each entry
            article_ids = []
            for entry in entries:
                try:
                    # Fetch full content
                    full_entry = self.fetch_full_content(entry)

                    # Store in database
                    article_id = self.store_article(full_entry)
                    if article_id:
                        article_ids.append(article_id)
                        logger.info(f"Processed article: {full_entry['title']} (ID: {article_id})")
                except Exception as e:
                    logger.error(f"Error processing entry {entry.get('link')}: {e}")

            duration = time.time() - start_time
            logger.info(f"Scraping package '{self.name}' completed in {duration:.2f}s. Processed {len(article_ids)} articles.")
            return article_ids

        except Exception as e:
            logger.error(f"Failed to run scraping package '{self.name}': {e}")
            return []
        finally:
            # Close the database connection
            self.db.close()

    def _generate_embeddings(self, content: str) -> List[float]:
        """
        Generate vector embeddings for the article content

        Args:
            content: Article content to generate embeddings for

        Returns:
            List[float]: Vector embeddings
        """
        # If we already have an embedding generator, use it
        if hasattr(self, 'embedding_generator'):
            try:
                return self.embedding_generator.get_embedding(content)
            except Exception as e:
                logger.error(f"Error generating embeddings with existing generator: {e}")

        # Otherwise create a new one
        try:
            # Use the EmbeddingGenerator class
            from vector_embeddings import EmbeddingGenerator

            # Check for preference in package settings
            embedding_model = os.getenv("EMBEDDING_MODEL", "local").lower()

            # Create embedding generator
            embedder = EmbeddingGenerator(model_type=embedding_model)

            # Generate and return embedding
            embedding = embedder.get_embedding(content)
            return embedding
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            # Return empty list on error
            return []


class ScrapingScheduler:
    """
    Scheduler for managing and executing scraping packages at specified intervals
    """

    def __init__(self):
        """Initialize the scheduler"""
        self.packages = {}  # Dict of package_id -> ScrapingPackage
        self.running = False
        self.worker_thread = None

    def add_package(self, package: ScrapingPackage) -> None:
        """
        Add a scraping package to the scheduler

        Args:
            package: ScrapingPackage to add
        """
        self.packages[package.package_id] = package

        # Convert interval string to schedule
        interval = package.schedule_interval.lower()
        if interval.endswith('m'):
            minutes = int(interval[:-1])
            schedule.every(minutes).minutes.do(self._queue_job, package.package_id)
        elif interval.endswith('h'):
            hours = int(interval[:-1])
            schedule.every(hours).hours.do(self._queue_job, package.package_id)
        elif interval.endswith('d'):
            days = int(interval[:-1])
            schedule.every(days).days.do(self._queue_job, package.package_id)
        else:
            # Default to hourly if format not recognized
            schedule.every(1).hours.do(self._queue_job, package.package_id)

        logger.info(f"Added scraping package '{package.name}' (ID: {package.package_id}) to scheduler with interval {package.schedule_interval}")

    def remove_package(self, package_id: str) -> bool:
        """
        Remove a scraping package from the scheduler

        Args:
            package_id: ID of the package to remove

        Returns:
            bool: True if package was removed, False otherwise
        """
        if package_id in self.packages:
            # Remove all jobs for this package from the schedule
            schedule.clear(package_id)
            del self.packages[package_id]
            logger.info(f"Removed scraping package (ID: {package_id}) from scheduler")
            return True
        return False

    def _queue_job(self, package_id: str) -> None:
        """
        Add a job to the queue

        Args:
            package_id: ID of the package to run
        """
        job_queue.put(package_id)
        logger.debug(f"Queued scraping package (ID: {package_id})")

    def _worker(self) -> None:
        """Worker thread that processes the job queue"""
        while self.running:
            try:
                # Run pending scheduled tasks
                schedule.run_pending()

                # Process queue
                if not job_queue.empty():
                    package_id = job_queue.get(block=False)

                    if package_id in self.packages:
                        package = self.packages[package_id]
                        try:
                            package.run()
                        except Exception as e:
                            logger.error(f"Error running package {package_id}: {e}")
                    else:
                        logger.warning(f"Unknown package ID: {package_id}")

                    job_queue.task_done()

                # Sleep briefly
                time.sleep(1)

            except Exception as e:
                logger.error(f"Error in scheduler worker: {e}")
                time.sleep(5)  # Sleep longer after an error

    def start(self) -> None:
        """Start the scheduler worker thread"""
        if not self.running:
            self.running = True
            self.worker_thread = threading.Thread(target=self._worker, daemon=True)
            self.worker_thread.start()
            logger.info("Scraping scheduler started")

    def stop(self) -> None:
        """Stop the scheduler worker thread"""
        self.running = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5)
            self.worker_thread = None
        logger.info("Scraping scheduler stopped")

    def run_package_now(self, package_id: str) -> None:
        """
        Run a package immediately

        Args:
            package_id: ID of the package to run
        """
        if package_id in self.packages:
            self._queue_job(package_id)
            logger.info(f"Scheduled immediate run for package (ID: {package_id})")
        else:
            logger.warning(f"Cannot run unknown package (ID: {package_id})")


# Example usage
if __name__ == "__main__":
    # Test RSS feeds
    test_feeds = [
        "https://www.theverge.com/rss/index.xml",
        "https://feeds.feedburner.com/TechCrunch/",
        "https://www.wired.com/feed/rss"
    ]

    # Create test project and package IDs
    test_project_id = str(ObjectId())
    test_package_id = str(ObjectId())

    try:
        # Initialize the scraping package
        package = ScrapingPackage(
            package_id=test_package_id,
            project_ids=[test_project_id],
            name="Tech News Package",
            description="Combines tech news from The Verge, TechCrunch, and Wired",
            rss_feeds=test_feeds,
            schedule_interval="1h",
            max_articles_per_run=5
        )

        # Initialize the scheduler
        scheduler = ScrapingScheduler()
        scheduler.add_package(package)

        # Start the scheduler
        scheduler.start()

        # Run the package immediately for testing
        scheduler.run_package_now(test_package_id)

        # Keep the script running
        logger.info("Scheduler running. Press Ctrl+C to exit.")
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("Stopping scheduler...")
            scheduler.stop()
            logger.info("Done.")

    except Exception as e:
        logger.error(f"Error in main: {e}")
</file>

<file path="deploy/newsletter_agent/templates/final_index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agent Newsletter Generator</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body { padding-top: 20px; }
        .form-group { margin-bottom: 1rem; }
        #loading { display: none; }
        #streaming-window { display: none; }
        #result { display: none; }
        .card { margin-bottom: 20px; }

        /* Styling for the agent thoughts */
        #streaming-content {
            height: 400px;
            overflow-y: auto;
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 0.25rem;
            padding: 1rem;
            font-family: monospace;
            line-height: 1.4;
            font-size: 14px;
            color: #333;
            white-space: pre; /* Use pre to preserve whitespace */
        }

        /* Styling for the generated newsletter */
        #generated-content {
            line-height: 1.6;
            font-size: 16px;
        }

        /* Add some spacing between paragraphs in the newsletter */
        #generated-content p {
            margin-bottom: 1rem;
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-4">
            <h1>Agent Newsletter Generator</h1>
        </header>

        <div class="row">
            <div class="col-md-12">
                <div class="card">
                    <div class="card-header">
                        <h2>Generate Newsletter with AI Agent</h2>
                    </div>
                    <div class="card-body">
                        <form id="newsletter-form" method="post" enctype="multipart/form-data">
                            <div class="row">
                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="model_name"><strong>Model:</strong></label>
                                        <select class="form-control" id="model_name" name="model_name">
                                            <!-- Default to gpt-4.1 as the most capable model -->
                                            {% for model in openai_models %}
                                                {% if "gpt-4.1" in model and "web" in model %}
                                                    <option value="{{ model }}">OpenAI: {{ model }} (flagship model with web search)</option>
                                                {% elif "gpt-4.1" in model %}
                                                    <option value="{{ model }}">OpenAI: {{ model }} (flagship model)</option>
                                                {% elif "web" in model %}
                                                    <option value="{{ model }}">OpenAI: {{ model }} (with web search)</option>
                                                {% else %}
                                                    <option value="{{ model }}">OpenAI: {{ model }}</option>
                                                {% endif %}
                                            {% endfor %}
                                        </select>
                                        <small class="form-text text-muted">Models with "web" in the name will search the internet for the most up-to-date information. GPT-4.1 is the flagship model with a 1M token context window and enhanced capabilities.</small>
                                        <small class="form-text text-muted text-warning"><strong>Note:</strong> Anthropic (Claude) models may time out on deployment with large contexts. For best results, use OpenAI models.</small>
                                    </div>

                                    <div class="form-group">
                                        <label for="date_range"><strong>Date Range:</strong></label>
                                        <select class="form-control" id="date_range" name="date_range">
                                            <option value="all">All Time</option>
                                            <option value="7days">Last 7 Days</option>
                                            <option value="30days">Last 30 Days</option>
                                            <option value="90days">Last 90 Days</option>
                                        </select>
                                    </div>

                                    <div class="form-group">
                                        <label for="package_ids"><strong>Scraping Packages:</strong></label>
                                        <select class="form-control" id="package_ids" name="package_ids" multiple size="5">
                                            {% for package in packages %}
                                                <option value="{{ package._id }}" selected>{{ package.name|default('Unnamed Package') }}</option>
                                            {% endfor %}
                                        </select>
                                        <small class="form-text text-muted">All packages are selected by default. Hold Ctrl/Cmd to select specific packages.</small>
                                        <small class="form-text text-muted"><strong>Note:</strong> If no packages are selected, the system will automatically use a web-enabled model to search the internet for information.</small>
                                    </div>

                                    <div class="form-group">
                                        <label for="search_query"><strong>Search Query:</strong></label>
                                        <input type="text" class="form-control" id="search_query" name="search_query" placeholder="Enter search terms...">
                                        <small class="form-text text-muted">The agent will search for relevant articles using this query.</small>
                                    </div>

                                    <div class="form-group">
                                        <label for="article_limit"><strong>Article Limit:</strong></label>
                                        <select class="form-control" id="article_limit" name="article_limit">
                                            <option value="10">10 articles (default)</option>
                                            <option value="20">20 articles</option>
                                            <option value="30">30 articles</option>
                                            <option value="50">50 articles</option>
                                            <option value="100">100 articles (for large context models)</option>
                                        </select>
                                        <small class="form-text text-muted">Maximum number of articles to include in the context. Higher values work best with GPT-4.1.</small>
                                    </div>
                                </div>

                                <div class="col-md-6">
                                    <div class="form-group">
                                        <label for="client_context"><strong>Client Context:</strong></label>
                                        <textarea class="form-control" id="client_context" name="client_context" rows="3" placeholder="Information about the client..."></textarea>
                                    </div>

                                    <div class="form-group">
                                        <label for="project_context"><strong>Project Context:</strong></label>
                                        <textarea class="form-control" id="project_context" name="project_context" rows="3" placeholder="Information about the project..."></textarea>
                                    </div>

                                    <div class="form-group">
                                        <label for="context_files"><strong>Additional Context Files:</strong></label>
                                        <input type="file" class="form-control" id="context_files" name="context_files" multiple>
                                        <small class="form-text text-muted">Upload TXT, PDF, MD, or DOCX files with additional context.</small>
                                    </div>
                                </div>
                            </div>

                            <div class="form-group mt-3">
                                <label for="prompt"><strong>Newsletter Instructions:</strong></label>
                                <textarea class="form-control" id="prompt" name="prompt" rows="5" placeholder="Detailed instructions for the newsletter..."></textarea>
                            </div>

                            <div class="form-group mt-3 d-flex justify-content-between">
                                <div>
                                    <button type="submit" class="btn btn-primary" id="generate-btn">Generate Newsletter</button>
                                </div>
                                <div>
                                    <label for="json-import" class="btn btn-outline-secondary">Import from JSON</label>
                                    <input type="file" id="json-import" accept=".json" style="display: none;">
                                </div>
                            </div>
                        </form>

                        <div id="loading" class="mt-4">
                            <div class="d-flex justify-content-center">
                                <div class="spinner-border text-primary" role="status">
                                    <span class="visually-hidden">Loading...</span>
                                </div>
                            </div>
                            <p class="text-center mt-2">Connecting to streaming service...</p>
                        </div>

                        <!-- Streaming Window for Agent Thoughts -->
                        <div id="streaming-window" class="mt-4">
                            <div class="card">
                                <div class="card-header">
                                    <h3>Agent Activity</h3>
                                </div>
                                <div class="card-body">
                                    <div id="streaming-content"></div>
                                </div>
                            </div>
                        </div>

                        <!-- Result Window for Generated Newsletter -->
                        <div id="result" class="mt-4">
                            <div class="card">
                                <div class="card-header d-flex justify-content-between align-items-center">
                                    <h3>Generated Newsletter</h3>
                                    <div>
                                        <button class="btn btn-sm btn-outline-secondary me-2" id="download-btn">Download JSON</button>
                                        <button class="btn btn-sm btn-outline-secondary" id="copy-btn">Copy to Clipboard</button>
                                    </div>
                                </div>
                                <div class="card-body">
                                    <div id="generated-content"></div>
                                    <div id="web-search-results" class="mt-4" style="display: none;">
                                        <div class="card">
                                            <div class="card-header d-flex justify-content-between align-items-center">
                                                <h4>Web Search Results</h4>
                                                <button class="btn btn-sm btn-outline-secondary" id="copy-search-btn">Copy Results</button>
                                            </div>
                                            <div class="card-body">
                                                <div id="web-search-content" class="border p-3 bg-light" style="max-height: 400px; overflow-y: auto;"></div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <!-- Context Used Section -->
                            <div class="card mt-3">
                                <div class="card-header">
                                    <h3>Context Used <small class="text-muted">(<span id="articles-count">0</span> articles)</small></h3>
                                </div>
                                <div class="card-body">
                                    <pre id="context-used" class="border p-3 bg-light" style="max-height: 300px; overflow-y: auto; white-space: pre-wrap; font-size: 14px;"></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <footer class="mt-5 py-3 text-center text-secondary">
            <p>&copy; {{ now.year }} Proton CRM</p>
        </footer>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        // Set default model based on environment
        const modelSelect = document.getElementById('model_name');
        if (modelSelect) {
            // Check if we're on render.com deployment (look for render.com in the URL)
            const isDeployment = window.location.hostname.includes('render.com');

            if (isDeployment) {
                // On deployment, prefer OpenAI models
                const gpt41Option = Array.from(modelSelect.options).find(option => option.value === 'gpt-4.1');
                const gpt41WebOption = Array.from(modelSelect.options).find(option => option.value === 'gpt-4.1-web');

                if (gpt41WebOption) {
                    // Prefer web-enabled model on deployment
                    modelSelect.value = gpt41WebOption.value;
                } else if (gpt41Option) {
                    modelSelect.value = gpt41Option.value;
                } else {
                    // Fallback to any OpenAI model
                    const openaiOption = Array.from(modelSelect.options).find(option => option.text.includes('OpenAI'));
                    if (openaiOption) {
                        modelSelect.value = openaiOption.value;
                    }
                }
            }
        }
        const form = document.getElementById('newsletter-form');
        const generateBtn = document.getElementById('generate-btn');
        const loading = document.getElementById('loading');
        const streamingWindow = document.getElementById('streaming-window');
        const streamingContent = document.getElementById('streaming-content');
        const result = document.getElementById('result');
        const generatedContent = document.getElementById('generated-content');
        const copyBtn = document.getElementById('copy-btn');
        const downloadBtn = document.getElementById('download-btn');
        const jsonImport = document.getElementById('json-import');

        // Store form data and results for download
        let currentFormData = {};
        let currentResults = {};

        form.addEventListener('submit', function(e) {
            e.preventDefault();

            // Clear previous content
            streamingContent.textContent = '';
            generatedContent.innerHTML = '';

            // Show loading indicator
            loading.style.display = 'block';
            streamingWindow.style.display = 'none';
            result.style.display = 'none';
            generateBtn.disabled = true;

            // Submit form data via AJAX
            const formData = new FormData(form);

            // Store form data for download
            currentFormData = {
                model_name: formData.get('model_name'),
                date_range: formData.get('date_range'),
                search_query: formData.get('search_query'),
                client_context: formData.get('client_context'),
                project_context: formData.get('project_context'),
                prompt: formData.get('prompt'),
                article_limit: formData.get('article_limit')
            };

            // Get selected package IDs
            const packageSelect = document.getElementById('package_ids');
            currentFormData.package_ids = Array.from(packageSelect.selectedOptions).map(option => option.value);

            // First send the form data to start the generation process
            fetch('/generate', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                if (data.error) {
                    alert('Error: ' + data.error);
                    loading.style.display = 'none';
                    generateBtn.disabled = false;
                    return;
                }

                // Hide loading and show streaming window
                loading.style.display = 'none';
                streamingWindow.style.display = 'block';

                // Connect to the streaming endpoint for agent thoughts
                const eventSource = new EventSource('/stream-thoughts');

                // Handle incoming messages
                eventSource.onmessage = function(event) {
                    if (event.data === '[DONE]') {
                        // End of stream
                        eventSource.close();

                        // Get the final newsletter content
                        fetch('/get-newsletter')
                            .then(response => response.json())
                            .then(data => {
                                // Show the final result
                                result.style.display = 'block';

                                // Check if the content contains web search results
                                const webSearchResultsDiv = document.getElementById('web-search-results');
                                const webSearchContentDiv = document.getElementById('web-search-content');

                                if (data.content && data.content.includes('WEB SEARCH RESULTS')) {
                                    // Split the content to separate the newsletter from the web search results
                                    const parts = data.content.split('WEB SEARCH RESULTS');
                                    const newsletterContent = parts[0];
                                    const searchResults = 'WEB SEARCH RESULTS' + parts[1];

                                    // Display the newsletter content
                                    generatedContent.innerHTML = formatContent(newsletterContent);

                                    // Display the web search results
                                    webSearchContentDiv.innerHTML = formatContent(searchResults);
                                    webSearchResultsDiv.style.display = 'block';
                                } else {
                                    // Display the full content
                                    generatedContent.innerHTML = formatContent(data.content);
                                    webSearchResultsDiv.style.display = 'none';
                                }

                                // Update context information
                                document.getElementById('context-used').textContent = data.context_used;
                                document.getElementById('articles-count').textContent = data.articles_count;

                                // Store results for download
                                currentResults = {
                                    content: data.content,
                                    context_used: data.context_used,
                                    articles_count: data.articles_count,
                                    timestamp: new Date().toISOString()
                                };

                                // Re-enable the generate button
                                generateBtn.disabled = false;

                                // Scroll to result
                                result.scrollIntoView({ behavior: 'smooth' });
                            })
                            .catch(error => {
                                console.error('Error fetching newsletter:', error);
                                generateBtn.disabled = false;
                            });
                    } else if (event.data.trim() !== '') {
                        // Decode the SSE-encoded newlines
                        const decodedData = event.data.replace(/\\n/g, '\n');

                        // Append to streaming content
                        streamingContent.textContent += decodedData;

                        // Scroll to bottom
                        streamingContent.scrollTop = streamingContent.scrollHeight;
                    }
                };

                eventSource.onerror = function(error) {
                    console.error('EventSource error:', error);
                    eventSource.close();
                    generateBtn.disabled = false;
                    alert('Error in streaming. Please try again.');
                };
            })
            .catch(error => {
                console.error('Fetch error:', error);
                loading.style.display = 'none';
                generateBtn.disabled = false;
                alert('Error: ' + error);
            });
        });

        // Copy to clipboard functionality
        copyBtn.addEventListener('click', function() {
            const content = document.getElementById('generated-content').innerText;
            navigator.clipboard.writeText(content).then(() => {
                const originalText = copyBtn.textContent;
                copyBtn.textContent = 'Copied!';
                setTimeout(() => {
                    copyBtn.textContent = originalText;
                }, 2000);
            });
        });

        // Copy web search results to clipboard
        document.getElementById('copy-search-btn').addEventListener('click', function() {
            const content = document.getElementById('web-search-content').innerText;
            navigator.clipboard.writeText(content).then(() => {
                const originalText = this.textContent;
                this.textContent = 'Copied!';
                setTimeout(() => {
                    this.textContent = originalText;
                }, 2000);
            });
        });

        // Download JSON functionality
        downloadBtn.addEventListener('click', function() {
            // Create a combined object with inputs and outputs
            const downloadData = {
                inputs: currentFormData,
                outputs: currentResults
            };

            // Convert to JSON string
            const jsonString = JSON.stringify(downloadData, null, 2);

            // Create a blob and download link
            const blob = new Blob([jsonString], { type: 'application/json' });
            const url = URL.createObjectURL(blob);

            // Create a temporary link and trigger download
            const a = document.createElement('a');
            a.href = url;
            a.download = `newsletter_${new Date().toISOString().replace(/[:.]/g, '-')}.json`;
            document.body.appendChild(a);
            a.click();

            // Clean up
            setTimeout(() => {
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
            }, 0);
        });

        // Handle JSON import
        jsonImport.addEventListener('change', function(e) {
            if (!this.files || !this.files[0]) return;

            const file = this.files[0];
            if (!file.name.endsWith('.json')) {
                alert('Please select a JSON file');
                return;
            }

            // Show loading indicator
            loading.style.display = 'block';
            streamingWindow.style.display = 'none';
            result.style.display = 'none';
            generateBtn.disabled = true;

            // Create FormData and append the file
            const formData = new FormData();
            formData.append('json_file', file);

            // Send the file to the server
            fetch('/import-json', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                // Hide loading indicator
                loading.style.display = 'none';
                generateBtn.disabled = false;

                if (data.error) {
                    alert('Error: ' + data.error);
                    return;
                }

                // Update the form with the imported data
                const inputs = data.inputs;
                if (inputs) {
                    // Update form fields
                    if (inputs.model_name) document.getElementById('model_name').value = inputs.model_name;
                    if (inputs.date_range) document.getElementById('date_range').value = inputs.date_range;
                    if (inputs.search_query) document.getElementById('search_query').value = inputs.search_query;
                    if (inputs.client_context) document.getElementById('client_context').value = inputs.client_context;
                    if (inputs.project_context) document.getElementById('project_context').value = inputs.project_context;
                    if (inputs.prompt) document.getElementById('prompt').value = inputs.prompt;
                    if (inputs.article_limit) document.getElementById('article_limit').value = inputs.article_limit;

                    // Update package selection
                    if (inputs.package_ids && inputs.package_ids.length > 0) {
                        const packageSelect = document.getElementById('package_ids');
                        Array.from(packageSelect.options).forEach(option => {
                            option.selected = inputs.package_ids.includes(option.value);
                        });
                    }

                    // Store the imported form data
                    currentFormData = inputs;
                }

                // Update the results
                const outputs = data.outputs;
                if (outputs) {
                    // Show the result section
                    result.style.display = 'block';

                    // Update the content
                    generatedContent.innerHTML = formatContent(outputs.content);
                    document.getElementById('context-used').textContent = outputs.context_used;
                    document.getElementById('articles-count').textContent = outputs.articles_count;

                    // Store the imported results
                    currentResults = {
                        content: outputs.content,
                        context_used: outputs.context_used,
                        articles_count: outputs.articles_count,
                        timestamp: new Date().toISOString()
                    };

                    // Scroll to result
                    result.scrollIntoView({ behavior: 'smooth' });
                }

                // Reset the file input
                jsonImport.value = '';
            })
            .catch(error => {
                console.error('Error importing JSON:', error);
                loading.style.display = 'none';
                generateBtn.disabled = false;
                alert('Error importing JSON: ' + error);

                // Reset the file input
                jsonImport.value = '';
            });
        });

        // Simple Markdown-like formatting
        function formatContent(text) {
            if (!text) return '';

            // Replace newlines with <br>
            text = text.replace(/\n/g, '<br>');

            // Format headers
            text = text.replace(/^# (.+)$/gm, '<h1>$1</h1>');
            text = text.replace(/^## (.+)$/gm, '<h2>$1</h2>');
            text = text.replace(/^### (.+)$/gm, '<h3>$1</h3>');

            // Format bold and italic
            text = text.replace(/\*\*(.+?)\*\*/g, '<strong>$1</strong>');
            text = text.replace(/\*(.+?)\*/g, '<em>$1</em>');

            // Format lists
            text = text.replace(/^- (.+)$/gm, '<li>$1</li>');
            text = text.replace(/(<li>.+<\/li>\n)+/g, '<ul>$&</ul>');

            return text;
        }
    });
    </script>
</body>
</html>
</file>

<file path="deploy/scraper/requirements.txt">
# Database
pymongo==4.5.0
dnspython==2.4.2
python-dotenv==1.0.0

# Web scraping
feedparser==6.0.10
requests==2.31.0
beautifulsoup4==4.12.2
lxml==4.9.2
html5lib==1.1

# Core dependencies
numpy==1.23.5
scipy==1.10.1
python-dateutil==2.8.2

# Text processing
nltk==3.8.1
scikit-learn==1.0.2

# Utils
tqdm==4.66.1
argparse==1.4.0
schedule==1.2.0

# Vector embeddings
openai==1.3.5
torch==1.13.1
sentence-transformers==2.2.2
</file>

<file path="deploy/newsletter_agent/agent_newsletter_generator.py">
"""
Agent-Based Newsletter Generator

This module provides an agent-based approach to newsletter generation,
replacing the RAG-based approach while maintaining the same interface.
The agent can query the MongoDB vector database, process context,
and generate newsletters based on user instructions.
"""

import os
import datetime
import logging
import json
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv
import pymongo
from bson.objectid import ObjectId
import openai
from openai import OpenAI
import dns.resolver
import anthropic
from anthropic import Anthropic

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

class NewsletterAgent:
    """
    Agent for generating newsletters based on MongoDB vector database
    and user instructions.
    """

    def __init__(self):
        """Initialize the newsletter agent with API clients and database connection"""
        # Initialize OpenAI client
        self.openai_api_key = os.environ.get('OPENAI_API_KEY')
        self.openai_client = None
        if self.openai_api_key:
            try:
                # Create client with minimal arguments to avoid proxy issues
                import httpx
                # First try with default settings
                try:
                    self.openai_client = OpenAI(api_key=self.openai_api_key)
                    logger.info("OpenAI client initialized with default settings")
                except TypeError as te:
                    if 'proxies' in str(te):
                        # If there's a proxy issue, try with a custom httpx client
                        logger.info("Trying OpenAI client with custom httpx client")
                        http_client = httpx.Client()
                        self.openai_client = OpenAI(api_key=self.openai_api_key, http_client=http_client)
                        logger.info("OpenAI client initialized with custom httpx client")
                    else:
                        raise
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")
                logger.error(f"OpenAI API Key: {self.openai_api_key[:5]}...{self.openai_api_key[-5:] if len(self.openai_api_key) > 10 else ''}")
        else:
            logger.error("OPENAI_API_KEY environment variable not set or empty")
            logger.error(f"Environment variables: {[k for k in os.environ.keys() if not k.startswith('PATH')]}")

        # Initialize Anthropic client if needed
        self.anthropic_api_key = os.environ.get('ANTHROPIC_API_KEY')
        self.anthropic_client = None
        if self.anthropic_api_key:
            try:
                # First try with default settings
                try:
                    # Initialize with the API key and version header
                    self.anthropic_client = Anthropic(
                        api_key=self.anthropic_api_key,
                        # Set the API version explicitly
                        default_headers={
                            "anthropic-version": "2023-06-01"
                        }
                    )
                    logger.info("Anthropic client initialized with default settings and explicit version header")

                    # Log the client details
                    if hasattr(self.anthropic_client, 'api_key'):
                        masked_key = f"{self.anthropic_api_key[:5]}...{self.anthropic_api_key[-5:] if len(self.anthropic_api_key) > 10 else ''}"
                        logger.info(f"Anthropic client API key: {masked_key}")

                    if hasattr(self.anthropic_client, 'default_headers'):
                        logger.info(f"Anthropic client headers: {self.anthropic_client.default_headers}")

                    # Test if the client has the messages attribute
                    if hasattr(self.anthropic_client, 'messages'):
                        logger.info("Anthropic client has messages attribute")
                    else:
                        logger.warning("Anthropic client does not have messages attribute")

                except TypeError as te:
                    if 'proxies' in str(te):
                        # If there's a proxy issue, try with a custom httpx client
                        logger.info("Trying Anthropic client with custom httpx client")
                        import httpx
                        http_client = httpx.Client()
                        self.anthropic_client = Anthropic(
                            api_key=self.anthropic_api_key,
                            http_client=http_client,
                            default_headers={
                                "anthropic-version": "2023-06-01"
                            }
                        )
                        logger.info("Anthropic client initialized with custom httpx client")
                    else:
                        raise
            except Exception as e:
                logger.error(f"Failed to initialize Anthropic client: {e}")
                logger.error(f"Anthropic API Key: {self.anthropic_api_key[:5]}...{self.anthropic_api_key[-5:] if len(self.anthropic_api_key) > 10 else ''}")
        else:
            logger.warning("ANTHROPIC_API_KEY environment variable not set or empty")

        # MongoDB connection
        self.mongodb_uri = os.environ.get('MONGODB_URI')
        self.mongodb_db_name = os.environ.get('MONGODB_DB_NAME', 'proton')
        self.mongo_client = None
        self.db = None

        if not self.mongodb_uri:
            logger.error("MONGODB_URI environment variable not set or empty")
            logger.error(f"Environment variables: {[k for k in os.environ.keys() if not k.startswith('PATH')]}")
        else:
            # Connect to MongoDB
            self._connect_to_mongodb()

    def _connect_to_mongodb(self):
        """Connect to MongoDB database"""
        if not self.mongodb_uri:
            logger.error("MONGODB_URI environment variable not set")
            return

        try:
            # Check if we're in local development mode
            local_mode = os.environ.get('LOCAL', 'False').lower() in ('true', '1', 't')

            if local_mode and 'mongodb+srv' in self.mongodb_uri:
                logger.info("Using local DNS resolver for MongoDB Atlas connection")
                # Configure DNS resolver to use Google's DNS servers
                dns.resolver.default_resolver = dns.resolver.Resolver(configure=False)
                dns.resolver.default_resolver.nameservers = ['8.8.8.8', '1.1.1.1']

                # Set pymongo to use our DNS resolver
                pymongo.uri_parser.SRV_SCHEME_TXT = 'mongodb+srv'

            # Connect to MongoDB
            self.mongo_client = pymongo.MongoClient(self.mongodb_uri)
            # Test the connection
            self.mongo_client.admin.command('ping')
            self.db = self.mongo_client[self.mongodb_db_name]

            # Log database info
            collections = self.db.list_collection_names()
            logger.info(f"Connected to MongoDB database: {self.mongodb_db_name}")
            logger.info(f"Available collections: {collections}")

            # Check if scraping_packages collection exists
            try:
                if 'scraping_packages' in collections:
                    package_count = self.db.scraping_packages.count_documents({})
                    logger.info(f"Found {package_count} scraping packages")
                else:
                    logger.warning("No scraping_packages collection found")
            except Exception as e:
                logger.error(f"Error checking scraping_packages collection: {e}")

            # Check if articles collection exists
            try:
                if 'articles' in collections:
                    article_count = self.db.articles.count_documents({})
                    logger.info(f"Found {article_count} articles")
                else:
                    logger.warning("No articles collection found")
            except Exception as e:
                logger.error(f"Error checking articles collection: {e}")

        except Exception as e:
            logger.error(f"Failed to connect to MongoDB: {e}")
            self.mongo_client = None
            self.db = None

    def close(self):
        """Close MongoDB connection"""
        if self.mongo_client:
            self.mongo_client.close()

    def get_scraping_packages(self) -> List[Dict[str, Any]]:
        """Get all scraping packages from the database"""
        if self.db is None:
            logger.error("No database connection")
            return []

        try:
            # Get all packages
            logger.info("Attempting to retrieve scraping packages from database")
            packages = list(self.db.scraping_packages.find({}))
            logger.info(f"Successfully retrieved {len(packages)} scraping packages")

            # Convert ObjectId to string for each package
            for package in packages:
                if '_id' in package and isinstance(package['_id'], ObjectId):
                    package['_id'] = str(package['_id'])

            return packages
        except Exception as e:
            logger.error(f"Error getting scraping packages: {e}")
            # Try a more direct approach as fallback
            try:
                logger.info("Trying fallback method to retrieve scraping packages")
                # Use a simpler query with explicit collection reference
                collection = self.db['scraping_packages']
                packages = list(collection.find({}))
                logger.info(f"Fallback retrieved {len(packages)} scraping packages")

                # Convert ObjectId to string for each package
                for package in packages:
                    if '_id' in package and isinstance(package['_id'], ObjectId):
                        package['_id'] = str(package['_id'])

                return packages
            except Exception as e2:
                logger.error(f"Fallback also failed: {e2}")
                return []

    def get_vector_search_filter(self, query, filters, db, limit=10):
        """
        Perform search with filters, with fallback options for non-Atlas MongoDB

        Args:
            query: Search query text
            filters: MongoDB filters to apply
            db: MongoDB database connection
            limit: Maximum number of results to return

        Returns:
            List of matching articles
        """
        try:
            # Log the filters being applied
            logger.info(f"Applying filters: {filters}")

            # Skip vector search for local MongoDB (requires Atlas)
            # Just use regular search methods

            # Try text search if available
            try:
                # Check if text index exists - use a more compatible approach
                try:
                    indexes = list(db.articles.list_indexes())
                    index_names = [idx.get("name") for idx in indexes]
                    logger.info(f"Available indexes: {index_names}")
                except Exception as idx_error:
                    logger.warning(f"Could not list indexes: {idx_error}")
                    index_names = []

                has_text_index = any("text" in idx for idx in index_names)

                if has_text_index and query:
                    logger.info("Using text search")
                    # Create text search filter
                    text_filter = {"$text": {"$search": query}}
                    combined_filter = {**text_filter, **filters} if filters else text_filter

                    # Perform the search
                    results = list(db.articles.find(combined_filter).sort("published_date", -1).limit(limit))
                    logger.info(f"Text search returned {len(results)} results")

                    if results:
                        return results
            except Exception as text_error:
                logger.error(f"Error in text search: {text_error}")

            # If text search failed or returned no results, try regex search
            if query:
                logger.info("Using regex search")
                # Try searching in title and content fields
                relaxed_filter = {
                    "$or": [
                        {"title": {"$regex": query, "$options": "i"}},
                        {"content": {"$regex": query, "$options": "i"}},
                        {"summary": {"$regex": query, "$options": "i"}}
                    ]
                }
                if filters:
                    # Combine with date and package filters
                    combined_filter = {"$and": [relaxed_filter, filters]}
                else:
                    combined_filter = relaxed_filter

                results = list(db.articles.find(combined_filter).sort("published_date", -1).limit(limit))
                logger.info(f"Regex search returned {len(results)} results")

                if results:
                    return results

            # If all else fails or no query provided, just use the filters
            logger.info("Using filter-only search")
            results = list(db.articles.find(filters if filters else {}).sort("published_date", -1).limit(limit))
            logger.info(f"Filter-only search returned {len(results)} results")

            return results
        except Exception as e:
            logger.error(f"Error performing search: {e}")
            return []

    def get_date_filter(self, date_range):
        """Get MongoDB date filter based on selected range"""
        now = datetime.datetime.now(datetime.timezone.utc)
        logger.info(f"Filtering by date range: {date_range}")

        if date_range == "7days":
            # Last 7 days
            start_date = now - datetime.timedelta(days=7)
            logger.info(f"Date filter: from {start_date} to {now}")
            return {"published_date": {"$gte": start_date}}
        elif date_range == "30days":
            # Last 30 days
            start_date = now - datetime.timedelta(days=30)
            logger.info(f"Date filter: from {start_date} to {now}")
            return {"published_date": {"$gte": start_date}}
        elif date_range == "90days":
            # Last 90 days
            start_date = now - datetime.timedelta(days=90)
            logger.info(f"Date filter: from {start_date} to {now}")
            return {"published_date": {"$gte": start_date}}
        else:
            # All time (no filter)
            logger.info("No date filter applied (all time)")
            return {}

    def run_agent_with_query(
        self,
        search_query: str = "",
        date_range: str = "all",
        package_ids: List[str] = None,
        client_context: str = "",
        project_context: str = "",
        prompt: str = "",
        uploaded_context: str = "",
        model: str = "gpt-4o",
        article_limit: int = 10,
        stream_callback=None
    ) -> Dict[str, Any]:
        """
        Run the agent with the given query and parameters

        Args:
            search_query: Search query for finding relevant articles
            date_range: Date range for filtering articles (7days, 30days, 90days, all)
            package_ids: List of scraping package IDs to include
            client_context: Context about the client
            project_context: Context about the project
            prompt: Specific instructions for the newsletter
            uploaded_context: Additional context from uploaded files
            model: Model to use for generation (gpt-4.1, gpt-4o, gpt-4-turbo, etc.)
            article_limit: Maximum number of articles to include in the context (default: 10)
            stream_callback: Callback function for streaming output

        Returns:
            Dictionary with generated content and metadata
        """
        if self.db is None:
            error_msg = "No database connection"
            logger.error(error_msg)
            if stream_callback:
                stream_callback(f"Agent Error: {error_msg}\n")
            return {"error": error_msg, "generated_content": "Error: Database not connected", "context_used": "", "articles_count": 0}

        try:
            # Build filters
            filters = {}

            # Add date filter
            date_filter = self.get_date_filter(date_range)
            if date_filter:
                filters.update(date_filter)

            # Add package filter
            if package_ids and len(package_ids) > 0:
                # Log the package IDs we're working with
                logger.info(f"Processing package IDs: {package_ids}")
                if stream_callback:
                    stream_callback(f"Agent: Processing {len(package_ids)} selected package(s)\n")

                # Convert string IDs to ObjectId
                object_ids = []
                for pid in package_ids:
                    try:
                        # Check if it's already an ObjectId string
                        if isinstance(pid, str) and len(pid) == 24:
                            object_ids.append(ObjectId(pid))
                        else:
                            logger.warning(f"Skipping invalid package ID format: {pid}")
                    except Exception as e:
                        logger.warning(f"Invalid package ID: {pid} - {e}")
                        if stream_callback:
                            stream_callback(f"Agent Warning: Invalid package ID: {pid}\n")

                if object_ids:
                    logger.info(f"Using {len(object_ids)} valid package IDs as filter")
                    filters["scraping_package_id"] = {"$in": object_ids}
                else:
                    logger.warning("No valid package IDs found, not applying package filter")
            else:
                logger.info("No package IDs provided, not applying package filter")

            # Check if we should search for articles
            articles = []

            # Check if we have package IDs and should search for articles
            if package_ids and len(package_ids) > 0:
                try:
                    # Use the article_limit parameter
                    logger.info(f"Searching for articles with limit: {article_limit}")
                    if stream_callback:
                        stream_callback(f"Agent: Searching for up to {article_limit} relevant articles...\n")

                    articles = self.get_vector_search_filter(
                        query=search_query,
                        filters=filters,
                        db=self.db,
                        limit=article_limit  # Use the article_limit parameter
                    )

                    if stream_callback:
                        stream_callback(f"Agent: Found {len(articles)} articles matching criteria\n")

                    # Log article titles for debugging
                    logger.info(f"Retrieved {len(articles)} articles")
                    for i, article in enumerate(articles[:5]):  # Log first 5 articles
                        logger.info(f"Article {i+1}: {article.get('title', 'No title')}")

                    if len(articles) > 5:
                        logger.info(f"... and {len(articles) - 5} more articles")

                except Exception as e:
                    logger.error(f"Error performing vector search: {e}")
                    if stream_callback:
                        stream_callback(f"Agent Error: Failed to search articles: {str(e)}\n")
                    articles = []
            else:
                # No package IDs provided, we'll rely on web search
                logger.info("No package IDs provided, will rely on web search instead")
                if stream_callback:
                    stream_callback(f"Agent: No scraping packages selected, will rely on web search for information\n")
                    stream_callback(f"Agent: Make sure to use a web-enabled model for best results\n")
                articles = []

            # Build context
            context = ""

            # Add client context
            if client_context:
                context += "CLIENT CONTEXT:\n"
                context += client_context.strip() + "\n\n"

            # Add project context
            if project_context:
                context += "PROJECT CONTEXT:\n"
                context += project_context.strip() + "\n\n"

            # Add articles
            if articles:
                context += "RELEVANT ARTICLES:\n"
                for i, article in enumerate(articles, 1):
                    # Format date
                    pub_date = article.get('published_date', 'Unknown date')
                    if isinstance(pub_date, datetime.datetime):
                        pub_date = pub_date.strftime('%Y-%m-%d')

                    # Get source URL
                    source_url = article.get('source_url', '')

                    # Get package name
                    package_name = article.get('package_name', 'Unknown package')

                    # Add article to context
                    context += f"[{i}] {article.get('title', 'Untitled')}\n"
                    context += f"Source: {article.get('source_name', 'Unknown source')}\n"
                    if source_url:
                        context += f"URL: {source_url}\n"
                    context += f"Date: {pub_date}\n"
                    context += f"Package: {package_name}\n"
                    context += f"Summary: {article.get('summary', 'No summary available')}\n"

                    # Add a snippet of the content if available
                    content = article.get('content', '')
                    if content:
                        # Limit to first 300 characters
                        snippet = content[:300] + "..." if len(content) > 300 else content
                        context += f"Content snippet: {snippet}\n"

                    context += "\n"

            # Add uploaded context
            if uploaded_context:
                context += "UPLOADED CONTEXT:\n"
                context += uploaded_context.strip() + "\n\n"

            # Log the context
            logger.info(f"Context length: {len(context)} characters")
            if stream_callback:
                stream_callback(f"Agent: Collected {len(articles)} relevant articles for context.\n")
                stream_callback(f"Agent: Total context length: {len(context)} characters.\n")

            # Generate content
            if model.startswith("gpt-"):
                # Use OpenAI
                if stream_callback:
                    # Check if this is a web-enabled model
                    if "web" in model:
                        stream_callback(f"Agent: Using OpenAI model: {model} with web search enabled\n")
                        stream_callback(f"Agent: This model will search the web for the most up-to-date information\n")
                    else:
                        stream_callback(f"Agent: Using OpenAI model: {model}\n")

                if not self.openai_client:
                    error_msg = "OpenAI client not initialized. Check API key."
                    logger.error(error_msg)
                    if stream_callback:
                        stream_callback(f"Agent Error: {error_msg}\n")
                    return {"error": error_msg}

                # Handle web-enabled models
                if "web" in model:
                    # Extract the base model name without the -web suffix
                    base_model = model.replace("-web", "")
                    if stream_callback:
                        stream_callback(f"Agent: Using base model {base_model} with web search enabled\n")
                        stream_callback(f"Agent: The model will search the web for the most up-to-date information\n")

                    # Generate content with web search
                    generated_content = self.generate_with_openai(prompt, context, base_model, stream_callback)

                    # Check if web search results are included in the response
                    if "WEB SEARCH RESULTS" in generated_content:
                        # Extract and log the web search results
                        web_search_section = generated_content.split("WEB SEARCH RESULTS")[1].strip()
                        logger.info(f"Web search results found in response: {web_search_section[:500]}...")

                        if stream_callback:
                            stream_callback(f"Agent: Web search results found in the response\n")
                    else:
                        logger.info("No explicit web search results section found in the response")
                else:
                    generated_content = self.generate_with_openai(prompt, context, model, stream_callback)

            elif model.startswith("claude-"):
                # Use Anthropic
                if stream_callback:
                    stream_callback(f"Agent: Using Anthropic model: {model}\n")
                    stream_callback(f"Agent: This is a powerful model for complex reasoning and creative tasks\n")

                if not self.anthropic_client:
                    error_msg = "Anthropic client not initialized. Check API key."
                    logger.error(error_msg)
                    if stream_callback:
                        stream_callback(f"Agent Error: {error_msg}\n")
                    return {"error": error_msg}

                # Log the model being used
                logger.info(f"Using Anthropic model: {model}")

                # Generate content with Anthropic
                try:
                    if stream_callback:
                        stream_callback(f"Agent: Sending request to Anthropic API...\n")
                    generated_content = self.generate_with_anthropic(prompt, context, model, stream_callback)
                    if stream_callback:
                        stream_callback(f"Agent: Successfully received response from Anthropic\n")
                except Exception as e:
                    error_msg = f"Error generating with Anthropic: {str(e)}"
                    logger.error(error_msg)
                    if stream_callback:
                        stream_callback(f"Agent Error: {error_msg}\n")

                    # Try with the latest model as fallback
                    fallback_model = "claude-3-7-sonnet-20250219"
                    if model != fallback_model:
                        if stream_callback:
                            stream_callback(f"Agent: Trying fallback model {fallback_model}\n")
                        try:
                            generated_content = self.generate_with_anthropic(prompt, context, fallback_model, stream_callback)
                            if stream_callback:
                                stream_callback(f"Agent: Successfully received response from fallback model\n")
                        except Exception as fallback_e:
                            error_msg = f"Error with fallback model: {str(fallback_e)}"
                            logger.error(error_msg)
                            if stream_callback:
                                stream_callback(f"Agent Error: {error_msg}\n")
                            return {"error": error_msg}
                    else:
                        # If Anthropic completely fails, try OpenAI as a last resort
                        if self.openai_client:
                            if stream_callback:
                                stream_callback(f"Agent: Anthropic API failed, falling back to OpenAI gpt-4.1 model\n")
                            try:
                                # Use GPT-4.1 as the most capable fallback
                                fallback_openai_model = "gpt-4.1"
                                generated_content = self.generate_with_openai(prompt, context, fallback_openai_model, stream_callback)
                                if stream_callback:
                                    stream_callback(f"Agent: Successfully received response from OpenAI fallback\n")
                                return {
                                    "generated_content": generated_content,
                                    "context_used": context,
                                    "articles_count": len(articles)
                                }
                            except Exception as openai_e:
                                logger.error(f"OpenAI fallback also failed: {openai_e}")
                                if stream_callback:
                                    stream_callback(f"Agent Error: OpenAI fallback also failed: {str(openai_e)}\n")
                                return {"error": f"{error_msg}\nOpenAI fallback also failed: {str(openai_e)}"}
                        else:
                            return {"error": error_msg}
            else:
                # Default to OpenAI
                if stream_callback:
                    stream_callback(f"Agent: Model {model} not recognized, defaulting to gpt-4o\n")
                if not self.openai_client:
                    error_msg = "OpenAI client not initialized. Check API key."
                    logger.error(error_msg)
                    if stream_callback:
                        stream_callback(f"Agent Error: {error_msg}\n")
                    return {"error": error_msg}
                generated_content = self.generate_with_openai(prompt, context, "gpt-4o")

            # Return the result
            return {
                "generated_content": generated_content,
                "context_used": context,
                "articles_count": len(articles)
            }

        except Exception as e:
            logger.error(f"Error running agent: {e}")
            return {"error": str(e)}

    def generate_with_openai(self, prompt, context, model="gpt-4o", stream_callback=None):
        """Generate content using OpenAI"""
        if not self.openai_client:
            return "Error: OpenAI API key not configured"

        # Check if we're using GPT-4.1
        is_gpt41 = model.startswith("gpt-4.1")
        if is_gpt41:
            logger.info(f"Using GPT-4.1 model: {model}")
            logger.info("GPT-4.1 has a 1M token context window and improved capabilities")

        try:
            # Check if we should use web search
            use_web_search = "web" in model.lower() or "search" in prompt.lower() or "latest" in prompt.lower()

            system_message = """You are a newsletter generation assistant that creates high-quality newsletters based on the provided context and instructions.
            Your task is to synthesize information from the provided articles and create a professional, well-structured newsletter.
            Focus on creating valuable insights and actionable information for the reader.
            Include proper formatting with headers, bullet points, and sections as appropriate.
            """

            # Check if we have any articles in the context
            has_articles = "RELEVANT ARTICLES:" in context

            if use_web_search:
                # Adjust the system message based on whether we have articles
                if not has_articles:
                    system_message += """
                    You have access to web search to find information on the topic.
                    Since no articles were provided from the database, you should rely ENTIRELY on web search to create the newsletter.
                    Be thorough in your search to find the most relevant and up-to-date information.
                    Focus on high-quality, reputable sources for your information.
                    Always include proper citations and links to sources in your newsletter.

                    IMPORTANT: You MUST include a section at the end of your response titled "WEB SEARCH RESULTS"
                    that lists all the search queries you made and the top results you found. Format it like this:

                    WEB SEARCH RESULTS:
                    Query 1: "[your search query]"
                    - Result 1: [title] - [URL]
                    - Result 2: [title] - [URL]

                    Query 2: "[your search query]"
                    - Result 1: [title] - [URL]
                    - Result 2: [title] - [URL]

                    This section is critical for transparency and must be included in your response.
                    """
                    logger.info("Using OpenAI with web search as the primary source of information")
                    if stream_callback:
                        stream_callback(f"Agent: No articles provided, using web search as the primary source of information\n")
                else:
                    system_message += """
                    You have access to web search to find the most up-to-date and relevant information.
                    Use web search to supplement the provided articles with additional information when needed.
                    Focus on high-quality, reputable sources for your information.
                    Always include proper citations and links to sources in your newsletter.

                    IMPORTANT: You MUST include a section at the end of your response titled "WEB SEARCH RESULTS"
                    that lists all the search queries you made and the top results you found. Format it like this:

                    WEB SEARCH RESULTS:
                    Query 1: "[your search query]"
                    - Result 1: [title] - [URL]
                    - Result 2: [title] - [URL]

                    Query 2: "[your search query]"
                    - Result 1: [title] - [URL]
                    - Result 2: [title] - [URL]

                    This section is critical for transparency and must be included in your response.
                    """
                    logger.info("Using OpenAI with web search to supplement articles")

            user_message = f"Context:\n{context}\n\nInstructions:\n{prompt}"

            # Prepare the API call parameters
            params = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                "temperature": 0.7,
                "max_tokens": 4000
            }

            # Adjust parameters for GPT-4.1
            if is_gpt41:
                # GPT-4.1 can handle more tokens and has better capabilities
                params["max_tokens"] = 8000  # Increase output token limit for more comprehensive newsletters

                # Add a note about the model's capabilities
                if stream_callback:
                    stream_callback(f"Agent: Using GPT-4.1 with 1M context window and enhanced capabilities\n")
                    stream_callback(f"Agent: This model can generate longer, more detailed newsletters\n")

            # Add web search tools if needed
            if use_web_search and "gpt-4" in model:
                logger.info("Adding web search tools to OpenAI request")
                params["tools"] = [
                    {
                        "type": "retrieval"
                    },
                    {
                        "type": "function",
                        "function": {
                            "name": "search_web",
                            "description": "Search the web for relevant information",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "query": {
                                        "type": "string",
                                        "description": "The search query"
                                    }
                                },
                                "required": ["query"]
                            }
                        }
                    }
                ]
                params["tool_choice"] = "auto"

            # Make the API call
            logger.info(f"Making OpenAI API call with model {model}")
            response = self.openai_client.chat.completions.create(**params)

            # Log tool usage if available
            if hasattr(response, 'choices') and len(response.choices) > 0:
                message = response.choices[0].message

                # Check if the model used any tools
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    logger.info(f"Model used {len(message.tool_calls)} tool calls")

                    # Log each tool call
                    for i, tool_call in enumerate(message.tool_calls):
                        tool_type = getattr(tool_call, 'type', 'unknown')
                        # Get the tool ID for logging purposes
                        tool_id = getattr(tool_call, 'id', 'unknown')
                        logger.info(f"Tool ID: {tool_id}")

                        if tool_type == 'function':
                            function_name = getattr(tool_call.function, 'name', 'unknown')
                            function_args = getattr(tool_call.function, 'arguments', '{}')

                            logger.info(f"Tool call {i+1}: {function_name}")
                            logger.info(f"Arguments: {function_args}")

                            # If it's a web search, extract and log the query
                            if function_name == 'search_web':
                                try:
                                    args = json.loads(function_args)
                                    query = args.get('query', 'unknown')
                                    logger.info(f"Web search query: {query}")
                                except Exception as e:
                                    logger.error(f"Error parsing function arguments: {e}")

            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error generating with OpenAI: {e}")
            return f"Error generating content: {str(e)}"

    def generate_with_anthropic(self, prompt, context, model="claude-3-7-sonnet-20250219", stream_callback=None):
        """Generate content using Anthropic Claude"""
        if not self.anthropic_client:
            return "Error: Anthropic API key not configured"

        try:
            # Log the Anthropic client version and type
            logger.info(f"Anthropic version: {anthropic.__version__ if hasattr(anthropic, '__version__') else 'unknown'}")
            logger.info(f"Anthropic client type: {type(self.anthropic_client)}")
            logger.info(f"Using Anthropic model: {model}")

            # Create system and user messages
            system_message = "You are a newsletter generation assistant that creates high-quality newsletters based on the provided context and instructions."

            # Check context length and truncate if needed to avoid timeouts
            max_context_length = 100000  # Set a reasonable limit for deployment
            if len(context) > max_context_length:
                logger.warning(f"Context too long ({len(context)} chars), truncating to {max_context_length} chars")
                if stream_callback:
                    stream_callback(f"Agent: Context too long, truncating to {max_context_length} characters\n")
                context = context[:max_context_length] + "\n\n[Context truncated due to length]\n"

            user_message = f"Context:\n{context}\n\nInstructions:\n{prompt}"

            # Use the Anthropic client to create a message with timeout handling
            try:
                logger.info(f"Creating message with Anthropic API for model {model}")
                if stream_callback:
                    stream_callback(f"Agent: Sending request to Anthropic API (this may take a moment)...\n")

                # Set a timeout for the API call to prevent worker timeouts
                import httpx

                # Create a client with a timeout
                timeout = httpx.Timeout(30.0, connect=10.0)  # 30 seconds total, 10 seconds connect
                http_client = httpx.Client(timeout=timeout)

                # Create a new client with the timeout
                temp_client = anthropic.Anthropic(
                    api_key=self.anthropic_api_key,
                    http_client=http_client,
                    default_headers={
                        "anthropic-version": "2023-06-01"
                    }
                )

                # Create the message using the Messages API with the temporary client
                try:
                    response = temp_client.messages.create(
                        model=model,
                        max_tokens=2000,  # Reduced for faster response
                        temperature=0.7,
                        system=system_message,
                        messages=[
                            {"role": "user", "content": user_message}
                        ]
                    )

                    logger.info("Successfully received response from Anthropic API")

                    # Extract the content from the response
                    if hasattr(response, 'content') and len(response.content) > 0:
                        content_block = response.content[0]
                        if hasattr(content_block, 'text'):
                            return content_block.text
                        else:
                            return str(content_block)
                    else:
                        return "Error: Empty response from Anthropic API"

                except httpx.TimeoutException:
                    logger.error("Timeout while calling Anthropic API")
                    if stream_callback:
                        stream_callback(f"Agent Error: Timeout while calling Anthropic API. Try using an OpenAI model instead.\n")
                    return "Error: Timeout while calling Anthropic API. Please try using an OpenAI model instead."

            except Exception as specific_e:
                logger.error(f"Specific error in Anthropic API call: {specific_e}")
                if stream_callback:
                    stream_callback(f"Agent Error: Failed to call Anthropic API: {str(specific_e)}\n")
                return f"Error generating content with Anthropic: {str(specific_e)}"

        except Exception as e:
            logger.error(f"Error generating with Anthropic: {e}")
            if stream_callback:
                stream_callback(f"Agent Error: {str(e)}\n")
            return f"Error generating content: {str(e)}"
</file>

<file path="deploy/newsletter_agent/final_app.py">
"""
Final Newsletter Generator App

A refined version of the agent newsletter app that:
1. Properly handles line breaks without being excessive
2. Separates agent thoughts from the generated newsletter
"""

import os
import datetime
import logging
import threading
import queue
import re
import json
from flask import Flask, render_template, request, jsonify, Response
from dotenv import load_dotenv
import PyPDF2
import io

# Import our agent
from agent_newsletter_generator import NewsletterAgent

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev_key_for_testing')

# Configure upload folder
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload size
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'md', 'docx'}

# Global variables for streaming
agent_thoughts_queue = queue.Queue()
newsletter_content = ""
context_used = ""
articles_count = 0
agent = None

def init_agent():
    """Initialize the newsletter agent"""
    global agent
    try:
        # Log environment variables (excluding sensitive ones)
        env_vars = {k: v for k, v in os.environ.items()
                   if not k.startswith('PATH') and 'KEY' not in k.upper() and 'SECRET' not in k.upper() and 'PASSWORD' not in k.upper()}
        logger.info(f"Environment variables: {env_vars}")

        agent = NewsletterAgent()
        logger.info("Newsletter agent initialized")

        # Check if the agent was properly initialized
        if agent.openai_client is None:
            logger.warning("OpenAI client not initialized. Some features may not work.")
        if agent.mongo_client is None:
            logger.warning("MongoDB connection not established. Some features may not work.")
        elif agent.db is None:
            logger.warning("MongoDB database not selected. Some features may not work.")
    except Exception as e:
        logger.error(f"Failed to initialize newsletter agent: {e}")
        agent = None

def allowed_file(filename):
    """Check if file has an allowed extension"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_from_file(file):
    """Extract text from uploaded file"""
    filename = file.filename
    file_ext = filename.rsplit('.', 1)[1].lower() if '.' in filename else ''

    if file_ext == 'txt' or file_ext == 'md':
        # Text files
        return file.read().decode('utf-8')
    elif file_ext == 'pdf':
        # PDF files
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            return ""
    elif file_ext == 'docx':
        # DOCX files (requires python-docx package)
        try:
            import docx
            doc = docx.Document(io.BytesIO(file.read()))
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""

    return ""

# Initialize agent on startup
init_agent()

@app.route('/')
def index():
    """Render the main page"""
    global agent

    if agent is None:
        init_agent()

    try:
        # Get all scraping packages
        packages = []
        if agent:
            try:
                packages = agent.get_scraping_packages()
                logger.info(f"Retrieved {len(packages)} scraping packages")

                # Log package details for debugging
                for i, pkg in enumerate(packages[:5]):  # Log first 5 packages
                    pkg_id = pkg.get('_id', 'No ID')
                    pkg_name = pkg.get('name', 'Unnamed')
                    logger.info(f"Package {i+1}: ID={pkg_id}, Name={pkg_name}")

                if not packages:
                    logger.warning("No scraping packages found in database")
            except Exception as pkg_error:
                logger.error(f"Error retrieving scraping packages: {pkg_error}")

                # Try to get a direct connection to the collection for debugging
                try:
                    if agent.db:
                        collection_names = agent.db.list_collection_names()
                        logger.info(f"Available collections: {collection_names}")
                        if 'scraping_packages' in collection_names:
                            count = agent.db.scraping_packages.count_documents({})
                            logger.info(f"scraping_packages collection has {count} documents")
                except Exception as e:
                    logger.error(f"Error checking collections: {e}")

        # Define available models
        openai_models = []
        if agent and agent.openai_client is not None:
            openai_models = [
                "gpt-4.1",       # New flagship model with 1M context window
                "gpt-4.1-web",   # Web-enabled version of GPT-4.1
                "gpt-4o",
                "gpt-4o-web",    # Web-enabled version
                "gpt-4-turbo",
                "gpt-4-turbo-web",  # Web-enabled version
                "gpt-3.5-turbo"
            ]
            logger.info(f"OpenAI models available: {openai_models}")
        else:
            logger.warning("OpenAI client not available, no OpenAI models will be shown")

        # Define Anthropic models if available
        anthropic_models = []
        if agent and agent.anthropic_client is not None:
            anthropic_models = [
                "claude-3-7-sonnet-20250219",  # Latest Claude 3.7 Sonnet
                "claude-3-5-sonnet-20241022",  # Latest Claude 3.5 Sonnet
                "claude-3-opus-20240229"       # Claude 3 Opus
            ]
            logger.info(f"Anthropic models available: {anthropic_models}")
        else:
            logger.warning("Anthropic client not available, no Anthropic models will be shown")

        # Combine all models - put OpenAI models first for better deployment experience
        all_models = openai_models + anthropic_models
        if not all_models:
            # Add a default model to prevent empty dropdown
            all_models = ["gpt-4o"]
            logger.warning("No API clients available, using default model list")

        # Check if we're running on deployment (Render.com)
        is_deployment = os.environ.get('RENDER', '') == 'true'
        if is_deployment:
            logger.info("Running on deployment environment, recommending OpenAI models")
            # Move any Anthropic models to the end of the list
            openai_first = [m for m in all_models if not m.startswith("claude-")]
            anthropic_last = [m for m in all_models if m.startswith("claude-")]
            all_models = openai_first + anthropic_last

        return render_template(
            'final_index.html',
            packages=packages,
            openai_models=all_models,  # Use combined models list
            now=datetime.datetime.now()
        )
    except Exception as e:
        logger.error(f"Error loading index page: {e}")
        return render_template('final_index.html', packages=[], openai_models=["gpt-4o"], now=datetime.datetime.now())

@app.route('/stream-thoughts')
def stream_thoughts():
    """Stream the agent's thoughts and process"""
    def generate():
        while True:
            # Get message from queue
            try:
                message = agent_thoughts_queue.get(timeout=1.0)
                if message == "[DONE]":
                    yield "data: [DONE]\n\n"
                    break
                else:
                    # Explicitly encode newlines for SSE
                    message_sse = message.replace("\n", "\\n")
                    yield f"data: {message_sse}\n\n"
            except queue.Empty:
                # Send a keep-alive message
                yield "data: \n\n"
                continue
            except Exception as e:
                logger.error(f"Error in stream: {e}")
                yield f"data: Error: {str(e)}\n\n"
                yield "data: [DONE]\n\n"
                break

    return Response(generate(), mimetype='text/event-stream')

@app.route('/get-newsletter')
def get_newsletter():
    """Get the generated newsletter content and context information"""
    global newsletter_content, context_used, articles_count
    return jsonify({
        "content": newsletter_content,
        "context_used": context_used,
        "articles_count": articles_count
    })

@app.route('/import-json', methods=['POST'])
def import_json():
    """Import newsletter data from JSON file"""
    global newsletter_content, context_used, articles_count

    try:
        # Get the uploaded JSON file
        if 'json_file' not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files['json_file']
        if file.filename == '':
            return jsonify({"error": "No file selected"}), 400

        # Read and parse the JSON data
        try:
            json_data = json.loads(file.read().decode('utf-8'))
        except Exception as e:
            logger.error(f"Error parsing JSON file: {e}")
            return jsonify({"error": f"Invalid JSON file: {str(e)}"}), 400

        # Validate the JSON structure
        if 'inputs' not in json_data or 'outputs' not in json_data:
            return jsonify({"error": "Invalid JSON format: missing 'inputs' or 'outputs' sections"}), 400

        # Extract the data
        inputs = json_data.get('inputs', {})
        outputs = json_data.get('outputs', {})

        # Update the global variables
        newsletter_content = outputs.get('content', '')
        context_used = outputs.get('context_used', '')
        articles_count = outputs.get('articles_count', 0)

        # Return the imported data
        return jsonify({
            "status": "success",
            "message": "JSON data imported successfully",
            "inputs": inputs,
            "outputs": {
                "content": newsletter_content,
                "context_used": context_used,
                "articles_count": articles_count
            }
        })
    except Exception as e:
        logger.error(f"Error importing JSON data: {e}")
        return jsonify({"error": f"Error importing JSON data: {str(e)}"}), 500

@app.route('/generate', methods=['POST'])
def generate():
    """Generate newsletter content using the agent"""
    global agent, agent_thoughts_queue, newsletter_content

    # Log the request
    logger.info(f"Generate request received with form data: {request.form}")

    if agent is None:
        logger.info("Agent not initialized, attempting to initialize")
        init_agent()

    if agent is None:
        logger.error("Failed to initialize newsletter agent")
        return jsonify({"error": "Failed to initialize newsletter agent"}), 500

    # Check if MongoDB is connected
    if agent.db is None:
        logger.error("MongoDB database not connected")
        return jsonify({"error": "MongoDB database not connected"}), 500

    # Clear the queue and reset all content
    while not agent_thoughts_queue.empty():
        try:
            agent_thoughts_queue.get_nowait()
        except queue.Empty:
            break

    # Reset all global variables
    global newsletter_content, context_used, articles_count
    newsletter_content = ""
    context_used = ""
    articles_count = 0

    # Get form data
    model_name = request.form.get('model_name', 'gpt-4o')
    date_range = request.form.get('date_range', 'all')
    package_ids = request.form.getlist('package_ids')

    # Get the article limit
    try:
        article_limit = int(request.form.get('article_limit', '10'))
        logger.info(f"Using article limit: {article_limit}")
    except ValueError:
        article_limit = 10
        logger.warning(f"Invalid article limit value, using default: {article_limit}")

    # Log the package IDs for debugging
    logger.info(f"Received package_ids from form: {package_ids}")

    # Check if no packages are selected
    if not package_ids:
        logger.info("No packages selected, will rely on web search")

        # Check if the model has web search capability
        if "web" not in model_name:
            logger.warning(f"No packages selected but model {model_name} doesn't have web search capability")
            # Suggest using a web-enabled model
            if model_name.startswith("gpt-4.1"):
                logger.info("Suggesting gpt-4.1-web instead")
                model_name = "gpt-4.1-web"
            elif model_name.startswith("gpt-4o"):
                logger.info("Suggesting gpt-4o-web instead")
                model_name = "gpt-4o-web"
            elif model_name.startswith("gpt-4-turbo"):
                logger.info("Suggesting gpt-4-turbo-web instead")
                model_name = "gpt-4-turbo-web"
            else:
                logger.info("Defaulting to gpt-4.1-web for web search capability")
                model_name = "gpt-4.1-web"

            logger.info(f"Automatically switched to web-enabled model: {model_name}")

            # Add a notification to the stream
            agent_thoughts_queue.put(f"Note: No packages selected, automatically switched to web-enabled model: {model_name}\n")
            agent_thoughts_queue.put(f"This model will search the web for information since no articles are available from the database.\n")

    search_query = request.form.get('search_query', '')
    client_context = request.form.get('client_context', '')
    project_context = request.form.get('project_context', '')
    prompt = request.form.get('prompt', '')

    # Process uploaded files
    uploaded_context = ""
    if 'context_files' in request.files:
        files = request.files.getlist('context_files')
        for file in files:
            if file and file.filename and allowed_file(file.filename):
                file_text = extract_text_from_file(file)
                if file_text:
                    uploaded_context += f"\n\n--- Content from {file.filename} ---\n{file_text}"

    # Define the callback function for streaming
    def stream_callback(content):
        global newsletter_content

        # Check if this is part of the actual newsletter
        # We'll consider content that starts with a title-like format to be the newsletter
        if re.match(r'^#+ |^\*\*[^*]+\*\*|^[A-Z][^a-z]+:', content) and not content.startswith("Agent"):
            # This is likely part of the newsletter
            newsletter_content += content
            # Also send to the thoughts queue but mark it as newsletter content
            agent_thoughts_queue.put(content)
        else:
            # This is agent thoughts/process
            agent_thoughts_queue.put(content)

    # Run the agent in a separate thread
    def run_agent():
        # Declare globals at the beginning of the function
        global newsletter_content, context_used, articles_count

        try:
            # Put initial message in queue
            agent_thoughts_queue.put("Starting newsletter generation...\n")

            # Run the agent
            result = agent.run_agent_with_query(
                search_query=search_query,
                date_range=date_range,
                package_ids=package_ids,
                client_context=client_context,
                project_context=project_context,
                prompt=prompt,
                uploaded_context=uploaded_context,
                model=model_name,
                article_limit=article_limit,
                stream_callback=stream_callback
            )

            # Store the context information and article count

            # If we didn't capture any newsletter content, use the generated_content from the result
            if not newsletter_content and "generated_content" in result:
                newsletter_content = result["generated_content"]

            # Store the context used and article count
            if "context_used" in result:
                context_used = result["context_used"]

            if "articles_count" in result:
                articles_count = result["articles_count"]

            # Signal the end of the stream
            agent_thoughts_queue.put("[DONE]")

            # Return the result
            return result
        except Exception as e:
            logger.error(f"Error running agent: {e}")
            agent_thoughts_queue.put(f"Error: {str(e)}")
            agent_thoughts_queue.put("[DONE]")

            # Update global variables for error case
            newsletter_content = f"Error generating newsletter: {str(e)}"
            context_used = ""
            articles_count = 0

            return {
                "error": str(e),
                "generated_content": newsletter_content,
                "context_used": context_used,
                "articles_count": articles_count
            }

    # Start the agent thread
    agent_thread = threading.Thread(target=run_agent)
    agent_thread.daemon = True
    agent_thread.start()

    # Return success response
    return jsonify({"status": "success"})

@app.route('/health')
def health_check():
    """Health check endpoint for monitoring"""
    status = {
        "status": "ok",
        "timestamp": datetime.datetime.now().isoformat(),
        "agent_initialized": agent is not None,
        "openai_client": agent.openai_client is not None if agent else False,
        "anthropic_client": agent.anthropic_client is not None if agent else False,
        "mongodb_connected": agent.mongo_client is not None if agent else False,
        "environment_variables": {
            k: "[SET]" if v else "[NOT SET]"
            for k, v in os.environ.items()
            if k in ['MONGODB_URI', 'MONGODB_DB_NAME', 'OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'LOCAL', 'FLASK_SECRET_KEY']
        }
    }
    return jsonify(status)

@app.route('/debug/packages')
def debug_packages():
    """Debug endpoint to check scraping packages"""
    if not agent:
        return jsonify({"error": "Agent not initialized"}), 500

    try:
        # Get packages directly from MongoDB
        packages = []
        collections = []
        package_count = 0

        if agent.db:
            try:
                collections = agent.db.list_collection_names()
                if 'scraping_packages' in collections:
                    cursor = agent.db.scraping_packages.find({})
                    packages = [{"_id": str(p.get("_id")), "name": p.get("name"), "description": p.get("description")} for p in cursor]
                    package_count = len(packages)
            except Exception as e:
                return jsonify({"error": f"Error accessing MongoDB: {str(e)}"}), 500

        # Get packages through the agent method
        agent_packages = []
        try:
            agent_packages = agent.get_scraping_packages()
        except Exception as e:
            return jsonify({"error": f"Error from agent.get_scraping_packages(): {str(e)}"}), 500

        return jsonify({
            "direct_mongodb": {
                "collections": collections,
                "package_count": package_count,
                "packages": packages
            },
            "agent_method": {
                "package_count": len(agent_packages),
                "packages": agent_packages
            }
        })
    except Exception as e:
        return jsonify({"error": f"Unexpected error: {str(e)}"}), 500

# For Render.com deployment
app = app

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5006))  # Use a different port
    app.run(host='0.0.0.0', port=port, debug=True)
</file>

<file path="deploy/scraper/run_scraper.py">
#!/usr/bin/env python3
"""
RSS Feed Scraper for Proton DB

This script fetches articles from RSS feeds stored in packages in MongoDB,
processes them using NLP, generates vector embeddings, and stores them
in the articles collection.
"""

import os
import sys
import logging
import datetime
import re
import html

import feedparser
import requests
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import numpy as np
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from dotenv import load_dotenv

# Import local modules
from vector_embeddings import EmbeddingGenerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Add parent directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(os.path.dirname(current_dir))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Import ProtonDB for database operations
try:
    from proton_db_setup import ProtonDB
except ImportError:
    logger.error("Failed to import ProtonDB. Make sure the module is in your PYTHONPATH.")
    sys.exit(1)

# Initialize NLTK resources
def initialize_nltk():
    """Initialize NLTK resources needed for NLP processing"""
    try:
        # Ensure NLTK data directory exists
        nltk_data_dir = os.path.expanduser('~/nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)

        # Set NLTK data path
        nltk.data.path.append(nltk_data_dir)

        # Download required NLTK resources
        resources = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words']
        for resource in resources:
            try:
                nltk.download(resource, quiet=True, download_dir=nltk_data_dir)
                logger.info(f"Downloaded NLTK resource: {resource}")
            except Exception as e:
                logger.warning(f"Failed to download NLTK resource {resource}: {e}")
                # Continue anyway, we'll handle missing resources gracefully

        logger.info("NLTK initialization completed")
    except Exception as e:
        logger.warning(f"Failed to initialize NLTK: {e}")
        logger.warning("Will continue with limited NLP functionality")

def run_scraper():
    """
    Run the scraper for all active packages in the database
    """
    logger.info("Starting scraper run")
    print("Starting scraper run")

    # Initialize statistics counters
    stats = {
        "packages_processed": 0,
        "articles_added": 0,
        "articles_skipped": 0,
        "feed_errors": 0,
        "article_errors": 0,
        "embedding_errors": 0,
        "start_time": datetime.datetime.now()
    }

    # Initialize NLTK
    initialize_nltk()

    # Initialize embedding generator - explicitly set OpenAI as the model
    embedding_model = os.environ.get('EMBEDDING_MODEL', 'openai')
    embedding_generator = EmbeddingGenerator(model_type=embedding_model)
    logger.info(f"Using {embedding_model} embeddings")

    # Connect to MongoDB using ProtonDB
    db = ProtonDB()
    try:
        logger.info(f"Connected to database: {db.db.name}")

        # Log available collections
        collections = db.db.list_collection_names()
        logger.info(f"Available collections: {collections}")

        # Get all active packages
        packages = list(db.db.scraping_packages.find({"status": "active"}))
        total_packages = len(packages)
        logger.info(f"Found {total_packages} active packages")
        print(f"Found {total_packages} active packages")

        # Process each package
        for package in packages:
            package_id = package["_id"]
            package_name = package.get("name", "Unnamed package")
            package_description = package.get("description", "")

            logger.info(f"Running package: {package_name} ({package_id})")
            print(f"Running package: {package_name} ({package_id})")

            # Package-specific stats
            package_stats = {
                "articles_added": 0,
                "articles_skipped": 0,
                "feed_errors": 0,
                "article_errors": 0
            }

            # Get feeds directly from the package document
            rss_feeds = package.get("rss_feeds", [])
            feed_count = len(rss_feeds)
            logger.info(f"Found {feed_count} feeds directly in package {package_name}")
            print(f"Found {feed_count} feeds directly in package {package_name}")

            if feed_count == 0:
                logger.warning(f"No feeds found for package: {package_name}")
                continue

            # Get project_ids from the package
            project_ids = package.get("project_ids", [])

            # Process each feed
            articles_processed = 0
            for feed_item in rss_feeds:
                # Check if feed_item is a string (URL) or a dictionary
                if isinstance(feed_item, str):
                    feed_url = feed_item
                    feed_name = urlparse(feed_url).netloc  # Use domain as name
                elif isinstance(feed_item, dict) and "url" in feed_item:
                    feed_url = feed_item["url"]
                    feed_name = feed_item.get("name", urlparse(feed_url).netloc)
                else:
                    logger.warning(f"Invalid feed format in package {package_name}, skipping")
                    continue

                logger.info(f"Processing feed: {feed_name} ({feed_url})")
                print(f"Processing feed: {feed_name} ({feed_url})")

                try:
                    # Parse the feed
                    parsed_feed = feedparser.parse(feed_url)

                    # Check if feed parsing was successful
                    if hasattr(parsed_feed, "status") and parsed_feed.status != 200:
                        logger.error(f"Error fetching feed: {feed_url}, status: {parsed_feed.status}")
                        print(f"Error fetching feed: {feed_url}, status: {parsed_feed.status}")
                        package_stats["feed_errors"] += 1
                        stats["feed_errors"] += 1
                        continue

                    # Process each entry (limit to 10 per feed)
                    for entry in parsed_feed.entries[:10]:
                        title = entry.get("title", "No title")
                        link = entry.get("link", "")

                        # Skip if no link
                        if not link:
                            continue

                        # Check if article already exists
                        existing = db.db.articles.find_one({"source_url": link})
                        if existing:
                            logger.info(f"Article already exists: {title}")
                            package_stats["articles_skipped"] += 1
                            stats["articles_skipped"] += 1
                            continue

                        try:
                            # Process article with full NLP
                            article_data = process_article(
                                entry,
                                feed_name,
                                package_name,
                                package_description,
                                package_id,
                                project_ids,
                                embedding_generator,
                                stats
                            )

                            # Save to database
                            result = db.db.articles.insert_one(article_data)
                            article_id = result.inserted_id
                            logger.info(f"Added article: {title} (ID: {article_id})")
                            print(f"Added article: {title} (ID: {article_id})")

                            package_stats["articles_added"] += 1
                            stats["articles_added"] += 1
                            articles_processed += 1

                        except Exception as article_error:
                            logger.error(f"Error processing article {title}: {article_error}")
                            package_stats["article_errors"] += 1
                            stats["article_errors"] += 1
                            continue

                except Exception as feed_error:
                    logger.error(f"Error processing feed {feed_url}: {feed_error}")
                    package_stats["feed_errors"] += 1
                    stats["feed_errors"] += 1
                    continue

            # Update package stats
            db.db.scraping_packages.update_one(
                {"_id": package_id},
                {
                    "$set": {"last_run": datetime.datetime.now(datetime.timezone.utc)},
                    "$inc": {"article_count": articles_processed, "articles_last_run": articles_processed}
                }
            )

            # Log package summary
            logger.info(f"Package summary - {package_name}:")
            logger.info(f"  Articles added: {package_stats['articles_added']}")
            logger.info(f"  Articles skipped (duplicates): {package_stats['articles_skipped']}")
            logger.info(f"  Feed errors: {package_stats['feed_errors']}")
            logger.info(f"  Article processing errors: {package_stats['article_errors']}")

            print(f"Completed package: {package_name}")
            print(f"  Articles added: {package_stats['articles_added']}")
            print(f"  Articles skipped (duplicates): {package_stats['articles_skipped']}")
            print(f"  Feed errors: {package_stats['feed_errors']}")
            print(f"  Article processing errors: {package_stats['article_errors']}")

            stats["packages_processed"] += 1

        # Calculate run duration
        end_time = datetime.datetime.now()
        duration = end_time - stats["start_time"]

        # Print overall summary
        print("\n" + "="*50)
        print("SCRAPER RUN SUMMARY")
        print("="*50)
        print(f"Run completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total run time: {duration.total_seconds():.2f} seconds")
        print(f"Packages processed: {stats['packages_processed']}/{total_packages}")
        print(f"Articles added: {stats['articles_added']}")
        print(f"Articles skipped (duplicates): {stats['articles_skipped']}")
        print(f"Feed errors: {stats['feed_errors']}")
        print(f"Article processing errors: {stats['article_errors']}")
        print(f"Embedding generation errors: {stats['embedding_errors']}")
        print("="*50)

        logger.info("All packages completed")
        logger.info(f"SUMMARY - Articles added: {stats['articles_added']}, Skipped: {stats['articles_skipped']}, Errors: {stats['article_errors'] + stats['feed_errors']}")

    except Exception as e:
        logger.error(f"Error running scraper: {e}")
        print(f"Error running scraper: {e}")
    finally:
        # Close MongoDB connection
        db.close()
        logger.info("MongoDB connection closed")

def process_article(entry, feed_name, package_name, package_description, package_id, project_ids, embedding_generator, stats):
    """
    Process an article with full NLP and vector embeddings

    Args:
        entry: RSS feed entry
        feed_name: Name of the feed
        package_name: Name of the package
        package_description: Description of the package
        package_id: MongoDB ID of the package
        project_ids: List of project IDs associated with the package
        embedding_generator: Embedding generator instance
        stats: Statistics dictionary to update

    Returns:
        dict: Processed article data
    """
    title = entry.get("title", "No title")
    link = entry.get("link", "")

    # Get summary
    summary = entry.get("summary", "")
    if not summary and "description" in entry:
        summary = entry.get("description", "")

    # Clean up summary - remove HTML tags
    summary = re.sub(r"<[^>]+>", "", summary)
    summary = html.unescape(summary)

    # Extract published date
    published_date = None
    if "published_parsed" in entry and entry.published_parsed:
        published_date = datetime.datetime(*entry.published_parsed[:6])
    elif "updated_parsed" in entry and entry.updated_parsed:
        published_date = datetime.datetime(*entry.updated_parsed[:6])

    # Get domain from URL for source_name
    domain = urlparse(link).netloc
    source_name = feed_name or domain

    # Author information
    author = entry.get("author", "Unknown")

    try:
        # Fetch full content
        content = fetch_article_content(link)

        # If content is too short or failed to fetch, use summary
        if not content or len(content) < 100:
            content = summary

        # Create a package-enhanced version of the content
        package_info = f"This article is part of the '{package_name}' package. {package_description}\n\n"
        enhanced_content = package_info + content

        # Extract keywords
        keywords = extract_keywords(enhanced_content, package_name, package_description)

        # Extract named entities
        entities = extract_entities(enhanced_content)

        # Generate summary
        summarized_text = generate_summary(content, summary)

        # Calculate vector embedding
        vector_embedding = None
        embedding_dim = 0
        try:
            # Combine title, package name, description, and content for embedding
            text_to_embed = f"{title}. {package_name}. {package_description}. {content}"
            vector_embedding = embedding_generator.get_embedding(text_to_embed)
            embedding_dim = len(vector_embedding)
            logger.debug(f"Generated embedding with dimension: {embedding_dim}")

            # Verify embedding dimensions are correct for OpenAI's text-embedding-3-small (1536 dimensions)
            if embedding_generator.model_type == "openai" and embedding_dim != 1536:
                logger.warning(f"OpenAI embedding has unexpected dimension: {embedding_dim}, expected 1536")
                logger.info("This might indicate an issue with the OpenAI API or model version")
        except Exception as e:
            logger.warning(f"Failed to generate embedding: {e}")
            stats["embedding_errors"] += 1
            # Continue without embedding

        # Calculate relevance scores
        recency_score = calculate_recency_score(published_date)
        package_fit_score = calculate_package_fit(content, package_name, package_description)
        overall_score = 0.3 * recency_score + 0.7 * package_fit_score

        # Create the article data
        article_data = {
            "title": title,
            "content": enhanced_content,
            "original_content": content,
            "summary": summarized_text,
            "source_url": link,
            "source_name": source_name,
            "author": author,
            "published_date": published_date,
            "date_scraped": datetime.datetime.now(datetime.timezone.utc),
            "scraping_package_id": package_id,
            "package_name": package_name,
            "project_ids": project_ids,  # Add project_ids from the package
            "keywords": keywords,
            "entities": entities,
            "content_type": "news",
            "status": "active",
            "relevance_scores": {
                "overall": overall_score,
                "recency": recency_score,
                "package_fit": package_fit_score
            }
        }

        # Add vector embedding if available
        if vector_embedding:
            article_data["vector_embedding"] = vector_embedding
            article_data["embedding_model"] = embedding_generator.model_type
            article_data["embedding_dimension"] = embedding_dim

        return article_data

    except Exception as e:
        logger.error(f"Error processing article from {link}: {e}")

        # Create minimal article data
        article_data = {
            "title": title,
            "content": f"This article is part of the '{package_name}' package. {package_description}\n\n{summary}",
            "original_content": summary,
            "summary": summary,
            "source_url": link,
            "source_name": source_name,
            "author": author,
            "published_date": published_date,
            "date_scraped": datetime.datetime.now(datetime.timezone.utc),
            "scraping_package_id": package_id,
            "package_name": package_name,
            "project_ids": project_ids,  # Add project_ids from the package
            "keywords": extract_keywords_simple(summary, package_name, package_description),
            "entities": [],
            "content_type": "news",
            "status": "active",
            "relevance_scores": {
                "overall": 0.5,
                "recency": calculate_recency_score(published_date),
                "package_fit": 0.5
            }
        }

        return article_data

def fetch_article_content(url):
    """
    Fetch and extract the main content from an article URL

    Args:
        url: The URL to fetch

    Returns:
        str: The extracted article content
    """
    try:
        # Fetch the page
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        # Parse with BeautifulSoup
        soup = BeautifulSoup(response.text, "html.parser")

        # Remove unwanted elements
        for tag in soup.find_all(["script", "style", "nav", "header", "footer", "aside"]):
            tag.decompose()

        # Extract content
        article_tag = soup.find("article")
        content_div = soup.find("div", class_=lambda c: c and ("content" in c or "article" in c))
        main_tag = soup.find("main")

        # Use the first content container found
        content_element = article_tag or content_div or main_tag or soup.body

        if content_element:
            # Get text with paragraph breaks
            paragraphs = content_element.find_all("p")
            if paragraphs:
                content = "\n\n".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
            else:
                content = content_element.get_text()

            # Clean up whitespace
            content = " ".join(content.split())
            return content

        return None
    except Exception as e:
        logger.error(f"Error fetching article content: {e}")
        return None

def extract_keywords(text, package_name, package_description):
    """Extract keywords from text using NLTK"""
    try:
        # Try with NLTK if available
        # Tokenize the text
        tokens = word_tokenize(text.lower())
        logger.info(f"Successfully tokenized text with NLTK (found {len(tokens)} tokens)")

        # Remove stopwords
        try:
            stop_words = set(stopwords.words('english'))
            logger.info("Successfully loaded NLTK stopwords")
        except Exception as e:
            logger.warning(f"Failed to load NLTK stopwords: {e}, using basic stopwords")
            # Fallback to basic stopwords
            stop_words = set(['the', 'and', 'a', 'to', 'in', 'of', 'is', 'that', 'it', 'for', 'with', 'as', 'on',
                             'this', 'be', 'are', 'was', 'were', 'has', 'have', 'had', 'do', 'does', 'did',
                             'but', 'or', 'by', 'not', 'what', 'all', 'their', 'there', 'when', 'up', 'use', 'how',
                             'out', 'if', 'so', 'no', 'such', 'they', 'then', 'than'])

        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 3]
        logger.info(f"Filtered to {len(filtered_tokens)} meaningful tokens")

        # Add package name and description keywords
        package_keywords = []
        try:
            for word in word_tokenize(package_name.lower() + ' ' + package_description.lower()):
                if word.isalpha() and word not in stop_words and len(word) > 3:
                    package_keywords.append(word)
        except Exception as e:
            logger.warning(f"Error tokenizing package info: {e}, using simple split")
            # Fallback to simple split
            for word in (package_name.lower() + ' ' + package_description.lower()).split():
                if word.isalpha() and word not in stop_words and len(word) > 3:
                    package_keywords.append(word)

        # Boost package keywords
        for word in package_keywords:
            filtered_tokens.extend([word] * 3)  # Add multiple instances to boost frequency

        # Count word frequency
        from collections import Counter
        word_counts = Counter(filtered_tokens)

        # Get top keywords
        keywords = [word for word, _ in word_counts.most_common(15)]
        logger.info(f"Extracted {len(keywords)} keywords: {', '.join(keywords[:5])}...")
        return keywords
    except Exception as nltk_error:
        logger.warning(f"Failed to extract keywords with NLTK: {nltk_error}")
        # Fallback to simple keyword extraction
        logger.info("Falling back to simple keyword extraction")
        return extract_keywords_simple(text, package_name, package_description)

def extract_keywords_simple(text, package_name, package_description):
    """Extract keywords without NLTK for fallback"""
    # Simple stopwords list
    simple_stopwords = set(['the', 'and', 'a', 'to', 'in', 'of', 'is', 'that', 'it',
                         'for', 'with', 'as', 'on', 'at', 'this', 'be', 'by', 'an',
                         'are', 'or', 'not', 'from', 'was', 'were', 'would', 'could',
                         'should', 'has', 'have', 'had', 'been', 'may', 'might', 'must'])

    # Get all words from text
    words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())

    # Remove stopwords
    filtered_words = [word for word in words if word not in simple_stopwords]

    # Add package keywords
    package_words = re.findall(r'\b[a-zA-Z]{4,}\b', package_name.lower() + ' ' + package_description.lower())
    package_keywords = [word for word in package_words if word not in simple_stopwords]

    # Count frequency
    word_freq = {}
    for word in filtered_words:
        word_freq[word] = word_freq.get(word, 0) + 1

    # Boost package keywords
    for word in package_keywords:
        word_freq[word] = word_freq.get(word, 0) + 3

    # Sort by frequency
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

    # Return top 15 keywords or all if less than 15
    return [word for word, _ in sorted_words[:15]]

def extract_entities(text):
    """Extract named entities from text using NLTK"""
    entity_list = []
    try:
        from nltk import pos_tag, ne_chunk

        # Tokenize and tag the text
        tokens = word_tokenize(text)
        tagged = pos_tag(tokens)

        # Extract named entities
        entities = ne_chunk(tagged)

        # Process entity tree
        for entity in entities:
            if hasattr(entity, 'label'):
                # Extract the word from each (word, tag) tuple in the entity
                entity_text = ' '.join([word for word, _ in entity])
                entity_type = entity.label()
                entity_list.append({
                    "text": entity_text,
                    "type": str(entity_type)
                })
    except Exception as e:
        logger.warning(f"Failed to extract entities: {e}")
        # Continue without entities

    return entity_list

def generate_summary(content, fallback_summary):
    """Generate a summary from the content"""
    try:
        # Make sure NLTK resources are downloaded
        try:
            nltk.download('punkt', quiet=True)
        except Exception as e:
            logger.warning(f"Failed to download NLTK punkt: {e}")

        # Simple extractive summary (first 3 sentences)
        try:
            sentences = sent_tokenize(content)
            logger.info(f"Successfully tokenized text into {len(sentences)} sentences with NLTK")
            summary_text = ' '.join(sentences[:3])

            # If summary is long enough, use it
            if len(summary_text) >= 50:
                logger.info(f"Generated summary of {len(summary_text)} characters")
                return summary_text
            else:
                logger.info("Summary too short, using fallback")
                return fallback_summary
        except Exception as e:
            logger.warning(f"NLTK sentence tokenization failed: {e}, falling back to simple split")
            # Simple fallback for sentence tokenization
            sentences = []
            for sentence in content.split('.'):
                if len(sentence.strip()) > 10:  # Ignore very short fragments
                    sentences.append(sentence.strip() + '.')
            logger.info(f"Fallback tokenization found {len(sentences)} sentences")

            if sentences:
                summary_text = ' '.join(sentences[:3])
                if len(summary_text) >= 50:
                    logger.info(f"Generated fallback summary of {len(summary_text)} characters")
                    return summary_text
    except Exception as e:
        logger.warning(f"Failed to generate summary: {e}")

    # If all else fails, use the fallback summary
    logger.info(f"Using fallback summary of {len(fallback_summary)} characters")
    return fallback_summary

def calculate_recency_score(published_date):
    """Calculate recency score for an article"""
    if not published_date:
        return 0.5  # Default score if no date

    now = datetime.datetime.now()
    age_days = (now - published_date).days

    # Exponential decay: score = exp(-age_days/14)
    # This gives ~0.95 for today, ~0.5 for 2 weeks old, ~0.25 for 1 month old
    recency = np.exp(-age_days/14) if age_days >= 0 else 0.95
    return min(1.0, max(0.0, recency))

def calculate_package_fit(content, package_name, package_description):
    """Calculate how well the article fits the package theme"""
    if not content or not package_name:
        return 0.5  # Default score

    # Convert to lowercase for matching
    content_lower = content.lower()
    package_name_lower = package_name.lower()
    package_description_lower = package_description.lower() if package_description else ""

    # Extract package keywords
    package_keywords = set()
    for word in package_name_lower.split() + package_description_lower.split():
        if len(word) > 3 and word.isalpha():
            package_keywords.add(word)

    if not package_keywords:
        return 0.5  # Default score if no keywords

    # Count matches in content
    match_count = sum(1 for word in package_keywords if word in content_lower)

    # Calculate package fit score (ratio of matched keywords to total keywords)
    # with a minimum score of 0.5
    package_fit = max(0.5, min(1.0, match_count / len(package_keywords)))
    return package_fit

if __name__ == "__main__":
    run_scraper()
</file>

<file path="deploy/scraper/render.yaml">
services:
  # Cron job for running the full scraper with NLP on schedule
  - type: cron
    name: proton-db-scraper
    runtime: python
    rootDir: deploy/scraper
    buildCommand: |
      pip install --upgrade pip wheel setuptools
      pip install --no-build-isolation --only-binary=:all: numpy==1.23.5
      pip install --no-build-isolation --only-binary=:all: scipy==1.10.1
      pip install pymongo==4.5.0 dnspython==2.4.2 python-dotenv==1.0.0 
      pip install feedparser==6.0.10 requests==2.31.0 beautifulsoup4==4.12.2 lxml==4.9.2 html5lib==1.1
      pip install nltk==3.8.1
      pip install --no-build-isolation --only-binary=:all: scikit-learn==1.0.2
      pip install openai==1.3.5
      pip install --no-build-isolation --only-binary=:all: torch==1.13.1
      pip install --no-build-isolation --only-binary=:all: sentence-transformers==2.2.2
      python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('averaged_perceptron_tagger'); nltk.download('maxent_ne_chunker'); nltk.download('words')"
    startCommand: python run_scraper.py
    envVars:
      - key: MONGODB_URI
        sync: false
      - key: MONGODB_DB_NAME
        value: proton
      - key: OPENAI_API_KEY
        sync: true
      - key: EMBEDDING_MODEL
        value: openai
    schedule: "0 6,18 * * *" # Run at 6AM and 6PM every day
    
  # Job to add feeds to packages (optional, only run manually)
  - type: cron
    name: proton-add-feeds
    runtime: python
    rootDir: deploy/scraper
    buildCommand: pip install pymongo==4.5.0 dnspython==2.4.2 python-dotenv==1.0.0
    startCommand: python add_feed.py
    envVars:
      - key: MONGODB_URI
        sync: false
      - key: MONGODB_DB_NAME
        value: proton
    schedule: "0 0 31 2 *" # February 31st (never runs automatically)
    
  # One-time job as a cron that runs only once
  - type: cron
    name: proton-create-test-package
    runtime: python
    rootDir: deploy/scraper
    buildCommand: pip install pymongo==4.5.0 dnspython==2.4.2 python-dotenv==1.0.0
    startCommand: python create_test_package.py
    envVars:
      - key: MONGODB_URI
        sync: false
    schedule: "0 0 31 2 *" # February 31st (never runs automatically)
</file>

<file path="render.yaml">
services:
  # Web service for the Flask application
  - type: web
    name: proton-db-viewer
    runtime: python
    rootDir: deploy/web
    buildCommand: |
      pip install --upgrade pip
      pip install flask==3.0.0 pymongo==4.5.0 dnspython==2.4.2 python-dotenv==1.0.0 gunicorn==21.2.0 markupsafe==2.1.3
    startCommand: gunicorn 'wsgi:app' --log-file -
    envVars:
      - key: MONGODB_URI
        sync: false
      - key: MONGODB_DB_NAME
        value: proton
    plan: free
    autoDeploy: true

  # Web service for the newsletter agent
  - type: web
    name: newsletter-agent
    runtime: python
    rootDir: deploy/newsletter_agent
    buildCommand: |
      pip install --upgrade pip
      pip install -r requirements.txt
    startCommand: gunicorn 'wsgi:app' --log-file -
    envVars:
      - key: MONGODB_URI
        sync: false
      - key: MONGODB_DB_NAME
        value: proton
      - key: LOCAL
        value: "True"
      - key: OPENAI_API_KEY
        sync: false
      - key: ANTHROPIC_API_KEY
        sync: false
      - key: FLASK_SECRET_KEY
        generateValue: true
      - key: EMBEDDING_MODEL
        value: openai
    plan: free
    autoDeploy: true

  # Cron job for running the full scraper with NLP on schedule
  - type: cron
    name: proton-db-scraper
    runtime: python
    rootDir: deploy/scraper
    buildCommand: |
      pip install --upgrade pip wheel setuptools
      pip install --no-build-isolation --only-binary=:all: numpy==1.23.5
      pip install --no-build-isolation --only-binary=:all: scipy==1.10.1
      pip install pymongo==4.5.0 dnspython==2.4.2 python-dotenv==1.0.0
      pip install feedparser==6.0.10 requests==2.31.0 beautifulsoup4==4.12.2 lxml==4.9.2 html5lib==1.1
      pip install nltk==3.8.1
      pip install --no-build-isolation --only-binary=:all: scikit-learn==1.0.2
      pip install openai==1.3.5
      pip install --no-build-isolation --only-binary=:all: torch==1.13.1
      pip install --no-build-isolation --only-binary=:all: sentence-transformers==2.2.2
      python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('averaged_perceptron_tagger'); nltk.download('maxent_ne_chunker'); nltk.download('words')"
    startCommand: python run_scraper.py
    envVars:
      - key: MONGODB_URI
        sync: false
      - key: MONGODB_DB_NAME
        value: proton
      - key: OPENAI_API_KEY
        sync: false
      - key: EMBEDDING_MODEL
        value: openai
    schedule: "0 6,18 * * *" # Run at 6AM and 6PM every day

  # Job to add feeds to packages
  - type: cron
    name: proton-add-feeds
    runtime: python
    rootDir: deploy/scraper
    buildCommand: pip install pymongo==4.5.0 dnspython==2.4.2 python-dotenv==1.0.0
    startCommand: python add_feed.py
    envVars:
      - key: MONGODB_URI
        sync: false
      - key: MONGODB_DB_NAME
        value: proton
    schedule: "0 0 31 2 *" # February 31st (never runs automatically)
</file>

</files>
